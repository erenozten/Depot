Atomicity: The atomicity acid property in SQL. It means either all the operations (insert, update, delete) inside a transaction take place or none. Or you can say, all the statements (insert, update, delete) inside a transaction are either completed or rolled back.
-

A transaction can be defined as a group of tasks. A single task is the minimum processing unit which cannot be divided further.
Let’s take an example of a simple transaction. Suppose a bank employee transfers Rs 500 from A's account to B's account. This very simple and small transaction involves several low-level tasks.
A’s Account
Open_Account(A)
Old_Balance = A.balance
New_Balance = Old_Balance - 500
A.balance = New_Balance
Close_Account(A)
B’s Account
Open_Account(B)
Old_Balance = B.balance
New_Balance = Old_Balance + 500
B.balance = New_Balance
Close_Account(B)
ACID Properties
A transaction is a very small unit of a program and it may contain several lowlevel tasks. A transaction in a database system must maintain Atomicity, Consistency, Isolation, and Durability − commonly known as ACID properties − in order to ensure accuracy, completeness, and data integrity.
•	Atomicity − This property states that a transaction must be treated as an atomic unit, that is, either all of its operations are executed or none. There must be no state in a database where a transaction is left partially completed. States should be defined either before the execution of the transaction or after the execution/abortion/failure of the transaction.
•	Consistency − The database must remain in a consistent state after any transaction. No transaction should have any adverse effect on the data residing in the database. If the database was in a consistent state before the execution of a transaction, it must remain consistent after the execution of the transaction as well.
•	Durability − The database should be durable enough to hold all its latest updates even if the system fails or restarts. If a transaction updates a chunk of data in a database and commits, then the database will hold the modified data. If a transaction commits but the system fails before the data could be written on to the disk, then that data will be updated once the system springs back into action.
•	Isolation − In a database system where more than one transaction are being executed simultaneously and in parallel, the property of isolation states that all the transactions will be carried out and executed as if it is the only transaction in the system. No transaction will affect the existence of any other transaction.
Serializability
When multiple transactions are being executed by the operating system in a multiprogramming environment, there are possibilities that instructions of one transactions are interleaved with some other transaction.
•	Schedule − A chronological execution sequence of a transaction is called a schedule. A schedule can have many transactions in it, each comprising of a number of instructions/tasks.
•	Serial Schedule − It is a schedule in which transactions are aligned in such a way that one transaction is executed first. When the first transaction completes its cycle, then the next transaction is executed. Transactions are ordered one after the other. This type of schedule is called a serial schedule, as transactions are executed in a serial manner.
In a multi-transaction environment, serial schedules are considered as a benchmark. The execution sequence of an instruction in a transaction cannot be changed, but two transactions can have their instructions executed in a random fashion. This execution does no harm if two transactions are mutually independent and working on different segments of data; but in case these two transactions are working on the same data, then the results may vary. This ever-varying result may bring the database to an inconsistent state.
To resolve this problem, we allow parallel execution of a transaction schedule, if its transactions are either serializable or have some equivalence relation among them.

Equivalence Schedules
An equivalence schedule can be of the following types −
Result Equivalence
If two schedules produce the same result after execution, they are said to be result equivalent. They may yield the same result for some value and different results for another set of values. That's why this equivalence is not generally considered significant.
View Equivalence
Two schedules would be view equivalence if the transactions in both the schedules perform similar actions in a similar manner.
For example −
•	If T reads the initial data in S1, then it also reads the initial data in S2.
•	If T reads the value written by J in S1, then it also reads the value written by J in S2.
•	If T performs the final write on the data value in S1, then it also performs the final write on the data value in S2.
Conflict Equivalence
Two schedules would be conflicting if they have the following properties −
•	Both belong to separate transactions.
•	Both accesses the same data item.
•	At least one of them is "write" operation.
Two schedules having multiple transactions with conflicting operations are said to be conflict equivalent if and only if −
•	Both the schedules contain the same set of Transactions.
•	The order of conflicting pairs of operation is maintained in both the schedules.
Note − View equivalent schedules are view serializable and conflict equivalent schedules are conflict serializable. All conflict serializable schedules are view serializable too.
States of Transactions
A transaction in a database can be in one of the following states −
 
•	Active − In this state, the transaction is being executed. This is the initial state of every transaction.
•	Partially Committed − When a transaction executes its final operation, it is said to be in a partially committed state.
•	Failed − A transaction is said to be in a failed state if any of the checks made by the database recovery system fails. A failed transaction can no longer proceed further.
•	Aborted − If any of the checks fails and the transaction has reached a failed state, then the recovery manager rolls back all its write operations on the database to bring the database back to its original state where it was prior to the execution of the transaction. Transactions in this state are called aborted. The database recovery module can select one of the two operations after a transaction aborts −
o	Re-start the transaction
o	Kill the transaction
•	Committed − If a transaction executes all its operations successfully, it is said to be committed. All its effects are now permanently established on the database system.
Wikipediden, güzel görünüyo baya:
In computer science, ACID (atomicity, consistency, isolation, durability) is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.
In 1983,[1] Andreas Reuter and Theo Härder coined the acronym ACID, building on earlier work by Jim Gray[2] who named atomicity, consistency, and durability, but not isolation, when characterizing the transaction concept. These four properties are the major guarantees of the transaction paradigm, which has influenced many aspects of development in database systems.
According to Gray and Reuter, the IBM Information Management System supported ACID transactions as early as 1973 (although the acronym was created later).[3]
Characteristics
The characteristics of these four properties as defined by Reuter and Härder are as follows:
Atomicity
	Main article: Atomicity (database systems)

Transactions are often composed of multiple statements. Atomicity guarantees that each transaction is treated as a single "unit", which either succeeds completely, or fails completely: if any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged. An atomic system must guarantee atomicity in each and every situation, including power failures, errors and crashes.[4] A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).
An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a consistent state, that is, money is neither debited nor credited if either of those two operations fail.[5]
Consistency
	Main article: Consistency (database systems)

Consistency ensures that a transaction can only bring the database from one valid state to another, maintaining database invariants: any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof. This prevents database corruption by an illegal transaction, but does not guarantee that a transaction is correct. Referential integrity guarantees the primary key – foreign key relationship. [6]
Isolation
	Main article: Isolation (database systems)

Transactions are often executed concurrently (e.g., multiple transactions reading and writing to a table at the same time). Isolation ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially. Isolation is the main goal of concurrency control; depending on the method used, the effects of an incomplete transaction might not even be visible to other transactions. [7]
Durability
	Main article: Durability (database systems)

Durability guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash). This usually means that completed transactions (or their effects) are recorded in non-volatile memory.[citation needed]
Examples
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2018) (Learn how and when to remove this template message)
The following examples further illustrate the ACID properties. In these examples, the database table has two columns, A and B. An integrity constraint requires that the value in A and the value in B must sum to 100. The following SQL code creates a table as described above:
CREATE TABLE acidtest (A INTEGER, B INTEGER, CHECK (A + B = 100));
Atomicity
Atomicity is the guarantee that series of database operations in an atomic transaction will either all occur (a successful operation), or none will occur (an unsuccessful operation). The series of operations cannot be separated with only some of them being executed, which makes the series of operations "indivisible". A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. In other words, atomicity means indivisibility and irreducibility.[8] Alternatively, we may say that a logical transaction may be made of, or composed of, one or more (several), physical transactions. Unless and until all component physical transactions are executed, the Logical transaction will not have occurred – to the effects of the database. Say our Logical transaction consists of transferring funds from account A to account B. This logical transaction may be composed of several physical transactions consisting of first removing the amount from account A as a first physical transaction and then, as a second transaction, depositing said amount in account B. We would not want to see the amount removed from account A before we are sure it has been transferred into account B. Then, unless and until both transactions have happened and the amount has been transferred to account B, the transfer has not, to the effects of the database, occurred.
Consistency failure
Consistency is a very general term, which demands that the data must meet all validation rules. In the previous example, the validation is a requirement that A + B = 100. All validation rules must be checked to ensure consistency. Assume that a transaction attempts to subtract 10 from A without altering B. Because consistency is checked after each transaction, it is known that A + B = 100 before the transaction begins. If the transaction removes 10 from A successfully, atomicity will be achieved. However, a validation check will show that A + B = 90, which is inconsistent with the rules of the database. The entire transaction must be cancelled and the affected rows rolled back to their pre-transaction state. If there had been other constraints, triggers, or cascades, every single change operation would have been checked in the same way as above before the transaction was committed. Similar issues may arise with other constraints. We may have required the data types of both A and B to be integers. If we were then to enter, say, the value 13.5 for A, the transaction will be cancelled, or the system may give rise to an alert in the form of a trigger (if/when the trigger has been written to this effect). Another example would be with integrity constraints, which would not allow us to delete a row in one table whose primary key is referred to by at least one foreign key in other tables.
Isolation failure
To demonstrate isolation, we assume two transactions execute at the same time, each attempting to modify the same data. One of the two must wait until the other completes in order to maintain isolation.
Consider two transactions: T1 transfers 10 from A to B. T2 transfers 20 from B to A.
Combined, there are four actions:
1.	T1 subtracts 10 from A.
2.	T1 adds 10 to B.
3.	T2 subtracts 20 from B.
4.	T2 adds 20 to A.
If these operations are performed in order, isolation is maintained, although T2 must wait. Consider what happens if T1 fails halfway through. The database eliminates T1's effects, and T2 sees only valid data.
By interleaving the transactions, the actual order of actions might be:
1.	T1 subtracts 10 from A.
2.	T2 subtracts 20 from B.
3.	T2 adds 20 to A.
4.	T1 adds 10 to B.
Again, consider what happens if T1 fails while modifying B in Step 4. By the time T1 fails, T2 has already modified A; it cannot be restored to the value it had before T1 without leaving an invalid database. This is known as a write-write conflict,[citation needed] because two transactions attempted to write to the same data field. In a typical system, the problem would be resolved by reverting to the last known good state, canceling the failed transaction T1, and restarting the interrupted transaction T2 from the good state.
Durability failure
Consider a transaction that transfers 10 from A to B. First it removes 10 from A, then it adds 10 to B. At this point, the user is told the transaction was a success. However, the changes are still queued in the disk buffer waiting to be committed to disk. Power fails and the changes are lost, but the user assumes (understandably) that the changes persist.
Implementation
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2015) (Learn how and when to remove this template message)
Processing a transaction often requires a sequence of operations that is subject to failure for a number of reasons. For instance, the system may have no room left on its disk drives, or it may have used up its allocated CPU time. There are two popular families of techniques: write-ahead logging and shadow paging. In both cases, locks must be acquired on all information to be updated, and depending on the level of isolation, possibly on all data that may be read as well. In write ahead logging, durability is guaranteed by copying the original (unchanged) data to a log before changing the database.[dubious – discuss] That allows the database to return to a consistent state in the event of a crash. In shadowing, updates are applied to a partial copy of the database, and the new copy is activated when the transaction commits.
Locking vs multiversioning
Many databases rely upon locking to provide ACID capabilities. Locking means that the transaction marks the data that it accesses so that the DBMS knows not to allow other transactions to modify it until the first transaction succeeds or fails. The lock must always be acquired before processing data, including data that is read but not modified. Non-trivial transactions typically require a large number of locks, resulting in substantial overhead as well as blocking other transactions. For example, if user A is running a transaction that has to read a row of data that user B wants to modify, user B must wait until user A's transaction completes. Two phase locking is often applied to guarantee full isolation.
An alternative to locking is multiversion concurrency control, in which the database provides each reading transaction the prior, unmodified version of data that is being modified by another active transaction. This allows readers to operate without acquiring locks, i.e., writing transactions do not block reading transactions, and readers do not block writers. Going back to the example, when user A's transaction requests data that user B is modifying, the database provides A with the version of that data that existed when user B started his transaction. User A gets a consistent view of the database even if other users are changing data. One implementation, namely snapshot isolation, relaxes the isolation property.
Distributed transactions
	Main article: Distributed transaction

Guaranteeing ACID properties in a distributed transaction across a distributed database, where no single node is responsible for all data affecting a transaction, presents additional complications. Network connections might fail, or one node might successfully complete its part of the transaction and then be required to roll back its changes because of a failure on another node. The two-phase commit protocol (not to be confused with two-phase locking) provides atomicity for distributed transactions to ensure that each participant in the transaction agrees on whether the transaction should be committed or not.[9] Briefly, in the first phase, one node (the coordinator) interrogates the other nodes (the participants) and only when all reply that they are prepared does the coordinator, in the second phase, formalize the transaction.
-
-
-




-
-
In C#, the following things are always true:

short == Int16
ushort == UInt16
int == Int32
uint == UInt32
long == Int64
ulong == UInt64
Both versions are data types. All of the above are integers of various lengths and signed-ness.

The main difference between the two versions (as far as I know) is what colour they are highlighted as in Visual Studio.-
-
using System;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using System.ComponentModel.DataAnnotations.Schema;
using System.Linq;

namespace TrainingLinqAndSql
{
    class Program
    {
        static void Main(string[] args)
        {
            // inner join örneği 
            // İki tablodaki id'leri eşleşen tüm veriler çekiliyor
            var result = Employee.GetAllEmployees().Join(Department.GetAllDepartments(), e => e.DepartmentId, d => d.Id, 
                (emp, dep) => new
                {
                    NewName = emp.Name,
                    NewDepartment = dep.Name
                });

            foreach (var item in result)
            {
                Console.WriteLine(item.NewName + " " + item.NewDepartment);
            }

            Console.WriteLine("---");

            var result2 = Employee.GetAllEmployees().Where(e=>e.DepartmentId == 1).Join(Department.GetAllDepartments(), e => e.DepartmentId, d => d.Id,
                (emp, dep) => new
                {
                    NewName = emp.Name,
                    NewDepartment = dep.Name
                });

            foreach (var item in result2)
            {
                Console.WriteLine(item.NewName + " " + item.NewDepartment);
            }
        }
    }

    class Employee
    {
        [Key]
        public int Id { get; set; }
        public string Name { get; set; }

        [ForeignKey("Department")]
        public int DepartmentId { get; set; }

        public Department Department { get; set; }

        public static List<Employee> GetAllEmployees()
        {
            return new List<Employee>()
            {
                new Employee()
                {
                    Id = 1,
                    Name = "Ahmet",
                    DepartmentId = 1
                },
                new Employee()
                {
                    Id = 2,
                    Name = "Mehmet",
                    DepartmentId = 1
                },
                new Employee()
                {
                    Id = 3,
                    Name = "Necdet",
                    DepartmentId = 2
                },
                new Employee()
                {
                    Id = 4,
                    Name = "Şevket",
                    DepartmentId = 2
                },
                new Employee()
                {
                    Id = 5,
                    Name = "Hıdır",
                },
                new Employee()
                {
                    Id = 6,
                    Name = "Murat"
                },
            }; // örnek employee'ler oluşturulup bu employee'lerin içinde olduğu liste dönülüyor.
        }
    }

    class Department
    {
        [Key]
        public int Id { get; set; }
        public string Name { get; set; }

        public List<Employee> Employees { get; set; } = new List<Employee>();

        public static List<Department> GetAllDepartments()
        {
            return new List<Department>()
            {
                new Department()
                {
                    Id = 1,
                    Name = "HR"
                },
                new Department()
                {
                    Id = 2,
                    Name = "IT"
                },
                new Department()
                {
                    Id = 3,
                    Name = "Payroll"
                },
            }; // örnek department'lar oluşturulup bu department'ların içinde olduğu liste dönülüyor.
        }
    }

}

-
-
-
-
-
-
        {
            int sayi = 942 / 10;
            string str = sayi.ToString(); // sonuç 94   - Yani sondaki rakamı attı.

            int sayi2 = 8 / 10;
            string str2 = sayi2.ToString(); // sonuç 0  -  sekizi ona böldü, buradan sonucu sıfır veriyor. Mantıken de böyle olmalı gibi.
        }
-
-
-
-
-
-
A LANGUAGE IS STRONGLY TYPED if the compiler can guarantee that the accepted programs will execute without type errors. 
-
-

CREATE PROCEDURE NEW44
AS
BEGIN
SELECT [CompanyName], [ContactName]
FROM CUSTOMERS
WHERE [ContactName] LIKE 'A_A_E%'
END
GO
-
-
-
-
-
-
CREATE PROCEDURE QWE
AS
BEGIN
SELECT ColumnName, ColumnId FROM DB 
WHERE  ColumnaSDF LIKE '_%2%_'   -- where içinde kullanılacak property'i SELECT ile seçmemiz gerekmiyor
END
GO
-
-
-
-
-
-
class Kutu
  {
      public int Yukseklik { get; set; }
      public int Genislik { get; set; }
 
      public Kutu(int yuk, int gen)
      {
          Yukseklik = yuk;
          Genislik = gen;
      }
 
      public static Kutu operator *(Kutu k1, Kutu k2)  // eksi işareti de kullanılabilirdi. Sonuç değişmez. Operatörle ilgisi yok yani. + - * / hepsi aynı sonuç.
      {
          int toplamYukseklik = k1.Yukseklik + k2.Yukseklik;
          int toplamGenislik = k1.Genislik + k2.Genislik;
 
          Kutu k3 = new Kutu(toplamYukseklik, toplamGenislik);
          return k3;
      }
  }
 
  static void Main(string[] args)
  {
      Kutu k1 = new Kutu(7, 3);
      Kutu k2 = new Kutu(16, 4);
      Kutu k3 = k1 * k2; // burada, static'te belirttiğimiz operatörü kullanıyoruz: *
 
      Console.WriteLine(k3.Yukseklik); //19
      Console.WriteLine(k3.Genislik); //10
      Console.ReadKey();
  }
-
-
-
-
-
-
using System;
using System.Collections;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;

class Program
{
    class Student
    {
        public string Name { get; set; }
    }

    class Araba: IAraba
    {
        // IAraba'nın metotlarını implement etmesi gerek...

        public void Cam() // implement edilen metotlar PUBLIC olmak zorunda.
        {
            Console.WriteLine("");
        }

        public void Teker()
        {

        }

        public string CamaYaziYazdir()
        {
            return "Text";
        }

        public Student OgrenciyiArabayaBindir()
        {
            Student student = new Student();
            return student;
        }
    }

    //class gibi bir şeydir interface. Parantez açıp kapama yokk. Zaten anlamsız oluyor diğer şekilde.
    interface IAraba
    {
        void Cam(); // köşeli parantez yokk
        void Teker();
        string CamaYaziYazdir();
        Student OgrenciyiArabayaBindir();
    }
}

CREATE PROCEDURE QWE
AS
BEGIN
SELECT ColumnName, ColumnId FROM DB 
WHERE  ColumnaSDF LIKE '_%2%_' 
END
-
-
xCode             (GITHUB)  (test) 
xCode nedir
Xcode macOS, iOS, watchOS ve tvOS için yazılım geliştirmek için Apple tarafından geliştirilen macOS için entegre bir tümleşik geliştirme ortamıdır. Yazılım geliştirme araçlarını içerir. İlk kez 2003 yılında piyasaya çıkan hizmetin en istikrarlı sürümü, sürüm 8.3.3'tür ve Mac App Store'den macOS Sierra kullanıcıları için ücretsiz olarak sunulmaktadır. Kayıtlı geliştiriciler, Apple Developer web sitesinden paketi önizleme sürümlerini ve önceki sürümlerini indirebilirler. Bununla birlikte, Apple son zamanlarda yazılımın sürüm 9 beta 2'nin beta versiyonunu Apple Developer hesaplarıyla halka açık hale getirdi.
-
-
WebPack             (GITHUB)  (test) 
Webpack nedir
Webpack is an open-source JavaScript module bundler. It is made primarily for JavaScript, but it can transform front-end assets such as HTML, CSS, and images if the corresponding loaders are included. webpack takes modules with dependencies and generates static assets representing those modules.
Webpack takes the dependencies and generates a dependency graph allowing web developers to use a modular approach for their web application development purposes. It can be used from the command line, or can be configured using a config file which is named webpack.config.js. This file is used to define rules, plugins, etc., for a project. (webpack is highly extensible via rules which allow developers to write custom tasks that they want to perform when bundling files together.)
Node.js is required for using webpack.
webpack provides code on demand using the moniker code splitting. The Technical Committee 39 for ECMAScript is working on standardization of a function that loads additional code: "proposal-dynamic-import".
-
-
-
CSS (Cascading Style Sheets)             (GITHUB)  (test) 
Css nedir
CSS, “Cascading Style Sheets” kelimelerinin kısaltılmasından oluştur ve türkçe anlamı Basamaklı Stil Şablonları ya da Basamaklı Biçim Sayfaları şeklindedir.
Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in a markup language such as HTML.[1] CSS is a cornerstone technology of the World Wide Web, alongside HTML and JavaScript.[2]
CSS is designed to enable the separation of presentation and content, including layout, colors, and fonts.[3] This separation can improve content accessibility, provide more flexibility and control in the specification of presentation characteristics, enable multiple web pages to share formatting by specifying the relevant CSS in a separate .css file which reduces complexity and repetition in the structural content as well as enabling the .css file to be cached to improve the page load speed between the pages that share the file and its formatting.
Separation of formatting and content also makes it feasible to present the same markup page in different styles for different rendering methods, such as on-screen, in print, by voice (via speech-based browser or screen reader), and on Braille-based tactile devices. CSS also has rules for alternate formatting if the content is accessed on a mobile device.[4]
The name cascading comes from the specified priority scheme to determine which style rule applies if more than one rule matches a particular element. This cascading priority scheme is predictable.
The CSS specifications are maintained by the World Wide Web Consortium (W3C). Internet media type (MIME type) text/css is registered for use with CSS by RFC 2318 (March 1998). The W3C operates a free CSS validation service for CSS documents.[5]
In addition to HTML, other markup languages support the use of CSS including XHTML, plain XML, SVG, and XUL.
-
-
-
ANSI C
 
Gömülü sistemler alanında bilinmesi gereken temel şeylerden biri ANSI C programlama dilidir. Belki C dilini duymuşsunuzdur ama ANSI de ne diye soruyor olabilirsiniz. Kısaca açıklayayım:
ANSI – Amerikan Ulusal Standartlar Enstitüsü’nün adıdır. ANSI C ile bu kurumun yayınlamış olduğu C programlama dili standartıdır. En geniş kullanımı olan standart 89 yılında yayınlanmıştır. Bu standart ayrıca ANSI C89 olarak da bilinmekte. Şu an birçok derleyici bu standarta göre tasarlanmıştır.
C ve C++ dilleri farklı olsalar da genellikle bir arada anılıyorlar. Bu konudan daha önceki “C ve C++” farklı programlama dilleridir” yazımda bahsetmiştim. Gömülü sistemler alanında en çok kullanımı bulunan dil C’dir. C dili prosedürel bir dil iken C++ nesneye yönelik bir dildir. Diller değerlendirilirken bu iki fark gözetilmelidir.
C dili hem assembly gibi donanıma yakın hem de üst seviye dillerin özelliklerine sahip bir dil olduğundan mikroişlemci / mikrodenetleyici tabanlı sistemlerin programlanmasında vazgeçilmez hale gelmiştir. Dolayısıyla gömülü sistemler alanında çalışma yapmak için bu dili öğrenmek de kaçınılmazdır.
ANSI C öğrenmek denildiğinde sadece bir dil öğrenmekten değil aynı zamanda işlemcilerin çalışma mantığını da öğrenmekten bahsediyoruz. Çoğu zaman donanımla direk olarak konuşan sürücüler yazmak zorunda kalındığında o donanımın yapısı ve çalışma biçimi iyice kavranmadan bunu gerçekleştirmek çok zordur. C dili bize assembly dilinin gücüne yakın bir şekilde ve aynı zamanda onun zorluklarından sıyrılarak programlama yapma olanağı sağlamaktadır.
ANSI C çalışmaya herhangi bir mikrodenetleyici kullanmadan başlayabilirsiniz. Bunun için internette ücretsiz derleyiciler bulunabiliyor. Bunlardan en yaygın kullanılan bir tanesi DEV-C’yi bu linkten indirebilirsiniz. Dev-C ile C çalışmalarınızı gerçekleştirerek dili iyice kavrayabilirsiniz.
-
-
-
ANSI C, ISO C and Standard C are successive standards for the C programming language published by the American National Standards Institute (ANSI) and the International Organization for Standardization (ISO). Historically, the names referred specifically to the original and best-supported version of the standard (known as C89 or C90). Software developers writing in C are encouraged to conform to the standards, as doing so helps portability between compilers.
-
-
-
-
ANSI
The American National Standards Institute (ANSI /ˈænsi/ AN-see) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States.[3] The organization also coordinates U.S. standards with international standards so that American products can be used worldwide.
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.[4]
The organization's headquarters are in Washington, D.C. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.
-
-
-
-





Bell Labs	(empty)  (empty) 
Bell labs nedir
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Bell Laboratuvarları (ayrıca Bell Labs ve geçmişte AT&T Bell Laboratuvarları ve Bell Telefon Laboratuvarları olarak bilinir), önceleri American Telephone & Telegraph Company (AT&T) 'nin şimdilerde ise Alcatel-Lucent 'in araştırma geliştirme kurumudur. Merkezi Murray Hill, New Jersey ABD'de bulunmaktadır.

 
Transistörler 1947'de Bell Laboratuvarları'nda icat edilmiştir
 
Murray Hill, New Jersey'deki Bell Laboratuvarları

En başarılı zamanlarında, Bell Laboratuvarları radyo astronomi, transistör, lazer, enformasyon teorisi, UNIX işletim sistemi, ve C programlama dili dahil çok çeşitli devrimsel teknolojiler geliştiren kendi türündeki ilk tesisti. Bell Laboratuvarlarında tamamlanan yedi esere Nobel Ödülü verilmiştir.
1937, Clinton J. Davisson "Deneysel olarak elektronların kristaller tarafından kırınımlarını" bulmasından dolayı George Paget Thomson ile Nobel Fizik Ödülünü paylaştı.
1956, John Bardeen, Walter H. Brattain, ve William Shockley"Yarı iletkenlere dair çalışmalarından ve transistör etkisini keşfinden dolayı" Nobel Fizik Ödülünü aldılar.
1977, Philip W. Anderson "Manyetik düzensiz sistemlerin elektronik yapısına dair temel teorik araştırmalarından dolayı" Nobel Fizik ödülünü Nevill Francis Mott ve John Hasbrouck Van Vleck ile paylaştı.
1978, Arno A. Penzias ve Robert W. Wilson "Kozmik mikrodalga arka plan ışımasını bulmalarından dolayı" Nobel Fizik Ödülünü aldılar.
1997, Steven Chu "Laser ışığıyla atomların soğutulması ve yakalanması için bir method geliştirdiğinden dolayı" Nobel Fizik Ödülünü Claude Cohen-Tannoudji ve William Daniel Phillips ile paylaştı.
1998, Horst Stormer, Robert Laughlin, ve Daniel Tsui çok küçük kuantum koridor etkisini bulmaları ve açıklamaları nedeniyle Nobel Fizik Ödülünü aldılar.
2009, Willard Boyle ve George E. Smith yarı iletken görüntüleme devresi (CCD sensörü) icat etmelerinden dolayı Nobel Fizik Ödülünü Charles K. Kao ile paylaştılar.-
-
-
-
-
Apache
Apache nedir
Apache is the most widely used web server software. Developed and maintained by Apache Software Foundation, Apache is an open source software available for free. It runs on 67% of all webservers in the world. It is fast, reliable, and secure. It can be highly customized to meet the needs of many different environments by using extensions and modules. Most WordPress hosting providers use Apache as their web server software. However, WordPress can run on other web server software as well.
Apache açık kaynak kodlu, güçlü, sağlam, yetenekli ve esnek bir http (web) sunucusudur. Apache Software Foundation (ASF) tarafından geliştirilir. ASF, Apache yazarları tarafından 1999’da yazılım için yasal bir şemsiye olması için oluşturulmuştur. Açık kaynak kodlu bir yazılımdır, lisansı ücretsizdir. Yazılım firmaları, kurumlara verdikleri hizmetten (kurulum, teknik destek, vb) kazanç sağlarlar.
Internetteki web sitelerinin %60’ı Apache üzerinde çalışmaktadır. Apache, en yakın rakibi Microsoft’un web sunucularının 3 katı pazar payına sahiptir.

Apache’yi Kim Kullanıyor?

Web siteleri Apache üzerinde çalışan farklı sektörlerden birkaç kurum:

Cumhurbaşkanlığı
Turkcell
Migros
Garanti Bankası
Sabah Gazetesi
Sabancı Üniversitesi
Orta Doğu Teknik Üniversitesi
AGB Anadolu Ajansı
Türkiye Odalar ve Borsalar Birliği
İstanbul Menkul Kıymetler Borsası
Oracle
Mercedes-Benz
Ericsson
Amazon
-
-
Apache Hadoop	(GITHUB)  (test) 
Hadoop nedir, apache hadoop nedir
Apache Hadoop is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Originally designed for computer clusters built from commodity hardware[3]—still the common use—it has also found use on clusters of higher-end hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.
-
-
-
-
Apache Hive     	(GITHUB) (test)
apache hive nedir, hive nedir
Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis.[3] Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop.[4] While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA).[5][6] Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.[7]
-
-
-
Apache Impala     	(GITHUB) (test)
apache Impala nedir, apache impala nedir, impala nedir
What is Impala?
Impala is a MPP (Massive Parallel Processing) SQL query engine for processing huge volumes of data that is stored in Hadoop cluster. It is an open source software which is written in C++ and Java. It provides high performance and low latency compared to other SQL engines for Hadoop.
In other words, Impala is the highest performing SQL engine (giving RDBMS-like experience) which provides the fastest way to access data that is stored in Hadoop Distributed File System.
Why Impala?
Impala combines the SQL support and multi-user performance of a traditional analytic database with the scalability and flexibility of Apache Hadoop, by utilizing standard components such as HDFS, HBase, Metastore, YARN, and Sentry.
•	With Impala, users can communicate with HDFS or HBase using SQL queries in a faster way compared to other SQL engines like Hive.
•	Impala can read almost all the file formats such as Parquet, Avro, RCFile used by Hadoop.
Impala uses the same metadata, SQL syntax (Hive SQL), ODBC driver, and user interface (Hue Beeswax) as Apache Hive, providing a familiar and unified platform for batch-oriented or real-time queries.
Unlike Apache Hive, Impala is not based on MapReduce algorithms. It implements a distributed architecture based on daemon processes that are responsible for all the aspects of query execution that run on the same machines.
Thus, it reduces the latency of utilizing MapReduce and this makes Impala faster than Apache Hive.
Advantages of Impala
Here is a list of some noted advantages of Cloudera Impala.
•	Using impala, you can process data that is stored in HDFS at lightning-fast speed with traditional SQL knowledge.
•	Since the data processing is carried where the data resides (on Hadoop cluster), data transformation and data movement is not required for data stored on Hadoop, while working with Impala.
•	Using Impala, you can access the data that is stored in HDFS, HBase, and Amazon s3 without the knowledge of Java (MapReduce jobs). You can access them with a basic idea of SQL queries.
•	To write queries in business tools, the data has to be gone through a complicated extract-transform-load (ETL) cycle. But, with Impala, this procedure is shortened. The time-consuming stages of loading & reorganizing is overcome with the new techniques such as exploratory data analysis & data discovery making the process faster.
•	Impala is pioneering the use of the Parquet file format, a columnar storage layout that is optimized for large-scale queries typical in data warehouse scenarios.
Features of Impala
Given below are the features of cloudera Impala −
•	Impala is available freely as open source under the Apache license.
•	Impala supports in-memory data processing, i.e., it accesses/analyzes data that is stored on Hadoop data nodes without data movement.
•	You can access data using Impala using SQL-like queries.
•	Impala provides faster access for the data in HDFS when compared to other SQL engines.
•	Using Impala, you can store data in storage systems like HDFS, Apache HBase, and Amazon s3.
•	You can integrate Impala with business intelligence tools like Tableau, Pentaho, Micro strategy, and Zoom data.
•	Impala supports various file formats such as, LZO, Sequence File, Avro, RCFile, and Parquet.
•	Impala uses metadata, ODBC driver, and SQL syntax from Apache Hive.
Relational Databases and Impala
Impala uses a Query language that is similar to SQL and HiveQL. The following table describes some of the key dfferences between SQL and Impala Query language.
Impala	Relational databases
Impala uses an SQL like query language that is similar to HiveQL.	Relational databases use SQL language.
In Impala, you cannot update or delete individual records.	In relational databases, it is possible to update or delete individual records.
Impala does not support transactions.	Relational databases support transactions.
Impala does not support indexing.	Relational databases support indexing.
Impala stores and manages large amounts of data (petabytes).	Relational databases handle smaller amounts of data (terabytes) when compared to Impala.
-
-
-
Apache Nutch	(GITHUB)  (test) 
Apache Nutch nedir
Apache Nutch is a highly extensible and scalable open source web crawler software project. Stemming from Apache Lucene, the project has diversified and now comprises two codebases, namely:
Nutch 1.x: A well matured, production ready crawler. 1.x enables fine grained configuration, relying on Apache Hadoop data structures, which are great for batch processing.
Nutch 2.x: An emerging alternative taking direct inspiration from 1.x, but which differs in one key area; storage is abstracted away from any specific underlying data store by using Apache Gora for handling object to persistent mappings. This means we can implement an extremely flexibile model/stack for storing everything (fetch time, status, content, parsed text, outlinks, inlinks, etc.) into a number of NoSQL storage solutions.
Being pluggable and modular of course has it's benefits, Nutch provides extensible interfaces such as Parse, Index and ScoringFilter's for custom implementations e.g. Apache Tika for parsing. Additionally, pluggable indexing exists for Apache Solr, Elastic Search, etc.
-
Apache Solr   (GITHUB)  (test)
apache solr nedir, solr nedir
Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites.
Solr (pronounced "solar") is an open-source enterprise-search platform, written in Java, from the Apache Lucene project. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features[2] and rich document (e.g., Word, PDF) handling. Providing distributed search and index replication, Solr is designed for scalability and fault tolerance.[3] Solr is widely used for enterprise search and analytics use cases and has an active development community and regular releases.
Solr runs as a standalone full-text search server. It uses the Lucene Java search library at its core for full-text indexing and search, and has REST-like HTTP/XML and JSON APIs that make it usable from most popular programming languages. Solr's external configuration allows it to be tailored to many types of applications without Java coding, and it has a plugin architecture to support more advanced customization.
Apache Lucene and Apache Solr are both produced by the same Apache Software Foundation development team.
-
-
-
Apache Spark     	(GITHUB) (test)
apache spark nedir, spark nedir
Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.

Overview
Apache Spark has its architectural foundation in the Resilient Distributed Dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.[2] The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. In Spark 1.x, the RDD was the primary application programming interface (API), but as of Spark 2.x use of the Dataset API is encouraged[3] even though the RDD API is not deprecated.[4][5] The RDD technology still underlies the Dataset API.[6][7]
Spark and its RDDs were developed in 2012 in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory.[8]
-
-
Metadata 	(GITHUB)  (read)
metadata nedir
Metadata is simply data about data. It means it is a description and context of the data. It helps to organize, find and understand data. Here are a few real world examples of metadata:
Those are some typical metadata elements:
1.	Title and description,
2.	Tags and categories,
3.	Who created and when,
4.	Who last modified and when,
5.	Who can access or update.
Every time you take a photo with today's cameras a bunch of metadata is gathered and saved with it:
•	date and time,
•	filename,
•	camera settings,
•	geolocation.
-
-
Redux  (GITHUB)  (test) 
Redux nedir
https://www.smashingmagazine.com/2018/07/redux-designers-guide/

Let’s say we are building a Dribbble shot page. What is the data we want to display on the page? They include the author’s profile photo, name, the animated GIF, the number of hearts, the comments, and so on.
 
Data on a Dribbble shot page (Large preview)
First, we need to fetch all these data from a server in the cloud and put it somewhere. Next, we need to actually display the data. We need to assign pieces of this data to corresponding UI elements that represent what we actually see in the browser. For example, we assign the URL of the profile photo to the src attribute of an HTML img tag:
<img src='https://url/to/profile_photo'>
Copy
Finally, we need to handle changes to the data. For example, if a user adds a new comment to a Dribbble shot, or adds a star, we need to update the HTML accordingly.
Coordinating these three aspects of state is a big part in front-end development, and React has various degree of support for this task. Sometimes the built-in facility in React works well enough. But as the app grows more complex, its state may become harder to manage with React alone. That’s why many people start using Redux as an alternative.
-
-
Apache Pig     	(GITHUB) (test)
pig nedir, apache pig nedir
Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.
-
-
In mathematics, a tuple is a finite ordered list (sequence) of elements. An n-tuple is a sequence (or ordered list) of n elements, where n is a non-negative integer. There is only one 0-tuple, referred to as the empty tuple. An n-tuple is defined inductively using the construction of an ordered pair.

In mathematics, a tuple is a finite ordered list (sequence) of elements. An n-tuple is a sequence (or ordered list) of n elements, where n is a non-negative integer. There is only one 0-tuple, referred to as the empty tuple. An n-tuple is defined inductively using the construction of an ordered pair.
Mathematicians usually write tuples by listing the elements within parentheses "( )" and separated by commas; for example, (2, 7, 4, 1, 7) denotes a 5-tuple. Sometimes other symbols are used to surround the elements, such as square brackets "[ ]" or angle brackets "⟨ ⟩". Braces "{ }" are only used in defining arrays in some programming languages but not in mathematical expressions, as they are the standard notation for sets. The term tuple can often occur when discussing other mathematical objects, such as vectors.
In computer science, tuples come in many forms. Most typed functional programming languages implement tuples directly as product types,[1] tightly associated with algebraic data types, pattern matching, and destructuring assignment.[2] Many programming languages offer an alternative to tuples, known as record types, featuring unordered elements accessed by label.[3] A few programming languages combine ordered tuple product types and unordered record types into a single construct, as in C structs and Haskell records. Relational databases may formally identify their rows (records) as tuples.
Tuples also occur in relational algebra; when programming the semantic web with the Resource Description Framework (RDF); in linguistics;[4] and in philosophy.[5]
-
-
C# 
(double, int) t1 = (4.5, 3);
Console.WriteLine($"Tuple with elements {t1.Item1} and {t1.Item2}.");
// Output:
// Tuple with elements 4.5 and 3.

(double Sum, int Count) t2 = (4.5, 3);
Console.WriteLine($"Sum of {t2.Count} elements is {t2.Sum}.");
// Output:
// Sum of 3 elements is 4.5.
As the preceding example shows, to define a tuple type, you specify types of all its data members and, optionally, the field names. You cannot define methods in a tuple type, but you can use the methods provided by .NET, as the following example shows:
C#Kopyala
Çalıştır
(double, int) t = (4.5, 3);
Console.WriteLine(t.ToString());
Console.WriteLine($"Hash code of {t} is {t.GetHashCode()}.");
// Output:
// (4.5, 3)
// Hash code of (4.5, 3) is 718460086.
Beginning with C# 7.3, tuple types support equality operators == and !=. For more information, see the Tuple equality section.
Tuple types are value types; tuple elements are public fields. That makes tuples mutable value types.
 Not
The tuples feature requires the System.ValueTuple type and related generic types (for example, System.ValueTuple<T1,T2>), which are available in .NET Core and .NET Framework 4.7 and later. To use tuples in a project that targets .NET Framework 4.6.2 or earlier, add the NuGet package System.ValueTuple to the project.
You can define tuples with an arbitrary large number of elements:
C#Kopyala
Çalıştır
var t = 
(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13, 14, 15, 16, 17, 18,
19, 20, 21, 22, 23, 24, 25, 26);
Console.WriteLine(t.Item26);  // output: 26
Use cases of tuples
One of the most common use cases of tuples is as a method return type. That is, instead of defining out method parameters, you can group method results in a tuple return type, as the following example shows:
C#Kopyala
Çalıştır
var xs = new[] { 4, 7, 9 };
var limits = FindMinMax(xs);
Console.WriteLine($"Limits of [{string.Join(" ", xs)}] are {limits.min} and {limits.max}");
// Output:
// Limits of [4 7 9] are 4 and 9

var ys = new[] { -9, 0, 67, 100 };
var (minimum, maximum) = FindMinMax(ys);
Console.WriteLine($"Limits of [{string.Join(" ", ys)}] are {minimum} and {maximum}");
// Output:
// Limits of [-9 0 67 100] are -9 and 100

(int min, int max) FindMinMax(int[] input)
{
    if (input is null || input.Length == 0)
    {
        throw new ArgumentException("Cannot find minimum and maximum of a null or empty array.");
    }

    var min = int.MaxValue;
    var max = int.MinValue;
    foreach (var i in input)
    {
        if (i < min)
        {
            min = i;
        }
        if (i > max)
        {
            max = i;
        }
    }
    return (min, max);
}
As the preceding example shows, you can work with the returned tuple instance directly or deconstruct it in separate variables.
You can also use tuple types instead of anonymous types; for example, in LINQ queries. For more information, see Choosing between anonymous and tuple types.
Typically, you use tuples to group loosely related data elements. That is usually useful within private and internal utility methods. In the case of public API, consider defining a class or a structure type.
Tuple field names
You can explicitly specify the names of tuple fields either in a tuple initialization expression or in the definition of a tuple type, as the following example shows:
C#Kopyala
Çalıştır
var t = (Sum: 4.5, Count: 3);
Console.WriteLine($"Sum of {t.Count} elements is {t.Sum}.");

(double Sum, int Count) d = (4.5, 3);
Console.WriteLine($"Sum of {d.Count} elements is {d.Sum}.");
Beginning with C# 7.1, if you don't specify a field name, it may be inferred from the name of the corresponding variable in a tuple initialization expression, as the following example shows:
C#Kopyala
Çalıştır
var sum = 4.5;
var count = 3;
var t = (sum, count);
Console.WriteLine($"Sum of {t.count} elements is {t.sum}.");
That's known as tuple projection initializers. The name of a variable isn't projected onto a tuple field name in the following cases:
•	The candidate name is a member name of a tuple type, for example, Item3, ToString, or Rest.
•	The candidate name is a duplicate of another tuple field name, either explicit or implicit.
In those cases you either explicitly specify the name of a field or access a field by its default name.
The default names of tuple fields are Item1, Item2, Item3 and so on. You can always use the default name of a field, even when a field name is specified explicitly or inferred, as the following example shows:
C#Kopyala
Çalıştır
var a = 1;
var t = (a, b: 2, 3);
Console.WriteLine($"The 1st element is {t.Item1} (same as {t.a}).");
Console.WriteLine($"The 2nd element is {t.Item2} (same as {t.b}).");
Console.WriteLine($"The 3rd element is {t.Item3}.");
// Output:
// The 1st element is 1 (same as 1).
// The 2nd element is 2 (same as 2).
// The 3rd element is 3.
Tuple assignment and tuple equality comparisons don't take field names into account.
At compile time, the compiler replaces non-default field names with the corresponding default names. As a result, explicitly specified or inferred field names aren't available at run time.
Tuple assignment and deconstruction
C# supports assignment between tuple types that satisfy both of the following conditions:
•	both tuple types have the same number of elements
•	for each tuple position, the type of the right-hand tuple element is the same as or implicitly convertible to the type of the corresponding left-hand tuple element
Tuple element values are assigned following the order of tuple elements. The names of tuple fields are ignored and not assigned, as the following example shows:
C#Kopyala
Çalıştır
(int, double) t1 = (17, 3.14);
(double First, double Second) t2 = (0.0, 1.0);
t2 = t1;
Console.WriteLine($"{nameof(t2)}: {t2.First} and {t2.Second}");
// Output:
// t2: 17 and 3.14

(double A, double B) t3 = (2.0, 3.0);
t3 = t2;
Console.WriteLine($"{nameof(t3)}: {t3.A} and {t3.B}");
// Output:
// t3: 17 and 3.14
You can also use the assignment operator = to deconstruct a tuple instance in separate variables. You can do that in one of the following ways:
•	Explicitly declare the type of each variable inside parentheses:
C#Kopyala
Çalıştır
var t = ("post office", 3.6);
(string destination, double distance) = t;
Console.WriteLine($"Distance to {destination} is {distance} kilometers.");
// Output:
// Distance to post office is 3.6 kilometers.
•	Use the var keyword outside the parentheses to declare implicitly typed variables and let the compiler infer their types:
C#Kopyala
Çalıştır
var t = ("post office", 3.6);
var (destination, distance) = t;
Console.WriteLine($"Distance to {destination} is {distance} kilometers.");
// Output:
// Distance to post office is 3.6 kilometers.
•	Use existing variables:
C#Kopyala
Çalıştır
var destination = string.Empty;
var distance = 0.0;

var t = ("post office", 3.6);
(destination, distance) = t;
Console.WriteLine($"Distance to {destination} is {distance} kilometers.");
// Output:
// Distance to post office is 3.6 kilometers.
For more information about deconstruction of tuples and other types, see Deconstructing tuples and other types.
Tuple equality
Beginning with C# 7.3, tuple types support the == and != operators. These operators compare members of the left-hand operand with the corresponding members of the right-hand operand following the order of tuple elements.
C#Kopyala
Çalıştır
(int a, byte b) left = (5, 10);
(long a, int b) right = (5, 10);
Console.WriteLine(left == right);  // output: True
Console.WriteLine(left != right);  // output: False

var t1 = (A: 5, B: 10);
var t2 = (B: 5, A: 10);
Console.WriteLine(t1 == t2);  // output: True
Console.WriteLine(t1 != t2);  // output: False
As the preceding example shows, the == and != operations don't take into account tuple field names.
Two tuples are comparable when both of the following conditions are satisfied:
•	Both tuples have the same number of elements. For example, t1 != t2 doesn't compile if t1 and t2 have different numbers of elements.
•	For each tuple position, the corresponding elements from the left-hand and right-hand tuple operands are comparable with the == and != operators. For example, (1, (2, 3)) == ((1, 2), 3) doesn't compile because 1 is not comparable with (1, 2).
The == and != operators compare tuples in short-circuiting way. That is, an operation stops as soon as it meets a pair of non equal elements or reaches the ends of tuples. However, before any comparison, all tuple elements are evaluated, as the following example shows:
C#Kopyala
Çalıştır
Console.WriteLine((Display(1), Display(2)) == (Display(3), Display(4)));

int Display(int s)
{
    Console.WriteLine(s);
    return s;
}
// Output:
// 1
// 2
// 3
// 4
// False
Tuples as out parameters
Typically, you refactor a method that has out parameters into a method that returns a tuple. However, there are cases in which an out parameter can be of a tuple type. The following example shows how to work with tuples as out parameters:
C#Kopyala
Çalıştır
var limitsLookup = new Dictionary<int, (int Min, int Max)>()
{
    [2] = (4, 10),
    [4] = (10, 20),
    [6] = (0, 23)
};

if (limitsLookup.TryGetValue(4, out (int Min, int Max) limits))
{
    Console.WriteLine($"Found limits: min is {limits.Min}, max is {limits.Max}");
}
// Output:
// Found limits: min is 10, max is 20
Tuples vs System.Tuple
C# tuples, which are backed by System.ValueTuple types, are different from tuples that are represented by System.Tuple types. The main differences are as follows:
•	ValueTuple types are value types. Tuple types are reference types.
•	ValueTuple types are mutable. Tuple types are immutable.
•	Data members of ValueTuple types are fields. Data members of Tuple types are properties.
-
-
-
-
HTML (HyperText Markup Language)             (GITHUB)  (test) 
Html nedir
Html web sayfaları hazırlamak için kullanılan bir işaretleme dilidir. Halihazırda kullandığımız web tarayıcıları (Internet Explorer, Firefox, Chrome v.b) bu kodları anlayarak görsel web sayfalarına dönüştürürler. Html bir programlama dili değildir.
Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript.
Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.
HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes and other items. HTML elements are delineated by tags, written using angle brackets. Tags such as <img /> and <input /> directly introduce content into the page. Other tags such as <p> surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page.
HTML can embed programs written in a scripting language such as JavaScript, which affects the behavior and content of web pages. Inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), former maintainer of the HTML and current maintainer of the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. 
-
-
UNIX		(GITHUB)
Unix nedir
Unix (/ˈjuːnɪks/; trademarked as UNIX) is a family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix, development starting in the 1970s at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[3]
-Unix vs linux yazılar var: https://chandigarhinfo.in/unix-vs-linux-whats-the-difference/
-
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
Linux 		(GITHUB)
Linux nedir

The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Bilgisayar işletim sistemlerinin en temel parçası olan çekirdek yazılımlarından bir tanesidir.verilmiştir.[1]Günümüzde süper bilgisayarlarda, akıllı cihazların ve internet altyapısında kullanılan cihazların işletim sistemlerinde yaygın olarak kullanılmaktadır. Bunlardan en popüler olanı Google tarafından geliştirilen Android işletim sistemidir.
Android, Linux çekirdeği üzerine inşa edilmiş bir mobil işletim sistemidir.
Android işletim sistemi beş kısımdan oluşur.
1.	Çekirdek: Linux kernelidir. Güvenlik, hafıza yönetimi, süreç yönetimi, ağ yığınları ve sürücü modellerini içermektedir.
2.	Android Runtime: Sanal makinedir. Dalvik Sanal Makinesini de içermektedir. 5.0 ile Dalvik kaldırılmış ve ART'ye geçilmiştir.
3.	Kütüphaneler: Veritabanı kütüphaneleri, web tarayıcı kütüphaneleri, grafik ve arayüz kütüphanelerini içermektedir.
4.	Uygulama Çatısı: Uygulama geliştiricilere geniş bir platform sunan kısımdır.
5.	Uygulama Katmanı: Doğrudan Java (programlama dili) ile geliştirilmiş uygulamaları içermektedir.
-
-
-
-
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
UNIX		(bosch)	 (bosch)
unix nedir
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
-






Smarty	(GITHUB)  (test)
Smarty nedir

Smarty is a template engine for PHP. More specifically, it facilitates a manageable way to separate application logic and content from its presentation. This is best described in a situation where the application programmer and the template designer play different roles, or in most cases are not the same person.
For example, let's say you are creating a web page that is displaying a newspaper article.
The article $headline, $tagline, $author and $body are content elements, they contain no information about how they will be presented. They are passed into Smarty by the application.
Then the template designer edits the templates and uses a combination of HTML tags and template tags to format the presentation of these variables with elements such as tables, div's, background colors, font sizes, style sheets, svg etc.
One day the programmer needs to change the way the article content is retrieved, ie a change in application logic. This change does not affect the template designer, the content will still arrive in the template exactly the same.
Likewise, if the template designer wants to completely redesign the templates, this would require no change to the application logic.
Therefore, the programmer can make changes to the application logic without the need to restructure templates, and the template designer can make changes to templates without breaking application logic.
One design goal of Smarty is the separation of business logic and presentation logic.
This means templates can certainly contain logic under the condition that it is for presentation only. Things such as including other templates, alternating table row colors, upper-casing a variable, looping over an array of data and displaying it are examples of presentation logic.
This does not mean however that Smarty forces a separation of business and presentation logic. Smarty has no knowledge of which is which, so placing business logic in the template is your own doing.
Also, if you desire no logic in your templates you certainly can do so by boiling the content down to text and variables only.

Some of Smarty's features:
It is extremely fast.
It is efficient since the PHP parser does the dirty work.
No template parsing overhead, only compiles once.
It is smart about recompiling only the template files that have changed.
You can easily create your own custom functions and variable modifiers, so the template language is extremely extensible.
Configurable template {delimiter} tag syntax, so you can use {$foo}, {{$foo}}, <!--{$foo}-->, etc.
The {if}..{elseif}..{else}..{/if} constructs are passed to the PHP parser, so the {if...} expression syntax can be as simple or as complex an evaluation as you like.
Allows unlimited nesting of sections, if's etc.
It is possible to embed PHP code right in your template files, although this may not be needed (nor recommended) since the engine is so customizable.
Built-in caching support
Arbitrary template sources
Custom cache handling functions
Plugin architecture
-
-
-
Ansible   	(empty)  (empty) 
[ansible nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Ansible is an open-source software provisioning, configuration management, and application-deployment tool enabling infrastructure as code.[2] It runs on many Unix-like systems, and can configure both Unix-like systems as well as Microsoft Windows. It includes its own declarative language to describe system configuration. Ansible was written by Michael DeHaan and acquired by Red Hat in 2015. Ansible is agentless, temporarily connecting remotely via SSH or Windows Remote Management (allowing remote PowerShell execution) to do its tasks.
-
-
Fedora             (GITHUB)  (test) 
Fedora nedir
Bir işletim sistemidir. Fedora (önceki adıyla Fedora Core), açık kaynak kodlu ve özgür bir Linux dağıtımı. Dünya çapında bir özgür yazılım topluluğu olan Fedeora Projesi tarafından geliştirilmekte ve yönetilmekte, Red Hat tarafından desteklenmektedir.
Red Hat'ın 2004 yılında sonlandırdığı Red Hat Linux dağıtımının devamı olarak adlandırılabilecek Fedora, Linux dünyasının önde gelen dağıtımlarından biridir. Fedora aynı zamanda Red Hat Enterprise Linux ürünü için bir test ortamı görevi görmektedir.
-
Fedora is a Linux distribution developed by the community-supported Fedora Project which is sponsored primarily by Red Hat, a subsidiary of IBM, with additional support from other companies.[11] Fedora contains software distributed under various free and open-source licenses and aims to be on the leading edge of free technologies.[12][13][14] Fedora is the upstream source of the commercial Red Hat Enterprise Linux distribution, and subsequently CentOS as well.
Since the release of Fedora 30, five different editions are currently available: Workstation, focused on the personal computer, Server for servers, CoreOS, focused on cloud computing, Silverblue, focused on an immutable desktop specialized to container-based workflows and IoT, focused on IoT devices.
As of February 2016, Fedora has an estimated 1.2 million users,[17] including Linus Torvalds (as of 2015), creator of the Linux kernel.
Features
Fedora has a reputation for focusing on innovation, integrating new technologies early on and working closely with upstream Linux communities.[14][20] Making changes upstream instead of specifically for Fedora ensures that the changes are available to all Linux distributions.
Fedora has a relatively short life cycle: each version is usually supported for at least 13 months, where version X is supported only until 1 month after version X+2 is released and with approximately 6 months between most versions.[21] Fedora users can upgrade from version to version without reinstalling.
The default desktop environment in Fedora is GNOME and the default user interface is the GNOME Shell. Other desktop environments, including KDE Plasma, Xfce, LXDE, MATE, Deepin and Cinnamon, are available and can be installed.
-
-
-
WWW     	(GITHUB)  (empty) 
[www nedir, world wide web nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
World Wide Web, Dünya Çapında Ağ (kısaca WWW veya Web), İnternet üzerinde yayınlanan birbirleriyle bağlantılı hiper-metin dokümanlarından oluşan bir bilgi sistemidir. Bu dokümanların her birine Web sayfası adı verilir ve Web sayfalarına İnternet kullanıcısının bilgisayarında çalışan Web tarayıcısı adı verilen bilgisayar programları aracılığıyla erişilir. Web sayfalarında metin, imaj, video ve diğer multimedya ögeleri bulunabilir ve diğer bağlantı ya da link adı verilen hiper-bağlantılar ile başka Web sayfalarına geçiş yapılabilir.

İnternet ve Web terimleri aynı olguyu tanımlamaz. Zira Web sadece İnternet üzerinde çalışan bir servistir. Web kavramı, CERN'de bir bilgisayar programcısı olan Tim Berners-Lee'nin HTML adlı metin işaretleme dilini geliştirmesiyle oluşmuştur. Bugün de kendisinin başkanı olduğu W3C (World Wide Web Consortium) tarafından standartları belirlenmektedir.

Yapısı
Web’in temeli İnternet'tir. Web İnternet üzerinde kurulmuştur ve İnternet'in sunduğu mekanizmalardan çoğunun kullanılmasını sağlar. İnternet'in fiziksel görünüşleri –bilgisayarlar, ağlar ve servisler– Dünya üzerindeki diğer binlerce bilgisayara bağlanmamıza izin verir. Web, İnternet'in en tepesindeki soyutlanmış genel servisler kümesidir. World Wide Web (W3), insanların fikir ve projelerinin paylaşılmasını sağlayan bir bilgi ve kültür havuzudur. İstemci-sunucu uygulamaları ile yapılan birçok organizasyon üzerinde Web tarayıcıları istemci olarak çalışabilirler. Web yürütümü standart İstemci-sunucu modelini izler. Aşağıdaki şekilde gösterildiği gibi "Web tarayıcısı" adı verilen programı çalıştıran bir istemci bilgisayar ile Web sunucu yazılımı çalıştıran bir sunucu bilgisayar arasındaki etkileşime "istemci-sunucu" etkileşimi adı verilir. İstemci bilgisayar sunucudan HTTP'yi (Hypertext Transfer Protocol) ve İnternet mesaj standardı TCP/IP'yi kullanarak bir doküman ister ve sunucu istemcinin göstereceği dokümanı geri döndürür.-
-
-
-





What is Configuration Management?	(empty)  (empty) 
[Configuration management nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Configuration management is the process of maintaining software and computer systems (example servers, storage, networks) in a known, desired and consistent state. It also allows access to an accurate historical record of system state for project management and audit purposes.
System Administrators mostly perform repetitive tasks like installing servers, configuring those servers, etc. These professionals can automate this task, by writing scripts.
However, it is a difficult job when they are working on a massive infrastructure. The Configuration Management tool like a Puppet was introduced to resolve such issues.

-
-
-
-
Design language / Design Vocabulary          (GITHUB)  (test) 
Design language nedir
A design language or design vocabulary is an overarching scheme or style that guides the design of a complement of products or architectural settings.
The term “Design Language” is used to describe the overall visual design of a digital product. Chances are, you’ve already got a set of resources that you refer to as your Design Language. But what makes a set of UI elements a design language?
At first glance, it appears to be: an amalgamation of component designs with a consistent visual aesthetic and brand, typically accompanied by usage guidelines in the form of online documentation. This is all good, but there’s a tremendous amount missing from this picture; arguably the very things that make it a language in the first place.
Visual communication is a complicated beast, and in most cases we over-simplify it in order to ship a consistent UI. I’m as guilty as anyone in this. However, what we cannot ignore the fact that a visual interface is in fact a very sophisticated method of visual communication that warrants attention to detail and an understanding of how the elements of design are used to speak to our users.
For the purpose of this article, when I refer to “design elements,” I am speaking to foundational elements such as color, shape, line, contrast, repetition, economy, shadows, animation types, measurements of time, etc. If thinking in terms of Atomic Design, these are the sub-atomic particles.
What is a Language?
Let’s start with a cliché. The definition of a Language (Dictionary.com):
“[4.] any set or system of such symbols as used in a more or less uniform fashion by a number of people, who are thus enabled to communicate intelligibly with one another.”
“[5.] any system of formalized symbols, signs, sounds, gestures, or the like used or conceived as a means of communicating thought, emotion, etc.”
“[7.] communication of meaning in any way; medium that is expressive, significant, etc.”
Pretty clear here that in order to call a set of designs a “design language”, they need to have a formal, unified system of meanings as a way to intelligibly communicate with our users.
 
 
If we look to written and verbal communication as a framework to guide us, we can see an atomic structure and patterns emerge that parallel UI design.
1.	Words have meaning
2.	Punctuation has meaning
3.	Types of words (nouns, adjectives, and verbs) have meaning.
4.	Grouped words have meaning & give formal structure
5.	Sentences have meaning & give formal structure
6.	Paragraphs have meaning & give formal structure
From here it continues to grow and we define more complex, larger methods of communication above these, level such as how to structure scientific objectives (hypothesis, method, result), term papers (thesis statement, outline, content, conclusion), and a myriad of story structures.
It’s clear that languages are complex systems. Everything has meaning and intent, and the structures and ways in which we combine these structures expand upon their meanings or create new ones altogether.
 
 
Make rules, then follow them.
Clear and consistent rules are what make languages successful. Inconsistencies, ambiguity, or straight-up contradictions can make languages very hard to learn, teach, and understand. How many times have you been confused by the rule “I before E, except after C”?
 
 
And make sure your rules are logical, not confusing
Language as it relates to the visual interface
A component can be a combination of atoms and molecules, so we need to define the atomic design elements in a way that gives semantic meaning, just like languages define punctuation and word types. The foundation of creating a Visual Design Language is
•	Clearly defined semantics (and no, “error”, “warning”, “success”, and “info” aren’t nearly enough)
•	Thorough and mature mapping of core elements of design with clear purposes and meanings
•	A solid family of UI components and patterns that effectively support the semantics, and use design elements (based on their meanings) to support the meaning of the components
•	Thorough, comprehensive documentation about the visual communication system
 
 
Colors are a great example of semantics, and may have multiple meanings, or no meaning at all.
Your pattern library or style guide is the dictionary and thesaurus for your Visual Design Language. Documenting the core semantics, structures, and meanings behind the elements of design (color, line, shape, etc) will add a concrete foundation behind the decisions that are made in designing and the usage of components in your interface.
Give yourself a Litmus test
When you’re working on your pattern library, UI Kit, or however else you prepare your Visual Design Language, there are some ways to ensure your design is truly linguistic. Comb through every single element of the designs and ask yourself:
•	What does this color mean?
•	What does dimensionality mean (such shadows/bevels)?
•	What does the thickness of a border mean?
•	What does a rounded corner mean?
•	What do my fonts say/mean?
•	What does the use of italics mean?
A simple framework is to fill in the blanks of this statement:
The ______ used in the ______ helps to communicate _________.
The first item of the statement is a specific design element, the second line is the component or pattern you are analyzing, and the final item is the semantic.
Example: The line thickness used in the primary button component helps to communicate important interactive elements.
Most design elements may have no meaning at all when you create your first design language. But evolving your system to follow and support semantics of the most atomic or subatomic elements will help your system to mature into a true visual language.
-
-
-
-
Objectives
Designers wishing to give their suite of products a unique but consistent look and feel define a specification for it, which can describe choices for design aspects such as materials, colour schemes, shapes, patterns, textures, or layouts. They then follow the scheme in the design of each object in the suite.
Usually, design languages are not rigorously defined; the designer basically makes one thing in a similar manner as another. In other cases, they are followed strictly, so that the products gain a strong thematic quality. For example, although there is a great variety of unusual chess set designs, the pieces within a set are usually thematically consistent.
Sometimes, designers encourage others to follow their design languages when decorating or accessorizing.
Industrial design
In automobiles, the design language is often in the grille design. For instance, many BMW vehicles share a design language,[1] including front-end styling consisting of a split "kidney grille" and four circular headlights. Some manufacturers have appropriated design language cues from rival firms.
Software
In software architecture, design languages are related to architecture description languages. The most well known design language is Unified Modeling Language.[citation needed]
In the context of graphical user interfaces, for example, human interface guidelines can be thought of as design languages for applications.
Examples
Apple
•	Snow White
•	Platinum
•	Aqua
•	Flat Design
Microsoft
•	Metro
•	Fluent Design System
•	Windows Aero
Google
•	Material Design
Automotive
Cadillac
•	Art and Science
Ford
•	New Edge
•	Kinetic Design
Mazda
•	Nagare
•	Kodo[4]
Mitsubishi
•	Dynamic Shield[5]
Subaru
•	Dynamic x Solid[6]-
-
-





Chef     	(empty)  (empty) 
[chef nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Chef is a company and the name of a configuration management tool written in Ruby and Erlang. It uses a pure-Ruby, domain-specific language (DSL) for writing system configuration "recipes". Chef is used to streamline the task of configuring and maintaining a company's servers, and can integrate with cloud-based platforms such as Internap, Amazon EC2, Google Cloud Platform, Oracle Cloud, OpenStack, SoftLayer, Microsoft Azure, and Rackspace to automatically provision and configure new machines. Chef contains solutions for both small and large scale systems, with features and pricing for the respective ranges.
-
-







C     	(empty)  (empty) 
c nedir, c programlama dili nedir, c dili nedir
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
AT&T Bell laboratuvarlarında, Ken Thompson ve Dennis Ritchie tarafından UNIX İşletim Sistemi' ni geliştirebilmek amacıyla B dilinden türetilmiş yapısal bir programlama dilidir. Geliştirilme tarihi 1972 olmasına rağmen yayılıp yaygınlaşması Brian Kernighan ve Dennis M. Ritchie tarafından yayımlanan "C Programlama Dili" kitabından sonra hızlanmıştır. 

Günümüzde neredeyse tüm işletim sistemlerinin (Microsoft Windows, GNU/Linux, *BSD, Minix) yapımında %95' lere varan oranda kullanılmış, hâlen daha sistem, sürücü yazılımı, işletim sistemi modülleri ve hız gereken her yerde kullanılan oldukça yaygın ve sınırları belirsiz oldukça keskin bir dildir.[kaynak belirtilmeli]" Keskinliği, programcıya sonsuz özgürlüğün yanında çok büyük hatalar yapabilme olanağı sağlamasıdır. Programlamanın gelişim süreciyle beraber programlamanın karmaşıklaşması, gereksinimlerin artması ile uygulama programlarında nesne yönelimliliğin ortaya çıkmasından sonra C programcıları büyük ölçüde nesne yönelimliliği destekleyen C++ diline geçmişlerdir.
 
C Dilinin erken tarihi
C'nin ilk gelişme safhaları 1969 ile 1974 arasında AT&T Bell Laboratuvarları'nda gerçekleşti. Ritchie'ye göre, en yaratıcı devre 1972 idi. Dilin pek çok özelliği "B" adlı bir dilden türediği için, yeni dile "C" adı verildi. B dili yorumlanan bir dildi ve veri tipi desteği yoktu. Yeni donanımların farklı veri tiplerini desteklemesi, ve yorumlanan dillerin çalışma zamanında görece yavaş olması sebebi ile, C dili, tip desteği eklenmiş ve derlenen B olarak geliştirildi.
"B" adının kökeni konusunda ise söylentiler değişik: Ken Thompson B'nin BCPL programlama dilinden türediğini söylemektedir, ancak Thompson eşi Bonnie'nin onuruna adını Bon koyduğu bir programlama dili de geliştirmiştir.
1973'e kadar C yeterince güçlü bir hale gelmiş ve ilk başta PDP-11/20 assembly dili ile yazılan UNIX'in çekirdeğinin büyük kısmı C ile yeniden yazılmıştı. 
(Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir.)
Böylece UNIX, çekirdeği bir assembly dili ile yazılmayan ilk işletim sistemlerinden biri olmuştu.
-
-
-
-
-
Coder vs Developer              (GITHUB)  (test) 
Coder nedir, developer nedir
Anyone who can write some code is often referred to as a coder by the people outside of the tech industry. But, usually, coders are considered the least trained or experienced level of programmers. These individuals do not have the same algorithmic knowledge as a programmer or developer, as they are often a beginner in the field, skilled in just one coding language. Coders are usually given the job of writing forthright pieces of code that can easily be delegated by the developers. As some are put-off by the title, it is sometimes used interchangeably with “Junior Programmer” or “Junior Developer.”
-
-
-
-
Web Server	 (GITHUB)
Web server nedir
Web server ya da ağ sunucusu, internet üzerinde bir web sitesinin yayınından sorumlu olan sunucudur. Web server, Hosting ya da “barındırma” işlemini internet protokolü üzerinden sunan bir sunucudur. Barındırma ya da hosting, Web sayfalarını internette yayınlamak için gerekli alanın kiralanmasıdır. Diğer bir ifade ile hosting, bir Web sitesinde yayınlanmak istenen sayfaların, resimlerin veya dokümanların internet kullanıcıları tarafından erişebileceği bir bilgisayarda tutulmasıdır.
A Web server is software or hardware that uses HTTP and other protocols to respond to client requests made over the World Wide Web (WWW). Web server software controls how a user accesses hosted files. It is accessed through the domain names of websites and ensures the delivery of the site's content to the requesting user. As hardware, a Web server is a computer that holds web server software and other files related to a website, such as HTML documents, images and JavaScript files. Web server hardware is connected to the internet and allows data to be exchanged with other connected devices.
The Web server process is an example of the client/server model. All computers that host Web sites must have Web server software. 
Leading Web servers include Apache, Microsoft's Internet Information Server (IIS) and Nginx -- pronounced engine X. Other Web servers include Novell's NetWare server, Google Web Server (GWS) and IBM's family of Domino servers.
Web servers often come as part of a larger package of internet- and intranet-related programs that are used for:
•	Sending and receiving emails.
•	Downloading requests for File Transfer Protocol (FTP) files.
•	Building and publishing Web pages.
Considerations in choosing a Web server include how well it works with the operating system and other servers; its ability to handle server-side programming; security characteristics; and the particular publishing, search engine and site building tools that come with it.
So basically a web server is the software that receives your request to access a web page. It runs a few security checks on your HTTP request and takes you to the web page. Depending on the page you have requested, the page may ask the server to run a few extra modules while generating the document to serve you. It then serves you the document you requested. Pretty awesome isn’t it.
-
A web server is server software, or hardware dedicated to running this software, that can satisfy client requests on the World Wide Web. A web server can, in general, contain one or more websites. A web server processes incoming network requests over HTTP and several other related protocols.
The primary function of a web server is to store, process and deliver web pages to clients.[1] The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently HTML documents, which may include images, style sheets and scripts in addition to the text content.
 
Multiple web servers may be used for a high traffic website; here, Dell servers are installed together being used for the Wikimedia Foundation.
A user agent, commonly a web browser or web crawler, initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage, but this is not necessarily the case and depends on how the web server is implemented.
While the major function is to serve content, a full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms, including uploading of files.
Many generic web servers also support server-side scripting using Active Server Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages. This means that the behaviour of the web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically ("on-the-fly") as opposed to returning static documents. The former is primarily used for retrieving or modifying information from databases. The latter is typically much faster and more easily cached but cannot deliver dynamic content.
-
-
Type System (örnek) (örnek)
type system nedir
In programming languages, a type system is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules.[1] These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. "string", "array of float", "function returning boolean"). 
The main purpose of a type system is to reduce possibilities for bugs in computer programs[2] by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of both. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.
A type system associates a type with each computed value and, by examining the flow of these values, attempts to ensure or prove that no type errors can occur. The given type system in question determines what constitutes a type error, but in general the aim is to prevent operations expecting a certain kind of value from being used with values for which that operation does not make sense (logic errors). Type systems are often specified as part of programming languages and built into interpreters and compilers, although the type system of a language can be extended by optional tools that perform added checks using the language's original type syntax and grammar.
Usage overview
An example of a simple type system is that of the C language. The portions of a C program are the function definitions. One function is invoked by another function. The interface of a function states the name of the function and a list of values that are passed to the function's code. The code of an invoking function states the name of the invoked, along with the names of variables that hold values to pass to it. During execution, the values are placed into temporary storage, then execution jumps to the code of the invoked function. The invoked function's code accesses the values and makes use of them. If the instructions inside the function are written with the assumption of receiving an integer value, but the calling code passed a floating-point value, then the wrong result will be computed by the invoked function. The C compiler checks the type declared for each variable sent, against the type declared for each variable in the interface of the invoked function. If the types do not match, the compiler throws a compile-time error.
A compiler may also use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).
The depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with various resolutions for each type, in the case of type polymorphism. Type theory is the study of type systems. The concrete types of some programming languages, such as integers and strings, depend on practical issues of computer architecture, compiler implementation, and language design.
Fundamentals
Formally, type theory studies type systems. A programming language must have occurrence to type check using the type system whether at compile time or runtime, manually annotated or automatically inferred. As Mark Manasse concisely put it:[3]
The fundamental problem addressed by a type theory is to ensure that programs have meaning. The fundamental problem caused by a type theory is that meaningful programs may not have meanings ascribed to them. The quest for richer type systems results from this tension.
Assigning a data type, termed typing, gives meaning to a sequence of bits such as a value in memory or some object such as a variable. The hardware of a general purpose computer is unable to discriminate between for example a memory address and an instruction code, or between a character, an integer, or a floating-point number, because it makes no intrinsic distinction between any of the possible values that a sequence of bits might mean.[note 1] Associating a sequence of bits with a type conveys that meaning to the programmable hardware to form a symbolic system composed of that hardware and some program.

-
Operations Support Systems (OSS)	(empty)  (empty) 
[OSS nedir, Operations Support Systems nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Operations Support Systems (OSS), or Operational Support Systems in British usage,[1] are computer systems used by telecommunications service providers to manage their networks (e.g., telephone networks). They support management functions such as network inventory, service provisioning, network configuration and fault management.
Together with Business Support Systems (BSS), they are used to support various end-to-end telecommunication services. BSS and OSS have their own data and service responsibilities. The two systems together are often abbreviated OSS/BSS, BSS/OSS or simply B/OSS.
The acronym OSS is also used in a singular form to refer to all the Operations Support Systems viewed as a whole system.
Different subdivisions of OSS have been proposed by the TM Forum, industrial research labs or OSS vendors. In general, an OSS covers at least the following five functions:
•	Network management systems
•	Service delivery
•	Service fulfillment, including the network inventory, activation and provisioning
•	Service assurance
•	Customer care
-
NPM	(GITHUB) (örnek)
npm  nedir
npm, short for Node Package Manager, is two things: first and foremost, it is an online repository for the publishing of open-source Node.js projects; second, it is a command-line utility for interacting with said repository that aids in package installation, version management, and dependency management. A plethora of Node.js libraries and applications are published on npm, and many more are added every day. These applications can be searched for on http://npmjs.org/. Once you have a package you want to install, it can be installed with a single command-line command.
 
npm is the world's largest Software Library (Registry)
npm is also a software Package Manager and Installer
________________________________________
The World's Largest Software Registry (Library)
npm is the world's largest Software Registry.
The registry contains over 800,000 code packages.
Open-source developers use npm to share software.
Many organizations also use npm to manage private development.
________________________________________
Using npm is Free
npm is free to use.
You can download all npm public software packages without any registration or logon.
________________________________________
Command Line Client
npm includes a CLI (Command Line Client) that can be used to download and install software:
Windows Example
C:\>npm install <package>
Mac OS Example
>npm install <package>
________________________________________
Installing npm
npm is installed with Node.js
This means that you have to install Node.js to get npm installed on your computer.
Download Node.js from the official Node.js web site: https://nodejs.org
________________________________________
Software Package Manager
The name npm (Node Package Manager) stems from when npm first was created as a package manager for Node.js.
All npm packages are defined in files called package.json.
The content of package.json must be written in JSON.
At least two fields must be present in the definition file: name and version.
Example
{
"name" : "foo",
"version" : "1.2.3",
"description" : "A package for fooing things",
"main" : "foo.js",
"keywords" : ["foo", "fool", "foolish"],
"author" : "John Doe",
"licence" : "ISC"
}
________________________________________
Managing Dependencies
npm can manage dependencies.
npm can (in one command line) install all the dependencies of a project.
Dependencies are also defined in package.json.
________________________________________
Sharing Your Software
If you want to share your own software in the npm registry, you can sign in at:
https://www.npmjs.com
________________________________________
Publishing a Package
You can publish any directory from your computer as long as the directory has a package.json file.
Check if npm is installed:
C:\>npm
Check if you are logged in:
C:\>npm whoami
If not, log in:
C:\>npm login
Username: <your username>
Password: <your password>
Navigate to your project and publish your project:
C:\Users\myuser>cd myproject
C:\Users\myuser\myproject>npm publish
-

Let's say you're hard at work one day, developing the Next Great Application. You come across a problem, and you decide that it's time to use that cool library you keep hearing about - let's use Caolan McMahon's async as an example. Thankfully, npm is very simple to use: you only have to run npm install async, and the specified module will be installed in the current directory under ./node_modules/. Once installed to your node_modules folder, you'll be able to use require() on them just like they were built-ins.
Let's look at an example of a global install - let's say coffee-script. The npm command is simple: npm install coffee-script -g. This will typically install the program and put a symlink to it in /usr/local/bin/. This will then allow you to run the program from the console just like any other CLI tool. In this case, running coffee will now allow you to use the coffee-script REPL.
Another important use for npm is dependency management. When you have a node project with a package.json file, you can run npm install from the project root and npm will install all the dependencies listed in the package.json. This makes installing a Node.js project from a git repo much easier! For example, vows, a Node.js testing framework, can be installed from git, and its single dependency, eyes, can be automatically handled:
Example:
git clone https://github.com/cloudhead/vows.git
cd vows
npm install
After running those commands, you will see a node_modules folder containing all of the project dependencies specified in the package.json.
-
-

Whats the difference between push , commit & fetch ,merge
Git commit basically “records changes to the local repository” while git push “updates remote refs along with associated objects”. So the first one is used in connection with your local repository, while the latter one is used to interact with a remote repository.
git fetch really only downloads new data from a remote repository — but it doesn’t integrate any of this new data into your working files. 
git pull, in contrast, is used to to update your current HEAD branch with the latest changes from the remote server. This means that pull not only downloads new data; it also directly integrates it into your current working copy files.

These three commands have entirely different purposes. They are not even remotely similar.
git revert
This command creates a new commit that undoes the changes from a previous commit. This command adds new history to the project (it doesn't modify existing history).
git checkout
This command checks-out content from the repository and puts it in your work tree. It can also have other effects, depending on how the command was invoked. For instance, it can also change which branch you are currently working on. This command doesn't make any changes to the history.
git reset
This command is a little more complicated. It actually does a couple of different things depending on how it is invoked. It modifies the index (the so-called "staging area"). Or it changes which commit a branch head is currently pointing at. This command may alter existing history (by changing the commit that a branch references).
Using these commands
If a commit has been made somewhere in the project's history, and you later decide that the commit is wrong and should not have been done, then git revert is the tool for the job. It will undo the changes introduced by the bad commit, recording the "undo" in the history.
If you have modified a file in your working tree, but haven't committed the change, then you can use git checkout to checkout a fresh-from-repository copy of the file.
If you have made a commit, but haven't shared it with anyone else and you decide you don't want it, then you can use git reset to rewrite the history so that it looks as though you never made that commit.
These are just some of the possible usage scenarios. There are other commands that can be useful in some situations, and the above three commands have other uses as well.

-
-
-
GUI 			(GITHUB)
GUI nedir, gui nedir
 
A graphical user interface (GUI) is a type of user interface through which users interact with electronic devices via visual indicator representations.

What is a Graphical User Interface?
The graphical user interface, developed in the late 1970s by the Xerox Palo Alto research laboratory and deployed commercially in Apple’s Macintosh and Microsoft’s Windows operating systems, was designed as a response to the problem of inefficient usability in early, text-based command-line interfaces for the average user.
Graphical user interfaces would become the standard of user-centered design in software application programming, providing users the capability to intuitively operate computers and other electronic devices through the direct manipulation of graphical icons such as buttons, scroll bars, windows, tabs, menus, cursors, and the mouse pointing device. Many modern graphical user interfaces feature touchscreen and voice-command interaction capabilities.
How Does a Graphical User Interface Work?
Graphical user interface design principles conform to the model–view–controller software pattern, which separates internal representations of information from the manner in which information is presented to the user, resulting in a platform where users are shown which functions are possible rather than requiring the input of command codes. Users interact with information by manipulating visual widgets, which are designed to respond in accordance with the type of data they hold and support the actions necessary to complete the user’s task.

The appearance, or “skin,” of an operating system or application software may be redesigned at will due to the nature of graphical user interfaces being independent from application functions. Applications typically implement their own unique graphical user interface display elements in addition to graphical user interface elements already present on the existing operating system. A typical graphical user interface also includes standard formats for representing graphics and text, making it possible to share data between applications running under common graphical user interface design software.

Graphical user interface testing refers to the systematic process of generating test cases in order to evaluate the functionality of the system and its design elements. Graphical user interface testing tools, which are either manual or automated and typically implemented by third-party operators, are available under a variety of licenses and are supported by a variety of platforms. Popular examples include: Tricentis Tosca, Squish GUI Tester, Unified Functional Testing (UFT), Maveryx, Appium, and eggPlant Functional.
Graphical User Interface Examples
Sketchpad, believed to be the first graphical computer-aided design program, was developed in 1962 by Ivan Sutherland while he was at MIT, and consisted of a light pen that enabled users to create and manipulate objects in engineering drawings in real time with coordinated graphics.
Modern operating systems and graphical user interfaces are incorporated into nearly every interactive application, such as ATMs, self-service checkouts, airline self-ticketing and check-in, video games, smartphones, and desktops. Some popular, modern graphical user interface examples include Microsoft Windows, macOS, Ubuntu Unity, and GNOME Shell for desktop environments, and Android, Apple's iOS, BlackBerry OS, Windows 10 Mobile, Palm OS-WebOS, and Firefox OS for smartphones.
Advantages of Graphical User Interfaces
The advantage of a graphical user interface is a stark improvement in useability for the average person. The features of a graphical user interface leverage familiar metaphors, such as drag-and-drop for transferring files, and use familiar icons, such as a trash bin for deleted files, creating an environment in which computer operations are intuitive and easily mastered without any prior practice or knowledge of computing machinery or languages. Graphical user interface applications are self descriptive, feedback is typically immediate, and visual cues encourage and steer discoverability.
Best Programming Language for Graphical User Interfaces
While there are several different visual programming languages with their own unique advantages for the development of a graphical user interface design, C# or Java may be considered preferable options due to their ability to run GUIs simultaneously in a browser and as a desktop application. Other options include Python, HTML5/Javascript, and C/C++.
Difference Between Character User Interface and Graphical User Interface
Character user interface, also known as command-line user interface or non graphical user interface, refers to the use of text commands, managed by a command-line interpreter, in order to communicate with a computer program. Typically software developers and system administrators rely on command-line interfaces to configure machines, manage computer files, and access program features that are otherwise unavailable on a graphical user interface.
Character user interfaces support automation and scripting and tend to provide greater granular control and a higher level of functionality than graphical user interfaces. While the character user interface was the primary method of operating computers through the 1980s, most modern electronic devices are equipped with intuitive graphical user interfaces and the average user will rarely if ever have cause to access a computer terminal.
Difference Between Web User Interface and Graphical User Interface
A web user interface, or web-based graphical user interface, refers to the interaction between a user and software running on a web server wherein the user interface is the web browser and the web page it downloaded and rendered. Technologies such as Flash, Java, JavaScript, and Silverlight enable interactions such as drag-and-drop, playing audio, drawing on the screen, and access to the keyboard and mouse.
Web graphical user interfaces are platform independent, require no installation or separate software development, easy to update and monitor due to the nature of not being dependent upon the user to deploy updates, provides a vibrant UI experience, and are low cost, requiring only Ethernet or WiFi interface connectivity.
Does OmniSci Offer Graphical User Interface Solutions?
The benefits of visualizations in computing are evident in the intuitive nature of graphical user interfaces. Visualization and interactivity are similarly beneficial elements in data analytics. OmniSci Immerse is a browser-based, interactive data visualization client that works seamlessly with the OmniSci server-side technologies, OmniSciDB and Render, providing an interactive, visual platform that reduces the time to insights and dramatically expands an analyst's ability to find previously hidden insights.
-
IT Nedir? |
IT Uzmanı Nedir?
Gün geçtikçe hızla gelişen ve büyüyen bilişim ve teknoloji dünyası, bilişimi ve teknolojiyi daha etkin, daha aktif kılmak için zamanla profesyonel, alanında uzman kişilere ihtiyaç duymaya başlamış, bu sayede kısaca IT denilen Enformasyon Teknolojileri (Information Technologies) programını ortaya çıkarmıştır. 

IT Nedir?
 
IT (Information Technology)'nin yani Enformasyon Teknolojileri'nin ne olduğunu öğrenmeden önce enformasyon kelimesine bakmamızda fayda var.  Enformasyon,  bilgi ve veri ile karıştırılsa da, genel anlamda, belirli ve dar kapsamdaki bir konuyu ölçüm, deney, gözlem, araştırma ve istihbarat sonuçlarının derlenmesi ile ortaya çıkan bir bilgi parçasıdır. Dolayısıyla birçok verinin toplanması sonucu oluşan enformasyonlar da bir araya gelerek bilgiyi oluştururlar.
 
 
Enformasyon Teknolojileri ise, büyüyen bilişim teknolojileri ile bilişim alanındaki bütün ihtiyaçlara çözüm üreten, kayıt, depolama, sorgulama, düzenleme ve özetleme süreçlerinden geçerek toplanan verileri işleyerek enformasyona dönüştürmeyi sağlayan ve bu bağlamda problemleri çözmeyi ve karar vermeyi sağlayan bir programdır. Daha basit bir ifadeyle, bilişim sektöründeki sorunlara, ihtiyaçlara, eksikliklere çözüm bulmayı sağlayan uzmanları yetiştirmeyi hedefleyen bir programdır. 
 
IT Tarihi ve Önemi
 
IT'nin başlangıcı ve gelişim süreci Elektronik Mühendisliği'ne dayanır. Ayrıca  bilgisayar teknolojisinin gelişmesi ile büyük önem kazanan Bilgisayar Mühendisliği, kuramsal altyapı ile Bilgisayar Bilimi ve bilgisayar teknolojilerinde daha spesifik bir alana sahip olan Yazılım Mühendisliği'nin  gelişmesinin sonucunda bu alanlar sayesinde IT'nin temelleri ortaya atılmıştır. 

Bilgisayar teknolojilerinin gelişimi ve üretiminden çok kullanımı ve ihtiyaçlara cevap vermesi önemli olan ülkelerde, bilgisayar teknolojilerinin ne kadar etkin ve geniş kullanıldığı önemlidir. Buna bağlı olarak bu etkinliği ve genişliği artırmak için Enformasyon Teknolojileri'ne verilen önem artmıştır ve böylece bilişim ve teknoloji dünyasında hızla yayılan IT, zamanla şirketlerin organizasyonlarında olmazsa olmaz bir departman haline gelmiş, buna bağlı olarak IT Uzmanlığı olarak bilinen bir iş alanı ortaya çıkarmıştır. 
 
IT Uzmanı Ne İş Yapar?
 
IT uzmanları şirketlerin, bilişim ve teknoloji alanındaki sorunlarını tespit ederek, teknolojik imkanlar ve yetenekler sayesinde sorunlara çözüm bulur, ihtiyaçları karşılar. Bu özellikleri itibariyle IT Uzmanları, bilişim araçlarına, sistemlere, teknolojik gelişmelere hakimdir ve açıktır.  
 
IT Uzmanları, aldıkları eğitimler ve yetkin teknolojiler ile büyük bilişim projelerinin tasarlanmasında ve idaresinde, bilgisayar sistemlerinin kurulması ve yönetiminde, bilgi güvenliğinin sağlanmasında ve veritabanları konusunda profesyonelleşirler.

IT Uzmanları aynı zamanda matematiksel düşünme ve analiz konusunda yetenekli, sorumluluk duygusuna sahip, işbirliğine ve takım çalışmalara yatkın insanlardır. Bilişim ve teknolojinin getirdiği yenilikleri takip eder, bu yenilikleri en yararlı biçimde uygulamaya, kullanmaya çalışır. 
 
IT Uzmanı Çalışma Alanları ve İş Fırsatları 
 
IT Uzmanları, şirketlerin bilgisayar, telekomünikasyon ve elektronik teknolojilerini kullanmaya başlaması ile birçok konuda iş sahibi olabilmekte ve bu sistemlerin gelişmesi ve her geçen gün yenilenmesiyle yeni iş alanları ortaya çıkmaktadır. IT Uzmanlarının çalışabileceği bu alanlardan bazıları şunlardır:
 
► Bilişim Sistemleri Yöneticisi              ►Bilgisayar Destek Analisti
► Bilgisayar Sistem Analisti                   ►Yazılım Proje Lideri
►Yazılım Geliştiriciliği                          ►Kurumsal Mimari Uzmanı 
► Bilgi Güvenliği Uzmanlığı                   ►Ağ Sistemleri Yöneticiliği
► Veri Tabanı Yöneticiliği                      ►İletişim Güvenliği Uzmanı

Birçok ülkede IT alanına yapılan yatırım yıldan yıla artmakta, bu da bu alana ne kadar önem verildiğini göstermektedir. Dünyada IT hizmetlerine yapılan yatırımlar 2012 yılında 881 milyar doları, 2013 yılında ise 927 milyar doları bulmuştur. Ayrıca bilgisayar teknolojileri ve tasarımını içeren hizmetlerde istihdam oranı büyük bir hızla artmaktadır.  Bu gelişmeler ve büyümeler IT hizmetlerine verilen önemi büyük derecede göstermektedir.

-
-
-
SharePoint 	(GITHUB)		    (read)
sharepoint nedir
There are the Windows SharePoint Services (WSS), the Microsoft Office SharePoint Server (MOSS) and Microsoft Search Server. Each bring their own functionalities to the table and build upon each other. 
When looking up what SharePoint is, you get a lot of different answers from the Internet:
•	Microsoft SharePoint is a browser-based collaboration and document management platform from Microsoft - Wikipedia
•	Microsoft's content management system. It allows groups to set up a centralized, password protected space for document sharing. Documents can be stored, downloaded and edited, then uploaded for continued sharing. - SAIT Polytechnic Teaching - Glossary
•	SharePoint is a web-based intranet that can help improve your organization's effectiveness by streamlining the management of and access to data. - Creative SharePoint
•	SharePoint is an enterprise information portal, from Microsoft, that can be configured to run Intranet, Extranet and Internet sites. - SharePoint HQ
•	{..} SharePoint is a sort of sharing/blogging/wiki-style server that mainly provides a back end to Microsoft Office - Jack Schofield, Technical Writer Guardian newspaper, UK.
So what is SharePoint? Let's first look at it from a technical perspective. There are the Windows SharePoint Services (WSS), the Microsoft Office SharePoint Server (MOSS) and Microsoft Search Server. Each bring their own functionality to the table and build upon each other.
Windows SharePoint Services function the basic functionality that Gartner calls Basic Content Services. It offers the user access to Versioning and Check-in/Checkout functionality. WSS can then be extended through the use of the applets to add additional collaborative functions like email alerts, shared calendars, etc.
Microsoft Office SharePoint Server (MOSS) sits on top of WSS and adds additional features, both to the underlying WSS layer as well as additional ways for the end-user to interact with the repository. The data resides in a SQL Database. Using Webparts it can then present this information to the user using a wide range of different modules and for interaction in different business scenarios. These include such diverse applications as Collaboration, Document management, Records Management (including DOD 5015.2 certified components), Workflow, Personalization, more complex metadata models as well as Blogs and Wikis from the Web 2.0 side.Open Sanshttps://www.youtube.com/watch?time_continue=104&v=s12Jb5Z2xaE&feature=emb_logoOpen Sanshttps://medium.com/@serdalkepil/sharepoint-nedir-neden-kullan%C4%B1l%C4%B1r-79a1e8426599
-
-
-
-
Java Servlets     	(GITHUB) (test)
java servlets nedir, servlets nedir
A servlet is a Java programming language class that is used to extend the capabilities of servers that host applications accessed by means of a request-response programming model. Although servlets can respond to any type of request, they are commonly used to extend the applications hosted by web servers. For such applications, Java Servlet technology defines HTTP-specific servlet classes.
Java servlet, Java EE'de Java Servlet API'siyle uyumlu bir Java (programlama) sınıfı olup HTTP istemlerine cevap vermek için kullanılır. Belirli bir istemci-sunucu protokolüne bağlı olmamasına rağmen genelde bu protokolle kullanılır. Servlet kelimesi genelde HTTP servlet yerine bu yüzden kullanılmaktadır.[1] Dolayısıyla bir yazılım uzmanı, Java platformu sayesinde servleti bir Web sunucusuna dinamik içerik eklemek için kullanabilir. Üretilen kod genelde HTML olsa da bazen XML de olabilir. Servletler, CGI ya da ASP.NET gibi Java-dışı Web içerik teknolojilerinin Java'daki karşıt üründür. Servletlerle HTTP çerezleri veya URL yeniden yazımı kullanılarak oturum değişkenlerinin sistem durumunu birçok sunucu hareketleri boyunca koruması sağlanmaktadır.
-
-
C		(bosch)	 (bosch)
c nedir, c programlama dili nedir, c dili nedir
AT&T Bell laboratuvarlarında, Ken Thompson ve Dennis Ritchie tarafından UNIX İşletim Sistemi' ni geliştirebilmek amacıyla B dilinden türetilmiş yapısal bir programlama dilidir. Geliştirilme tarihi 1972 olmasına rağmen yayılıp yaygınlaşması Brian Kernighan ve Dennis M. Ritchie tarafından yayımlanan "C Programlama Dili" kitabından sonra hızlanmıştır. 

Günümüzde neredeyse tüm işletim sistemlerinin (Microsoft Windows, GNU/Linux, *BSD, Minix) yapımında %95' lere varan oranda kullanılmış, hâlen daha sistem, sürücü yazılımı, işletim sistemi modülleri ve hız gereken her yerde kullanılan oldukça yaygın ve sınırları belirsiz oldukça keskin bir dildir.[kaynak belirtilmeli]" Keskinliği, programcıya sonsuz özgürlüğün yanında çok büyük hatalar yapabilme olanağı sağlamasıdır. Programlamanın gelişim süreciyle beraber programlamanın karmaşıklaşması, gereksinimlerin artması ile uygulama programlarında nesne yönelimliliğin ortaya çıkmasından sonra C programcıları büyük ölçüde nesne yönelimliliği destekleyen C++ diline geçmişlerdir.
 
C Dilinin erken tarihi
C'nin ilk gelişme safhaları 1969 ile 1974 arasında AT&T Bell Laboratuvarları'nda gerçekleşti. Ritchie'ye göre, en yaratıcı devre 1972 idi. Dilin pek çok özelliği "B" adlı bir dilden türediği için, yeni dile "C" adı verildi. B dili yorumlanan bir dildi ve veri tipi desteği yoktu. Yeni donanımların farklı veri tiplerini desteklemesi, ve yorumlanan dillerin çalışma zamanında görece yavaş olması sebebi ile, C dili, tip desteği eklenmiş ve derlenen B olarak geliştirildi.
"B" adının kökeni konusunda ise söylentiler değişik: Ken Thompson B'nin BCPL programlama dilinden türediğini söylemektedir, ancak Thompson eşi Bonnie'nin onuruna adını Bon koyduğu bir programlama dili de geliştirmiştir.
1973'e kadar C yeterince güçlü bir hale gelmiş ve ilk başta PDP-11/20 assembly dili ile yazılan UNIX'in çekirdeğinin büyük kısmı C ile yeniden yazılmıştı. 
(Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir.)
Böylece UNIX, çekirdeği bir assembly dili ile yazılmayan ilk işletim sistemlerinden biri olmuştu.
-
-
-
-
BCPL		(bosch)	 (bosch)
bcpl nedir
BCPL ("Basic Combined Programming Language") is a procedural, imperative, and structured programming language. Originally intended for writing compilers for other languages, BCPL is no longer in common use. However, its influence is still felt because a stripped down and syntactically changed version of BCPL, called B, was the language on which the C programming language was based. BCPL introduced several features of many modern programming languages, including using curly braces to delimit code blocks. BCPL was first implemented by Martin Richards of the University of Cambridge in 1967.
The book BCPL: The language and its compiler describes the philosophy of BCPL as follows:
"The philosophy of BCPL is not one of the tyrant who thinks he knows best and lays down the law on what is and what is not allowed; rather, BCPL acts more as a servant offering his services to the best of his ability without complaint, even when confronted with apparent nonsense. The programmer is always assumed to know what he is doing and is not hemmed in by petty restrictions."
-

Microsoft Azure HDInsight       	(GITHUB)  (test)
Microsoft Azure HDInsight nedir
Azure HDInsight is a cloud-based service from Microsoft for big data analytics that helps organizations process large amounts of streaming or historical data. Microsoft promotes HDInsight for applications in data warehousing and ETL (extract, transform, load) scenarios as well as machine learning and Internet of Things (IoT) environments.
The fully-managed and open source service is based on the Hortonworks Data Platform (HDP) Hadoop distribution and includes implementations of Apache products, including:
•	Spark
•	HBase
•	Storm
•	Pig
•	Hive
•	Sqoop
•	Oozie
•	Ambari
HDInsight enables integration with business intelligence tools like Power BI, Excel, SQL Server Analysis Services and SQL Server Reporting Services. The service's security measures for data include encryption, monitoring, virtual networks, Active Directory authentication, authorization and role-based access control (RBAC).
Açık kaynak analize yönelik uygun maliyetli, kurumsal sınıf bir hizmet olan Azure HDInsight’ı kullanarak Apache Hadoop, Spark ve Kafka dahil olmak üzere popüler açık kaynak çerçeveleri kolayca çalıştırın. Azure’ın küresel ölçeği sayesinde çok büyük miktarda veriyi hiç çaba harcamadan işleyin ve geniş açık kaynak ekosisteminin tüm avantajlarına sahip olun.
-
DHTML

What does Dynamic HyperText Markup Language (DHTML) mean?
Dynamic HyerText Markup Language (DHTML) is a combination of Web development technologies used to create dynamically changing websites. Web pages may include animation, dynamic menus and text effects. The technologies used include a combination of HTML, JavaScript or VB Script,
CSS and the document object model (DOM).

Designed to enhance a Web user’s experience, DHTML includes the following features:
•	Dynamic content, which allows the user to dynamically change Web page content
•	Dynamic positioning of Web page elements
•	Dynamic style, which allows the user to change the Web page’s color, font, size or content
RELATED QUESTION
What’s the difference between a function and a functor?
Techopedia explains Dynamic HyperText Markup Language (DHTML)
While DHTML enhances the website user’s experience, the technology may also be frustrating for users when it is used incorrectly. For example, a website menu with flashy DHTML animations can easily confuse user navigation. Another DHTML issue occurs when Web developers attempt to create cross-browser DHTML, which is very difficult.

For Web developers, DHTML poses the following problems:
•	It can be difficult to develop and debug because of lack of Web browser and technological support.
•	DHTML scripts may not work correctly in various Web browsers.
•	The Web page layout may not display correctly when it is developed to display in different screen size combinations and in different browsers.
As a result of these problems, Web developers must determine whether DHTML enhances the user experience in any given context. Most Web developers abandon complex DHTML and use simple cross-browser routines to improve user experience, as opposed to integrating excessive DHTML visual effects.
-
-
DHTMLX             (GITHUB)  (test) 
Dhtmlx nedir
DHTMLX: JavaScript UI Framework
The DHTMLX library supplies over 20 fully customizable widgets to help you build interfaces of different kinds, nicely present data and work with it. There are also multiple controls and specific features to make your applications good-looking and user-friendly.

 
-
-
-
DHTML  (Dynamic HyperText Markup Language)              (GITHUB)  (test) 
Dhtml nedir
Dynamic HTML, or DHTML, is a collection of technologies used together to create interactive and animated websites[1] by using a combination of a static markup language (such as HTML), a client-side scripting language (such as JavaScript), a presentation definition language (such as CSS), and the Document Object Model (DOM).[2] The application of DHTML was introduced by Microsoft with the release of Internet Explorer 4 in 1997. Today, references to unobtrusive JavaScript coding (DOM Scripting) have replaced the usage of the term DHTML.
DHTML allows scripting languages to change variables in a web page's definition language, which in turn affects the look and function of otherwise "static" HTML page content, after the page has been fully loaded and during the viewing process. Thus the dynamic characteristic of DHTML is the way it functions while a page is viewed, not in its ability to generate a unique page with each page load.
By contrast, a dynamic web page is a broader concept, covering any web page generated differently for each user, load occurrence, or specific variable values. This includes pages created by client-side scripting, and ones created by server-side scripting (such as PHP, Python, JSP or ASP.NET) where the web server generates content before sending it to the client.
DHTML is differentiated from Ajax by the fact that a DHTML page is still request/reload-based. With DHTML, there may not be any interaction between the client and server after the page is loaded; all processing happens in JavaScript on the client side. By contrast, an Ajax page uses features of DHTML to initiate a request (or 'subrequest') to the server to perform additional actions. For example, if there are multiple tabs on a page, pure DHTML approach would load the contents of all tabs and then dynamically display only the one that is active, while AJAX could load each tab only when it is really needed.
-
-
An important note	      (GITHUB)  (test) 
Why public variables are not allowed inside a method or any block except class in c#? 
(field nedir, local variable nedir)
Answer:
There is a difference between variables and fields. Only fields have access protection, while variables cannot be accessed at all outside method's body.
"Public variables" in C# are called fields. C# allows field declarations to appear only in the body of a class or a struct. Fields are parts of an object definition, which are either attached to an instance, or shared among all instances.
Variables inside methods, on the other hand, are not fields. Being local to a method body, they do not need access protection. Their lifetime and scope are limited inside the method, so there is no way for any code outside of that method to access local variables.

Another Answer:

Inside a method any variable you declare is only accessible within the method, it is called a local variable. Access modifiers (public, private etc.) make no sense here, as the variable won't exist outside the scope of the method.
-
You cannot define a public element inside a method. This element exists while the method is executing and cannot be accessed outside this main method.
-

Fields within a method are local to the method and are therefore don't have access modifiers.
Access modifiers are applied at the class level i.e.
public class Person {

  private String name

  public String getName() {
    return this.name
  }
}
-
Local variable can not be public. public, private, proctacted are only used with class level members.
Assume that if it is possible to do so then also how can you access it out of the method or class. So specifing scope before locel members is dose not make any sense.
-
Variables declared in methods (aka local variables) are only accessible within the method. After the method returns, all local variables in the method became unaccessible. You can treat all the local variables as if they were private.
Why is it so then?
Local variables' purpose is to temporarily store data during the execution of a method. As we all know, a method either does something, or return something, or both. While a method is doing this business, objects might be created to help with whatever the method is doing. And the objects are stored in variables. Since these variables only help the method do its job, they are not needed after the method finishes its job (returns).
See? They are temporary. There is no point accessing them from another scope.
-
The process in which a function calls itself directly or indirectly is called recursion and the corresponding function is called as recursive function. Using recursive algorithm, certain problems can be solved quite easily. Examples of such problems are Towers of Hanoi (TOH), Inorder/Preorder/Postorder Tree Traversals, DFS of Graph, etc.

-
In computer science, recursion is a method of solving a problem where the solution depends on solutions to smaller instances of the same problem.[1] Such problems can generally be solved by iteration, but this needs to identify and index the smaller instances at programming time. Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[2]
The power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.
— Niklaus Wirth, Algorithms + Data Structures = Programs, 1976[3]
Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure)[4] do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for.
Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for large problems, it is fundamental to use optimization techniques such as tail call optimization.[citation needed]
-
A common computer programming tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.

-
Recursive definition
Daha kolay anlaşılıyo şu acıklama:
: a definition of a function permitting values of the function to be calculated systematically in a finite number of stepsespecially : a mathematical definition in which the first case is given and the nth case is defined in terms of one or more previous cases and especially the immediately preceding one.
// a recursive definition of the factorial is given by 0! = 1 and (n + 1)! = (n + 1) ‧ n!
-

In mathematics and computer science, a recursive definition, or inductive definition, is used to define the elements in a set in terms of other elements in the set (Aczel 1977:740ff). Some examples of recursively-definable objects include factorials, natural numbers, Fibonacci numbers, and the Cantor ternary set.[1]
A recursive definition of a function defines values of the function for some inputs in terms of the values of the same function for other (usually smaller) inputs. For example, the factorial function n! is defined by the rules
0! = 1.
(n + 1)! = (n + 1)·n!.
This definition is valid for each natural number n, because the recursion eventually reaches the base case of 0. The definition may also be thought of as giving a procedure for computing the value of the function n!, starting from n = 0 and proceeding onwards with n = 1, n = 2, n = 3 etc.
The recursion theorem states that such a definition indeed defines a function that is unique. The proof uses mathematical induction.[2]
An inductive definition of a set describes the elements in a set in terms of other elements in the set. For example, one definition of the set N of natural numbers is:
1.	1 is in N.
2.	If an element n is in N then n + 1 is in N.
3.	N is the intersection of all sets satisfying (1) and (2).
There are many sets that satisfy (1) and (2) – for example, the set {1, 1.649, 2, 2.649, 3, 3.649, ...} satisfies the definition. However, condition (3) specifies the set of natural numbers by removing the sets with extraneous members. Note that this definition assumes that N is contained in a larger set (such as the set of real numbers) — in which the operation + is defined.
Properties of recursively defined functions and sets can often be proved by an induction principle that follows the recursive definition. For example, the definition of the natural numbers presented here directly implies the principle of mathematical induction for natural numbers: if a property holds of the natural number 0 (or 1), and the property holds of n+1 whenever it holds of n, then the property holds of all natural numbers (Aczel 1977:742).
-
 
Four stages in the construction of a Koch snowflake. As with many other fractals, the stages are obtained via a recursive definition.
-





JSF / JavaServer Faces  	(GITHUB) 	(örnek)
java server faces nedir, jsf nedir, javaserver faces nedir, javaserverfaces nedir
JavaServer Faces (JSF) is the Java standard technology for building component-based, event-oriented web interfaces. Like JavaServer Pages (JSP), JSF allows access to server-side data and logic. Unlike JSP, which is essentially an HTML page imbued with server-side capabilities, JSF is an XML document that represents formal components in a logical tree. JSF components are backed by Java objects, which are independent of the HTML and have the full range of Java abilities, including accessing remote APIs and databases.
The key idea to a framework like JSF is to encapsulate (or wrap) client-side technologies like HTML, CSS, and JavaScript, allowing developers to build web interfaces without much interaction with these technologies.
This article presents a snapshot of JSF's approach to component-based UI development for Java web applications. Simple examples introduce JSF's MVC architecture, event model, and component library. Examples include new features in JSF 2.3, and we'll use PrimeFaces for our component library.
-
-





Android SDK	(GITHUB)  (test)
Sdk nedir, android sdk nedir

The Android SDK (software development kit) is a set of development tools used to develop applications for Android platform. 
The Android SDK includes the following:
Required libraries
Debugger
An emulator
Relevant documentation for the Android application program interfaces (APIs)
Sample source code
Tutorials for the Android OS
-
-
-
Android  	(GITHUB)  (test)
Android nedir, android operation system nedir, android işletim sistemi nedir
Android; Google ve Open Handset Alliance tarafından, mobil cihazlar için geliştirilmekte olan, Linux tabanlı özgür ve ücretsiz bir işletim sistemidir.
Google Android OS is Google’s Linux-based open source operating system for mobile devices. Android has been the world’s most widely used smartphone platform as of 2010, with a worldwide smartphone market share of 75%.

Android is a mobile operating system based on a modified version of the Linux kernel and other open source software, designed primarily for touchscreen mobile devices such as smartphones and tablets. Android is developed by a consortium of developers known as the Open Handset Alliance, with the main contributor and commercial marketer being Google.

Initially developed by Android Inc., which Google bought in 2005, Android was unveiled in 2007, with the first commercial Android device launched in September 2008. The current stable version is Android 10, released on September 3, 2019. The core Android source code is known as Android Open Source Project (AOSP), which is primarily licensed under the Apache License. This has allowed variants of Android to be developed on a range of other electronics, such as game consoles, digital cameras, PCs and others, each with a specialized user interface. Some well known derivatives include Android TV for televisions and Wear OS for wearables, both developed by Google.
-
Vue.js             (GITHUB)  (test) 
Vue nedir, vuejs nedir, vue.js nedir
Vue.js (genellikle Vue olarak kullanılır) kullanıcı arayüzleri ve tek sayfa uygulamalar (single-page applications) inşa etmek için kullanılan açık kaynak Javascript is Vue.js, bildirimsel oluşturma (declarative rendering) ve bileşen kompozisyonuna odaklanan, aşamalı olarak benimsenebilen bir mimariye sahiptir. Yönlendirme (routing), durum yönetimi (state managment) ve yapım aracı(building tools) gibi karmaşık uygulamalar için gereken gelişmiş özellikler, resmi olarak destekleyici kütüphaneler ve paketler aracılığıyla sunulmaktadır.
-
-
-
Bell Labs		(bosch)	 (bosch)
Bell labs nedir
Bell Laboratuvarları (ayrıca Bell Labs ve geçmişte AT&T Bell Laboratuvarları ve Bell Telefon Laboratuvarları olarak bilinir), önceleri American Telephone & Telegraph Company (AT&T) 'nin şimdilerde ise Alcatel-Lucent 'in araştırma geliştirme kurumudur. Merkezi Murray Hill, New Jersey ABD'de bulunmaktadır.

 
Transistörler 1947'de Bell Laboratuvarları'nda icat edilmiştir
 
Murray Hill, New Jersey'deki Bell Laboratuvarları

En başarılı zamanlarında, Bell Laboratuvarları radyo astronomi, transistör, lazer, enformasyon teorisi, UNIX işletim sistemi, ve C programlama dili dahil çok çeşitli devrimsel teknolojiler geliştiren kendi türündeki ilk tesisti. Bell Laboratuvarlarında tamamlanan yedi esere Nobel Ödülü verilmiştir.
1937, Clinton J. Davisson "Deneysel olarak elektronların kristaller tarafından kırınımlarını" bulmasından dolayı George Paget Thomson ile Nobel Fizik Ödülünü paylaştı.
1956, John Bardeen, Walter H. Brattain, ve William Shockley"Yarı iletkenlere dair çalışmalarından ve transistör etkisini keşfinden dolayı" Nobel Fizik Ödülünü aldılar.
1977, Philip W. Anderson "Manyetik düzensiz sistemlerin elektronik yapısına dair temel teorik araştırmalarından dolayı" Nobel Fizik ödülünü Nevill Francis Mott ve John Hasbrouck Van Vleck ile paylaştı.
1978, Arno A. Penzias ve Robert W. Wilson "Kozmik mikrodalga arka plan ışımasını bulmalarından dolayı" Nobel Fizik Ödülünü aldılar.
1997, Steven Chu "Laser ışığıyla atomların soğutulması ve yakalanması için bir method geliştirdiğinden dolayı" Nobel Fizik Ödülünü Claude Cohen-Tannoudji ve William Daniel Phillips ile paylaştı.
1998, Horst Stormer, Robert Laughlin, ve Daniel Tsui çok küçük kuantum koridor etkisini bulmaları ve açıklamaları nedeniyle Nobel Fizik Ödülünü aldılar.
2009, Willard Boyle ve George E. Smith yarı iletken görüntüleme devresi (CCD sensörü) icat etmelerinden dolayı Nobel Fizik Ödülünü Charles K. Kao ile paylaştılar.-
-
-
-
-
-
Refactoring             (GITHUB)  (test) 
Refactor nedir, refactoring nedir
Refactoring, kodun işlevselliğini değiştirmeden, kodun kalitesinin artırılması sürecidir. Evet refactoring bir süreçtir. Her ne kadar amaç/niyet her zaman en baştan temiz kod yazmak olsa da, (her ne kadar yazarken en temiz şekilde yazılsa da) değişen ve gelişen kodların zaman içerisinde sürekli optimize edilmesi gerekecektir.
Refactoring nasıl yapılır?	
Refactoring yaparken, yapılacak olanların bir sıralamaya göre yapılması önemlidir. Ayrıca diğer bir önemli konu ise bu işi belirli bir ritimle yapmak önemlidir. Yani sırası ile yapılacak olan her küçük değişiklikten sonra test yapmak gerekir. Küçük değişikler, test, küçük değişiklikler, test, … gibi bir ritimle bu işi yapmak en basit ve garantili yoldur. Refactoring’in tanımında da bahsedildiği gibi, amacımız kodun işlevselliğinde hiçbir değişiklik yapmadan, kodların kalitesini en optimum seviyeye getirmek.Kodun işlevselliğinin değişmediğini garanti etmenin en iyi yolu da elbette test yazmaktır. 
-
-
-
UI (User Interface)   (GITHUB) (test)
Ui nedir, user interface nedir
The user interface (UI) is the point of human-computer interaction and communication in a device. This can include display screens, keyboards, a mouse and the appearance of a desktop. It is also the way through which a user interacts with an application or a website. The growing dependence of many businesses on web applications and mobile applications has led many companies to place increased priority on UI in an effort to improve the user's overall experience.
Types of user interfaces
The various types of user interfaces include:
•	graphical user interface (GUI)
•	command line interface (CLI)
•	menu-driven user interface
•	touch user interface
•	voice user interface (VUI)
•	form-based user interface
•	natural language user interface
Examples of user interfaces
Some examples of user interfaces include:
•	computer mouse
•	remote control
•	virtual reality
•	ATMs
•	speedometer
•	the old iPod click wheel
Websites such as Airbnb, Dropbox and Virgin America display strong user interface design. Sites like these have created pleasant, easily operable, user-centered designs (UCD) that focus on the user and their needs.

In early computers, there was very little user interface except for a few buttons at an operator's console. Many of these early computers used punched cards, prepared using keypunch machines, as the primary method of input for computer programs and data. While punched cards have been essentially obsolete in computing since 2012, some voting machines still use a punched card system.
The user interface evolved with the introduction of the command line interface, which first appeared as a nearly blank display screen with a line for user input. Users relied on a keyboard and a set of commands to navigate exchanges of information with the computer. This command line interface led to one in which menus (lists of choices written in text) predominated.
Finally, the GUI arrived, originating mainly in Xerox's Palo Alto Research Center (PARC), adopted and enhanced by Apple and effectively standardized by Microsoft in its Windows operating systems. Elements of a GUI include such things as windows, pull-down menus, buttons, scroll bars and icons. With the increasing use of multimedia as part of the GUI, sound, voice, motion video and virtual reality are increasingly becoming the GUI for many applications.
UI ve UX çoğu kez beraber kullanılmalarına rağmen aslında çok farklı iki alandır. UI konusunda olmasa bile UX konusunda çok geride olan ülkemizde, maalesef bu işi profesyonel olarak yapan insanlar bile aralarındaki farkı çok bilmiyorlar. Ülkemizde bir UXD (User Interface Designer) ile tanışmanız çok zor. Türkiye’de bu işle ilgili çoğu insanın özgeçmişinde “UI/UX” yazar. Bunun bir sebebi aralarındaki farkın bilinmemesi, diğer sebebi ise internet sektörünü geriden takip etmemizden dolayı bu işi iki parçaya bölmenin bir lüks olarak görünmesidir.

Kısaca bahsetmek gerekirse; UI, aslında arayüz tasarımıdır. Televizyon kumandasının üstündeki tuşlar, bir arayüz tasarımı örneğidir. Tuşların yerleri, büyüklüğü, renkleri tasarımcının verdiği kararlar sonucu uygulanır. Arayüz tasarımının internet sitelerindeki karşılığı ise butonlar, grid yapısı, mizanpaj, renkler, boşluklar gibi tüm grafik elemanlardan oluşan görsel tasarımdır. Zamanla internet sitelerindeki interaksiyon arttıkça, önceleri çok da önemsenmeyen arayüz tasarımı da gittikçe önemli bir hal aldı. Kullanıcıların sitede rahatça dolaşmaları ve hatta kalmalarını sağlamak için basit ve kullanılabilir bir arayüz artık şart. Kullanıcılar bir sitede dolaşmaya veya terk etmeye saniyeler içerisinde karar veriyorlar. Yoğun uğraşlarla sitenize yönlendirdiğiniz kullanıcıları sitenizde tutamadığınız zaman da maalesef tüm uğraşlarınız boşa gidiyor.
The emerging popularity of mobile applications has also affected UI, leading to something called mobile UI. Mobile UI is specifically concerned with creating usable, interactive interfaces on the smaller screens of smartphones and tablets and improving special features, like touch controls.
-
-
-
-
UX (User Experience) (GITHUB) (test)
Ux nedir, user experience nedir

UI ve UX çoğu kez beraber kullanılmalarına rağmen aslında çok farklı iki alandır. UI konusunda olmasa bile UX konusunda çok geride olan ülkemizde, maalesef bu işi profesyonel olarak yapan insanlar bile aralarındaki farkı çok bilmiyorlar. Ülkemizde bir UXD (User Interface Designer) ile tanışmanız çok zor. Türkiye’de bu işle ilgili çoğu insanın özgeçmişinde “UI/UX” yazar. Bunun bir sebebi aralarındaki farkın bilinmemesi, diğer sebebi ise internet sektörünü geriden takip etmemizden dolayı bu işi iki parçaya bölmenin bir lüks olarak görünmesidir.
-
-
-
UI ve UX arasındaki farklar nelerdir ?   (GITHUB) (örnek)

The UI is often talked about in conjunction with user experience (UX), which may include the aesthetic appearance of the device, response time and the content that is presented to the user within the context of the user interface. Both terms fall under the concept of human-computer interaction (HCI), which is the field of study focusing on the creation of computer technology and the interaction between humans and all forms of IT design. Specifically, HCI studies areas such as UCD, UI design and UX design.
An increasing focus on creating an optimized user experience has led some to carve out careers as UI and UX experts. Certain languages, such as HTML and CSS, have been geared toward making it easier to create a strong user interface and experience.
UI ingilizce User Interface'in (kullanıcı arayüzü) kısaltılmış hali, UX ise ingilizce User Experience'in (kullanıcı deneyimi) kısaltılmış halidir. UX, içinde UI'i de içeren bir çok disiplini de içeren daha büyük bir halkadır, yani UX > UI diyebiliriz. UX, bir kullanıcının ilgili şeyi (ürünü, hizmeti, web sitesini...) kullanırken, tasarım, kolay kullanım, kolay anlama, kullanırken memnun kalma gibi daha geniş bir perspektifte değerlendirir. UI ise bu deneyimin tasarım kısmını ifade eder. 
UI ve UX arasındaki farkı basit bir örnekle anlatalım. UX bir binanın tasarımını yapan mimar gibidir. Oda yerleşimleri ve büyüklükleri nasıl olacak, hangi malzemeler kullanılacak, ışıklandırma ve renkler nasıl olacak tüm bunları hesaplar, planlar... Tüm planlanan işlerin yapılması ise UI'dir. Görüldüğü gibi UX birden fazla disiplini içinde barındıran, daha çok planlamayı içeren ancak alt disiplinlerle ilgili  yeterli derecede bilgiyi barındıran, daha çok soyut bir çıktı veren bir alandır. UI ise, somut bir çıktı veren bir alandır. UI bir kahveyi içmek için gereken bardağı tasarlamış, UX ise buna ek olarak kahveyi içenin elinin yanmaması için bir bardak daha eklemiş. UX, UI'den farklı olarak kullanıcının ilgili ürünü kullanırken karşılaşabileceği problemleri de düşünür ve bunlarada çözümler bulur.
 
-
- 
We should also distinguish UX and usability: According to the definition of usability, it is a quality attribute of the UI, covering whether the system is easy to learn, efficient to use, pleasant, and so forth. Again, this is very important, and again total user experience is an even broader concept.
-
User Experience Basics
User experience (UX) focuses on having a deep understanding of users, what they need, what they value, their abilities, and also their limitations.  It also takes into account the business goals and objectives of the group managing the project. UX best practices promote improving the quality of the user’s interaction with and perceptions of your product and any related services.
Factors that Influence UX
At the core of UX is ensuring that users find value in what you are providing to them.  Peter Morville represents this through his User Experience Honeycomb  .
 
He notes that in order for there to be a meaningful and valuable user experience, information must be:
•	Useful: Your content should be original and fulfill a need
•	Usable: Site must be easy to use
•	Desirable: Image, identity, brand, and other design elements are used to evoke emotion and appreciation
•	Findable: Content needs to be navigable and locatable onsite and offsite
•	Accessible: Content needs to be accessible to people with disabilities
•	Credible: Users must trust and believe what you tell them
-
-
-
Technical Debt / Design Debt / Code Debt       (GITHUB)  (test)
Technical debt nedir
Technical debt (also known as design debt or code debt, but can be also related to other technical endeavors) is a concept in software development that reflects the implied cost of additional rework caused by choosing an easy (limited) solution now instead of using a better approach that would take longer.
As with monetary debt, if technical debt is not repaid, it can accumulate 'interest', making it harder to implement changes. Unaddressed technical debt increases software entropy. Technical debt is not necessarily a bad thing, and sometimes (e.g., as a proof-of-concept) is required to move projects forward. On the other hand, some experts claim that the "technical debt" metaphor tends to minimize the impact, which results in insufficient prioritization of the necessary work to correct it.
As a change is started on a codebase, there is often the need to make other coordinated changes in other parts of the codebase or documentation. Changes required that are not completed are considered debt, and until paid, will incur interest on top of interest, making it cumbersome to build a project. Although the term is used in software development primarily, it can also be applied to other professions.
-
-
-
Version Control 		(GITHUB)
version control nedir, versiyonlama nedir
 
Version control systems are a category of software tools that help a software team manage changes to source code over time. Version control software keeps track of every modification to the code in a special kind of database. If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.

Top 10 Version Control Systems
•	Azure DevOps Server.
•	AWS CodeCommit.
•	Git.
•	CA Harvest Software Change Manager.
•	Subversion.
•	Helix Core.
•	Rational ClearCase.
•	FogBugz.

For almost all software projects, the source code is like the crown jewels - a precious asset whose value must be protected. For most software teams, the source code is a repository of the invaluable knowledge and understanding about the problem domain that the developers have collected and refined through careful effort. Version control protects source code from both catastrophe and the casual degradation of human error and unintended consequences.
Software developers working in teams are continually writing new source code and changing existing source code. The code for a project, app or software component is typically organized in a folder structure or "file tree". One developer on the team may be working on a new feature while another developer fixes an unrelated bug by changing code, each developer may make their changes in several parts of the file tree.
Version control helps teams solve these kinds of problems, tracking every individual change by each contributor and helping prevent concurrent work from conflicting. Changes made in one part of the software can be incompatible with those made by another developer working at the same time. This problem should be discovered and solved in an orderly manner without blocking the work of the rest of the team. Further, in all software development, any change can introduce new bugs on its own and new software can't be trusted until it's tested. So testing and development proceed together until a new version is ready.
Good version control software supports a developer's preferred workflow without imposing one particular way of working. Ideally it also works on any platform, rather than dictate what operating system or tool chain developers must use. Great version control systems facilitate a smooth and continuous flow of changes to the code rather than the frustrating and clumsy mechanism of file locking - giving the green light to one developer at the expense of blocking the progress of others.
Software teams that do not use any form of version control often run into problems like not knowing which changes that have been made are available to users or the creation of incompatible changes between two unrelated pieces of work that must then be painstakingly untangled and reworked. If you're a developer who has never used version control you may have added versions to your files, perhaps with suffixes like "final" or "latest" and then had to later deal with a new final version. Perhaps you've commented out code blocks because you want to disable certain functionality without deleting the code, fearing that there may be a use for it later. Version control is a way out of these problems.
Version control software is an essential part of the every-day of the modern software team's professional practices. Individual software developers who are accustomed to working with a capable version control system in their teams typically recognize the incredible value version control also gives them even on small solo projects. Once accustomed to the powerful benefits of version control systems, many developers wouldn't consider working without it even for non-software projects.
Benefits of version control systems
Developing software without using version control is risky, like not having backups. Version control can also enable developers to move faster and it allows software teams to preserve efficiency and agility as the team scales to include more developers.
Version Control Systems (VCS) have seen great improvements over the past few decades and some are better than others. VCS are sometimes known as SCM (Source Code Management) tools or RCS (Revision Control System). One of the most popular VCS tools in use today is called Git. Git is a Distributed VCS, a category known as DVCS, more on that later. Like many of the most popular VCS systems available today, Git is free and open source. Regardless of what they are called, or which system is used, the primary benefits you should expect from version control are as follows.
1.	A complete long-term change history of every file. This means every change made by many individuals over the years. Changes include the creation and deletion of files as well as edits to their contents. Different VCS tools differ on how well they handle renaming and moving of files. This history should also include the author, date and written notes on the purpose of each change. Having the complete history enables going back to previous versions to help in root cause analysis for bugs and it is crucial when needing to fix problems in older versions of software. If the software is being actively worked on, almost everything can be considered an "older version" of the software.
2.	Branching and merging. Having team members work concurrently is a no-brainer, but even individuals working on their own can benefit from the ability to work on independent streams of changes. Creating a "branch" in VCS tools keeps multiple streams of work independent from each other while also providing the facility to merge that work back together, enabling developers to verify that the changes on each branch do not conflict. Many software teams adopt a practice of branching for each feature or perhaps branching for each release, or both. There are many different workflows that teams can choose from when they decide how to make use of branching and merging facilities in VCS.
3.	Traceability. Being able to trace each change made to the software and connect it to project management and bug tracking software such as Jira, and being able to annotate each change with a message describing the purpose and intent of the change can help not only with root cause analysis and other forensics. Having the annotated history of the code at your fingertips when you are reading the code, trying to understand what it is doing and why it is so designed can enable developers to make correct and harmonious changes that are in accord with the intended long-term design of the system. This can be especially important for working effectively with legacy code and is crucial in enabling developers to estimate future work with any accuracy.
While it is possible to develop software without using any version control, doing so subjects the project to a huge risk that no professional team would be advised to accept. So the question is not whether to use version control but which version control system to use.
There are many choices, but here we are going to focus on just one, Git. Learn more about other types of version control software. 
-
-
özet: 
What Is Git?
First developed back in 2005, Git is an extremely popular version control system that is at the heart of a wide variety of high-profile projects. Git is installed and maintained on your local system (rather than in the cloud) and gives you a self-contained record of your ongoing programming versions. It can be used completely exclusive of any cloud-hosting service — you don’t even need internet access, except to download it. 
Compared to other version control systems, Git is responsive, easy to use, and inexpensive (free, actually). Git is also specially designed to work well with text files — which, if you think about it, is what code actually is. But one thing that really sets Git apart is its branching model. Branching allows you to create independent local branches in your code. This means you can try out new ideas, set aside branches for production work, jump back to earlier branches, and easily delete, merge, and recall branches at the click of a button.
And that’s it. Git is a high-quality version control system. But what about GitHub?
What Is GitHub?
In the discussion of Git vs. GitHub, it’s been said that GitHub is to Git what Facebook is to your actual face. What’s that mean? Well, it means that while Facebook is kind of like an online face database (of sorts). GitHub is designed as a Git repository hosting service. 
And what exactly is a Git repository hosting service? It’s an online database that allows you to keep track of and share your Git version control projects outside of your local computer/server. Unlike Git, GitHub is exclusively cloud-based. Also unlike Git, GitHub is a for-profit service (although basic repository-hosting features are available at no cost to those who are willing to create a user profile, making GitHub a popular choice for open-source projects).
-
-
-
-
-

Git vs. GitHub: What's the Difference?
Given that coding relies so heavily on exact syntax, the naming convention surrounding programming languages and resources is anything but intuitive. Java and JavaScript are about as closely related as ham and hamster, and Python’s logo may be an image of intertwined snakes, but it’s actually named aftexr the Monty Python sketch comedy group. HTML and CSS are acronyms that describe what the code actually is or does (HyperText Markup Language and Cascading Style Sheets, respectively), while C++ describes its origins. And that’s only scratching the surface.
So, for someone first hearing about Git vs. GitHub, the apparent connection may not be quite so apparent. 
 
These questions are definitely worth asking. After all, Microsoft was willing to shell out $7.5 billion dollars to acquire GitHub back in 2018, so developers of all skill levels should be sitting up and taking notice. But the truth is that Git and GitHub are connected much more closely than Java and JavaScript — but with some key differences setting them apart.
What's the difference between Git and GitHub? Well, to answer that, we’ll be taking a closer look at each one. But before we do that, let’s first discuss the concept of version control.
Version Control
 
Development projects don’t come into existence ex nihilo; they’re built code line upon code line from the ground up. And often, there’s a lot of trial, error, and correction that goes into creating something that actually works the way it’s supposed to. That’s where version control comes in. 
Version control is like a savings program for your project. By tracking and logging the changes you make to your file or file sets over time, a version-control system gives you the power to review or even restore earlier versions. Version control takes snapshots of every revision to your project. You can then access these versions to compare or restore them as needed.
For example, let’s say that you’re working on a web development project, and through the course of your revisions, you suddenly notice all of your text has become misaligned. And because the first rule of coding is that it’s always your fault, you can bet that some change you made somewhere along the way is the cause. But not to worry; instead of having to crawl back through every line of code, you can just use your version control system to reload earlier versions, until you pinpoint the offending change and correct it.
And with that in mind, let’s move on to Git.
What Is Git?
First developed back in 2005, Git is an extremely popular version control system that is at the heart of a wide variety of high-profile projects. Git is installed and maintained on your local system (rather than in the cloud) and gives you a self-contained record of your ongoing programming versions. It can be used completely exclusive of any cloud-hosting service — you don’t even need internet access, except to download it. 
Compared to other version control systems, Git is responsive, easy to use, and inexpensive (free, actually). Git is also specially designed to work well with text files — which, if you think about it, is what code actually is. But one thing that really sets Git apart is its branching model. Branching allows you to create independent local branches in your code. This means you can try out new ideas, set aside branches for production work, jump back to earlier branches, and easily delete, merge, and recall branches at the click of a button.
And that’s it. Git is a high-quality version control system. But what about GitHub?
What Is GitHub?
In the discussion of Git vs. GitHub, it’s been said that GitHub is to Git what Facebook is to your actual face. What’s that mean? Well, it means that while Facebook is kind of like an online face database (of sorts). GitHub is designed as a Git repository hosting service. 
And what exactly is a Git repository hosting service? It’s an online database that allows you to keep track of and share your Git version control projects outside of your local computer/server. Unlike Git, GitHub is exclusively cloud-based. Also unlike Git, GitHub is a for-profit service (although basic repository-hosting features are available at no cost to those who are willing to create a user profile, making GitHub a popular choice for open-source projects).
That’s because, in addition to offering all of the features and advantages of Git, GitHub expands upon Git’s basic functionality. It presents an extremely intuitive, graphically represented user interface, and provides programmers with built-in control and task-management tools. Additional features can be implemented via the GitHub Marketplace service. And because GitHub is cloud-based, an individual’s Git repositories can be remotely accessed by any authorized person, from any computer, anywhere in the world (provided it has an internet connection). 
Through GitHub, you can share your code with others, giving them the power to make revisions or edits on your various Git branches. This makes it possible for entire teams to coordinate together on single projects in real-time. As changes are introduced, new branches are created, allowing the team to continue to revise the code without overwriting each other's work. These branches are like copies, and changes made on them do not reflect in the main directories on other users’ machines unless users choose to push/pull the changes to incorporate them. There is also a GitHub desktop application available, which offers some additional functionality for experienced developers.
Other Git repository hosting services also exist; GitLab, BitBucket, and SourceForge are all viable GitHub alternatives, and GitLab even offers a built-in option which allows GitHub users to migrate their projects directly into GitLab. 
Git vs. GitHub in Simple Terms
 
So, taken all together: Git vs. GitHub… what’s the difference? Simply put, Git is a version control system that lets you manage and keep track of your source code history. GitHub is a cloud-based hosting service that lets you manage Git repositories. If you have open-source projects that use Git, then GitHub is designed to help you better manage them. 
After all, in the world of programming, naming conventions aren’t always intuitive. That’s why it’s worth recognizing the connections and the differences in the similarly named Git and GitHub. Both Git and GitHub give programmers valuable version-control functionality so that they can build ongoing coding projects without being afraid of messing everything up. GitHub just takes things a little bit further than Git, offering more functionality and resources, as well as a place online to store and collaborate on projects. 
And that’s essentially it. Simple, right?
-
Anemic Domain Model, Rich Domain Model    (GITHUB) (asasas)

Anemic Domain Model nedir, Rich Domain Model nedir

Anemic domain model = database tables mapped to objects (only field values, no real behavior)
Rich domain model = a collection of objects that expose behavior
If you want to create a simple CRUD application, maybe an anemic model with a classic MVC framework is enough. But if you want to implement some kind of logic, anemic model means that you will not do object oriented programming.
*Note that object behavior has nothing to do with persistence. A different layer (Data Mappers, Repositories e.t.c.) is responsible for persisting domain objects.

Anemic domain model is the use of a software domain model where the domain objects contain little or no business logic (validations, calculations, business rules etc.).

-
-
-
Domain model nedir:

Basically, it's the "model" of the objects required for your business purposes. Say you were making a sales tracking website - you'd potentially have classes such as Customer, Vendor, Transaction, etc. That entire set of classes, as well as the relationships between them, would consititute your Domain Model.
-
Domain nedir ve domain model nedir açıklaması:
A domain model is a visual representation of real situation objects in a domain. A domain is an area of concern. Its used to refer to the area you are dealing with. The model is a diagram, for domain models the class diagram UML is mostly used. The class diagram is only used for the notation. The term domain model does not mean a set of diagrams describing software classes.
So for example imagine a store. For that store you want to build a brand new Point Of Sale system (lets call it POS system). A POS system is a computerized application used to record sale and handle payments. So you focus on the domain of the POS system. Now you will conceptualize the objects that will be used for this system. So you will get objects like: Sale, Payment, Register, Item etc. In a domain model you model these objects and draw associations between them so that you have an high level idea how this system will work. An example of the POS domain model will be like this:
 
-




Domain model:
In software engineering, a domain model is a conceptual model of the domain[definition needed] that incorporates both behavior and data.[1][2] In ontology engineering, a domain model is a formal representation of a knowledge domain with concepts, roles, datatypes, individuals, and rules, typically grounded in a description logic.
Overview
A domain model is a system of abstractions that describes selected aspects of a sphere of knowledge, influence or activity (a domain[3]). The model can then be used to solve problems related to that domain. The domain model is a representation of meaningful real-world concepts pertinent to the domain that need to be modeled in software. The concepts include the data involved in the business and rules the business uses in relation to that data. A domain model leverages natural language of the domain.
A domain model generally uses the vocabulary of the domain, thus allowing a representation of the model to be communicated to non-technical stakeholders. It should not refer to any technical implementations such as databases or software components that are being designed.
Usage
A domain model is generally implemented as an object model within a layer that uses a lower-level layer for persistence and "publishes" an API to a higher-level layer to gain access to the data and behavior of the model.
In the Unified Modeling Language (UML), a class diagram is used to represent the domain model. 
Sample domain model for a health insurance plan

-
Component Object Model (COM) is a method to facilitate communication between different applications and languages. COM is used by developers to create re-usable software components, link components together to build applications, and take advantage of Windows services. COM objects can be created with a variety of programming languages. Object-oriented languages, such as C++, provide programming mechanisms that simplify the implementation of COM objects. The family of COM technologies includes COM+, Distributed COM (DCOM) and ActiveX® Controls.

COM (Component Object Model) was the first programming model that provided component based approach to software development. This component based approach of COM allowed us to develop small, logical reusable and standalone modules that integrates into a single application. But these components could not be displayed over a network. So these drawback produce another model that is DCOM (Distributed COM). 

The DCOM programming model enables you to display COM components over a network and easily distribute applications across platforms. DCOM components also help in two-tier client/server applications. These models also have some drawbacks that help the development of the COM+ approach. 

Creating COM components in .NET

The following steps explain the way to create the COM server in C#: 
1.	Create a new Class Library project. 
2.	Create a new interface, say IManagedInterface, and declare the methods required. Then provide the Guid (this is the IID) for the interface using the GuidAttribute defined in System.Runtime.InteropServices. The Guid can be created using the Guidgen.exe. [Guid("3B515E51-0860-48ae-B49C-05DF356CDF4B")] 
3.	Define a class that implements this interface. Again provide the Guid (this is the CLSID) for this class also. 
4.	Mark the assembly as ComVisible. For this go to AssemblyInfo.cs file and add the following statement [assembly: ComVisible (true)]. This gives the accessibility of all types within the assembly to COM. 
5.	Build the project. This will generate an assembly in the output path. Now register the assembly using regasm.exe (a tool provided with the .NET Framework) - regasm \bin\debug\ComInDotNet.dll \tlb:ComInDotNet.tlb this will create a TLB file after registration. 
6.	Alternatively this can be done in the Project properties --> Build --> check the Register for COM interop. 
The COM server is ready. Now a client has to be created. It can be in any language. If the client is .NET, just add the above created COM assembly as a reference and use it. 

How to call COM components from .NET? 

COM components and .NET Components have a different internal architecture. For both of them to communicate with each other, the inter-operation feature is required, this feature is called interoperability. Enterprises that have written their business solutions using the old native COM technology need a way for re-using these components in the new .NET environment. 

.NET components communicate with COM using RCW (Runtime Callable Wrapper)

RCW:- RCW Means Runtime Callable Wrappers, The Common Language Runtime (CLR) exposes COM objects through a proxy called the Runtime Callable Wrapper (RCW). Although the RCW appears to be an ordinary object to .NET clients, it's primary function is to marshal calls between a .NET client and a COM object.

To use a COM component,
•	Right click the Project and click on Add References. 
•	Select the COM tab 
•	And at last select COM component 
 

                              Fig.1 Calling a COM component from .NET client

Another way of using a COM component is using the tblimp.exe tool (Type Library Import).

How to call .NET component from COM? 

In case a .NET component needs to be used in COM, we make use of CCW (COM Callable Wrapper).

CCW:- .NET components are accessed from COM via a COM Callable Wrapper (CCW). This is similar to a RCW, but works in the opposite direction. Again, if the wrapper cannot be automatically generated by the .NET development tools, or if the automatic behavior is not desirable, a custom CCW can be developed. Also, for COM to 'see' the .NET component, the .NET component must be registered in the registry. CCWs also manage the object identity and object lifetime of the managed objects they wrap.

 

                                    Fig.2 Calling a .NET component from COM client

Following are the different approaches to implement it: 
•	Explicitly declare interfaces. 
•	The second way to create CCW is using InteropServices attributes. Here interfaces are created automatically.
-
-
-
-
Business Intelligence	 (GITHUB)  (test)
business intelligence nedir
İş Zekası (Business Intelligence), ham veriyi işleyerek daha anlamlı ve kullanışlı hale getirmemizi sağlayan süreçlere, yöntemlere verilen isimdir. İş Zekası, büyük miktarda verileri işleyerek yeni projelerin belirlenmesine ve geliştirilmesine yardımcı olur. Çalışma alanlarımız genellikle işlenebilecek ham veri miktarının en çok olduğu alanlardır. Finans sektöründen, telekomünikasyona, perakende sektöründen, enerjiye, kamu sektöründen sağlığa ve diğer bütün sektörlerde İş Zekası uygulamalarının kullanıldığını görmekteyiz.

İş Zekasının bileşenleri
Tamam, iş zekası nedir tanım olarak öğrendik, peki iş zekasının kapsamı nedir? İş Zekasının temel bileşenleri raporlama, çok boyutlu analiz süreçleri, mantıksal çözümlemeler, veri madenciliği, süreç madenciliği(süreç yönetim teknikleri), karmaşık olay analizi, iş performans yönetimi, bir iş sürecini diğer performans ölçümleriyle ya da bu iş sürecinin en iyileri ile kıyaslama, tahmine ve kurala dayalı mantıksal çözümlerdir. İş Zekası uygulamaları ihtiyaçları belirler, analizleri yürütür, İş Zekası teknik çözümlerini ve mimari tasarımını yapar ve gerçekleştirir, merkezi veya dağıtık veri ambarını tasarlar, veri ambarında bulunan verileri yönetir, veriler üzerinde analizler yapılmasını ve ihtiyaç duyulan raporların alınmasını sağlar.  Finansal başarıyı sağlayan müşteri, üretim ve insan kaynakları kriterlerini belirleyerek finansal sonuçlar gibi ardıl ölçütlerin yanı sıra bu tür öncül ölçütleri de denetim altında tutabilir. Verilerinizi detaylı analiz ederek, stratejik ve taktiksel kararları doğru verebilmenizi sağlar. image3BI için çok sayıda uygulama firmaların etkileyici yatırım, geri dönüşüm değerleri elde etmelerine yardımcı oldu. İş Zekası maliyet düşürme yöntemleri belirleme, iş fırsatları ortaya çıkarma,  ERP verilerini erişilebilir raporlara dönüştürme, perakende taleplerine hızlı tepki verme ve fiyatları optimize etmede kullanıldı. Verileri erişebilir yapmanın yanı sıra BI yazılımı , tedarikçi ve müşterilerle olan ilişkinin değerini daha kolay bir biçimde ölçebilmelerini sağlayarak, firmalara anlaşmalar esnasında daha fazla güç verir.

İş Zekasına neden ihtiyaç duyarız?
İş Zekasının ortaya çıkmasındaki  nedenlerinden biri globalleşen rekabetin oranındaki artışlardır. Dünyanın bir ucundaki bir kurumda gerçekleşen ekonomik gelişme başka bir ülkenin ekonomisini altüst edebilmektedir. Tüm kurumlar böyle rekabet dolu bir ortamda çalışırken risk unsuru ortaya çıkar. Şirketler, gelişmelerin gerisinde kalmamak için çok daha hızlı ve doğru karar vermek zorunda kalır. Sonuç olarak, birim zamanda verilmesi gereken karar sayısındaki artışla birlikte iş Zekasına olan ihtiyaç da artmıştır. İş Zekası çözümlerinin en temel özelliği, işletmelere raporlama ve analiz alanında müthiş bir verimlilik kazandırması. İş Zekası çözümleri kullanıldığında raporlamalar gerçek zamanlı verilerle, anlık olarak hazırlanıyor. Böylece analizlerde de çok daha sağlıklı sonuçlar ortaya çıkıyor. İş Zekası uygulamaları daha çok kurumların raporlama araçlarının yetersiz kaldığı durumlarda ortaya çıkar. Uygulama geliştrililirken raporlama araçları da eklenir,zaman ilerledikçe veri artar ve farklı kaynaklardan veriler alınmaya başlanır. imagesBu durumda raporlar yetersiz kalır ve iş Zekası çözümlerine ihtiyaç olur.Diğer yandan farklı uygulama ve veri tabanlarından veri alan kurum sistemlerinde veriyi ilişkilendirerek anlamlandırma noktasında iş Zekası çözüm getirir.Raporlama süreçlerinde karar destek mekanizmaları,veri madenciliği, veri ambarı araçları da kullanılmaktadır. İş Zekası, acil durumlarda çok önemli olan ve çabuk karar vermeyi gerektiren iş süreçlerinde görünürlük ve beceriyi artırarak karmaşık raporların hızlı ve kısa sürelerde hazırlanmasını sağlar.

İş Zekası’nın geleceği
Vefaceri büyüklüğü gün geçtikçe artıyor. (Amazon’un iki büyük veritabanında toplam olarak 42 terabyte’lık veri var, YouTube’a her gün 65 bin yeni video yükleniyor , 2011 yılında 2 milyar Internet, 4,6 milyar cep telefonu kullanıcısının olduğu bir ortamdan söz ediyoruz , 2012 yılında  olabilmek için şirketlerin ya bu beklentileri karşılaması ya da aşması gerekiyor. Şirketlerin eğilimler ve gelecekteki yeni gelişmeler karşısında işlerini kolaylaştıracak unsurlar arasında da iş Zekası sistemleri yer alıyor.
-
-
Business intelligence definition
Business intelligence (BI) leverages software and services to transform data into actionable insights that inform an organization’s strategic and tactical business decisions. BI tools access and analyze data sets and present analytical findings in reports, summaries, dashboards, graphs, charts and maps to provide users with detailed intelligence about the state of the business.
The term business intelligence often also refers to a range of tools that provide quick, easy-to-digest access to insights about an organization's current state, based on available data. 
Business intelligence examples
Reporting is a central facet of business intelligence and the dashboard is perhaps the archetypical BI tool. Dashboards are hosted software applications that automatically pull together available data into charts and graphs that give a sense of the immediate state of the company.
Although business intelligence does not tell business users what to do or what will happen if they take a certain course, neither is BI solely about generating reports. Rather, BI offers a way for people to examine data to understand trends and derive insights by streamlining the effort needed to search for, merge and query the data necessary to make sound business decisions.
For example, a company that wants to better manage its supply chain needs BI capabilities to determine where delays are happening and where variabilities exist within the shipping process, says Chris Hagans, vice president of operations for WCI Consulting, a consultancy focused on BI. That company could also use its BI capabilities to discover which products are most commonly delayed or which modes of transportation are most often involved in delays.
The potential use cases for BI extend beyond the typical business performance metrics of improved sales and reduced costs, says Cindi Howson, research vice president at Gartner, an IT research and advisory firm. She points to the Columbus, Ohio, school system and its success using BI tools to examine numerous data points — from attendance rates to student performance — to improve student learning and high school graduate rates.
BI vendors Tableau and G2 also offer concrete examples of how organizations might put business intelligence tools to use:
•	A co-op organization could use BI to keep track of member acquisition and retention.
•	BI tools could automatically generate sales and delivery reports from CRM data.
•	A sales team could use BI to create a dashboard showing where each rep's prospects are on the sales pipeline.
Business intelligence vs. business analytics
One thing you will have noticed from those examples is that they provide insights into the current state of the business or organization: where are sales prospects in the pipeline today? How many members have we lost or gained this month? This gets to the key distinction between business intelligence and another, related term, business analytics.
Business intelligence is descriptive, telling you what's happening now and what happened in the past to get us to that state. Business analytics, on the other hand, is an umbrella term for data analysis techniques that are predictive — that is, they can tell you what's going to happen in the future — and prescriptive — that is, they can tell you what you should be doing to create better outcomes. (Business analytics are usually thought of as that subset of the larger category of data analytics that's specifically focused on business.)
The distinction between the descriptive powers of BI and the predictive or descriptive powers of business analytics goes a bit beyond just the timeframe we're talking about. It also gets to the heart of the question of who business intelligence is for. As the Stitchdata blog explains, BI aims to deliver straightforward snapshots of the current state of affairs to business managers. While the predictions and advice derived from business analytics requires data science professionals to analyze and interpret, one of the goals of BI is that it should be easy for relatively non-technical end users to understand, and even to dive into the data and create new reports.
For more, see “Business intelligence vs. business analytics: Where BI fits in your data strategy.”
Business intelligence strategy
In the past, IT professionals had been the primary users of BI applications. However, BI tools have evolved to be more intuitive and user-friendly, enabling a large number of users across a variety of organizational domains to tap the tools.
Gartner’s Howson differentiates two types of BI. The first is traditional or classic BI, where IT professionals use in-house transactional data to generate reports. The second is modern BI, where business users interact with agile, intuitive systems to analyze data more quickly.
Howson explains that organizations generally opt for classic BI for certain types of reporting, such as regulatory or financial reports, where accuracy is paramount and the questions and data sets used are standard and predicable. Organizations typically use modern BI tools when business users need insight into quickly changing dynamics, such as marketing events, in which being fast is valued over getting the data 100 percent right.
But while solid business intelligence is essential to making strategic business decisions, many organizations struggle to implement effective BI strategies, thanks to poor data practices, tactical mistakes and more.
For more, see “7 keys to a successful business intelligence strategy” and “9 ways you’re failing at business intelligence.”
Self-service business intelligence
The drive to make it possible for just about anyone to get useful information out of business intelligence tools has given rise to self-service business intelligence, a category of BI tools aimed at abstracting away the need for IT intervention in generating reports. Self-service BI tools enable organizations to make the company's internal data reports more readily available to managers and other nontechnical staff.
Among the keys to self-service BI success are business intelligence dashboards and UIs that include pull-down menus and intuitive drill-down points that allow users to find and transform data in easy-to-understand ways. A certain amount of training will no doubt be required, but if the advantages of the tools are obvious enough, employees will be eager to get on board. (If you're shopping for a self-service BI solution, CIO.com’s Martin Heller walks you through the decision making process and compares his top five choices.)
Keep in mind, though, that there are pitfalls to self-service BI as well. By steering your business users into becoming ad hoc data engineers, you can end up with a chaotic mix of metrics that vary across departments, run into data security problems, and even run up big licensing or SaaS bills if there's no centralized control over tool rollout. So even if you are committing to self-service business intelligence within your organization, you can't just buy an off-the-shelf product, point your staff to the UI, and hope for the best.
Business intelligence software and systems
A variety of different types of tools fall under the business intelligence umbrella. The software selection service SelectHub breaks down some of the most important categories and features:
•	Dashboards
•	Visualizations
•	Reporting
•	Data mining
•	ETL (extract-transfer-load —tools that import data from one data store into another)
•	OLAP (online analytical processing)
Of these tools, SelectHub says the dashboards and visualization are by far the most popular; they offer the quick and easy-to-digest data summaries that are at the heart of BI's value proposition.
There are tons of vendors and offerings in the BI space, and wading through them can get overwhelming. Some of the major players include:
•	Tableau, a self-service analytics platform provides data visualization and can integrate with a range of data sources, including Microsoft Azure SQL Data Warehouse and Excel
•	Splunk, a “guided analytics platform” capable of providing enterprise-grade business intelligence and data analytics
•	Alteryx, which blends analytics from a range of sources to simplify workflows as well as provide a wealth of BI insights
•	Qlik, which is grounded in data visualization, BI and analytics, providing an extensive, scalable BI platform
•	Domo, a cloud-based platform that offers business intelligence tools tailored to various industries (such as financial services, health care, manufacturing and education) and roles (including CEOs, sales, BI professionals and IT workers)
•	Dundas BI, which is mostly used for creating dashboards and scorecards, but can also do standard and ad-hoc reporting
•	Google Data Studio, a supercharged version of the familiar Google Analytics offering
•	Einstein Analytics, Salesforce.com’s attempt to improve BI with AI
•	Birst, a cloud-based service in which multilple instances of the BI software share a common data backend.
For a deeper look at today’s most popular business intelligence systems, see "Top 12 BI tools of 2019" and "Top 10 BI data visualization tools."
Business intelligence analyst
Any company that's serious about BI will need to have business intelligence analysts on staff. CIO.com has an in-depth article on what that job entails; in general, they aim to use all the features of BI tools to get the data that companies need, the most important being discovering areas of revenue loss and identifying where improvements can be made to save the company money or increase profits.
Even if your company relies on self-service BI tools on a day-to-day basis, business intelligence analysts have an important role to play, as they are necessary for managing and maintaining those tools and their vendors. They also set up and standardize the reports that managers are going to be generating to make sure that results are consistent and meaningful across your organization. And to avoid garbage in/garbage out problems, business intelligence analysts need to make sure the data going into the system is correct and consistent, which often involves getting it out of other data stores and cleaning it up.
Business intelligence analyst jobs often require only a bachelor's degree, at least at the entry level, though to advance up the ranks an MBA may be helpful or even required. As of October 2019, the median business intelligence salary is around $67,500, though depending on your employer that could range from $49,000 to $94,000.
The future of business intelligence
Moving ahead, Howson says Gartner sees a third wave of disruption on the horizon, something the research firm calls “augmented analytics,” where machine learning is baked into the software and will guide users on their queries into the data.
“It will be BI and analytics, and it will be smart,” she says.
The combinations included in these software platforms will make each function more powerful individually and more valuable to the businesspeople using them, Gorman says.
“Someone will look at reports from, for example, last year’s sales — that’s BI — but they’ll also get predictions about next year’s sales — that’s business analytics — and then add to that a what-if capability: What would happen if we did X instead of Y,” Gorman says, explaining that software makers are moving to develop applications that will provide those functions within a single application rather than delivering them via multiple platforms as is now the case.
“Now the system delivers higher-value recommendations. It makes the decision-maker more efficient, more powerful and more accurate,” he adds.
And although BI will remain valuable in and of itself, Howson says organizations can’t compete if they’re not moving beyond only BI and adopting advanced analytics as well.
In fact, Gartner’s Magic Quadrant report predicts that by 2020 organizations offering “users access to a curated catalog of internal and external data will realize twice the business value from analytics investments than those that do not.”
Howson adds: “There is a need for reporting, but reporting alone is not enough. If you’re only doing reporting you’re behind already. Unless your reporting is smart and agile, you’re behind. You’re a laggard.”
-
-
OLEDB - OLE DB

OLEDB nedir, ole db nedir
ODBC and OLE DB are two competing data access technologies. Specifically regarding SQL Server, Microsoft has promoted both of them as their Preferred Future Direction - though at different times.
ODBC
ODBC is an industry-wide standard interface for accessing table-like data. It was primarily developed for databases and presents data in collections of records, each of which is grouped into a collection of fields. Each field has its own data type suitable to the type of data it contains. Each database vendor (Microsoft, Oracle, Postgres, …) supplies an ODBC driver for their database.
There are also ODBC drivers for objects which, though they are not database tables, are sufficiently similar that accessing data in the same way is useful. Examples are spreadsheets, CSV files and columnar reports.
OLE DB
OLE DB is a Microsoft technology for access to data. Unlike ODBC it encompasses both table-like and non-table-like data such as email messages, web pages, Word documents and file directories. However, it is procedure-oriented rather than object-oriented and is regarded as a rather difficult interface with which to develop access to data sources. To overcome this, ADO was designed to be an object-oriented layer on top of OLE DB and to provide a simpler and higher-level – though still very powerful – way of working with it. ADO’s great advantage it that you can use it to manipulate properties which are specific to a given type of data source, just as easily as you can use it to access those properties which apply to all data source types. You are not restricted to some unsatisfactory lowest common denominator.
While all databases have ODBC drivers, they don’t all have OLE DB drivers. There is however an interface available between OLE and ODBC which can be used if you want to access them in OLE DB-like fashion. This interface is called MSDASQL (Microsoft OLE DB provider for ODBC).
SQL Server Data Access Technologies
Since SQL Server is (1) made by Microsoft, and (2) the Microsoft database platform, both ODBC and OLE DB are a natural fit for it.
ODBC
Since all other database platforms had ODBC interfaces, Microsoft obviously had to provide one for SQL Server. In addition to this, DAO, the original default technology in Microsoft Access, uses ODBC as the standard way of talking to all external data sources. This made an ODBC interface a sine qua non. The version 6 ODBC driver for SQL Server, released with SQL Server 2000, is still around. Updated versions have been released to handle the new data types, connection technologies, encryption, HA/DR etc. that have appeared with subsequent releases. As of 09/07/2018 the most recent release is v13.1 “ODBC Driver for SQL Server”, released on 23/03/2018.
OLE DB
This is Microsoft’s own technology, which they were promoting strongly from about 2002 – 2005, along with its accompanying ADO layer. They were evidently hoping that it would become the data access technology of choice. (They even made ADO the default method for accessing data in Access 2002/2003.) However, it eventually became apparent that this was not going to happen for a number of reasons, such as:
1.	The world was not going to convert to Microsoft technologies and away from ODBC;
2.	DAO/ODBC was faster than ADO/OLE DB and was also thoroughly integrated into MS Access, so wasn’t going to die a natural death;
3.	New technologies that were being developed by Microsoft, specifically ADO.NET, could also talk directly to ODBC. ADO.NET could talk directly to OLE DB as well (thus leaving ADO in a backwater), but it was not (unlike ADO) solely dependent on it.
For these reasons and others, Microsoft actually deprecated OLE DB as a data access technology for SQL Server releases after v11 (SQL Server 2012). For a couple of years before this point, they had been producing and updating the SQL Server Native Client, which supported both ODBC and OLE DB technologies. In late 2012 however, they announced that they would be aligning with ODBC for native relational data access in SQL Server, and encouraged everybody else to do the same. They further stated that SQL Server releases after v11/SQL Server 2012 would actively not support OLE DB!
This announcement provoked a storm of protest. People were at a loss to understand why MS was suddenly deprecating a technology that they had spent years getting them to commit to. In addition, SSAS/SSRS and SSIS, which were MS-written applications intimately linked to SQL Server, were wholly or partly dependent on OLE DB. Yet another complaint was that OLE DB had certain desirable features which it seemed impossible to port back to ODBC – after all, OLE DB had many good points.
In October 2017, Microsoft relented and officially un-deprecated OLE DB. They announced the imminent arrival of a new driver (MSOLEDBSQL) which would have the existing feature set of the Native Client 11 and would also introduce multi-subnet failover and TLS 1.2 support. The driver was released in March 2018.

-
-
Şu linkteki kod incelenebilir: https://docs.microsoft.com/tr-tr/dotnet/framework/data/adonet/ado-net-code-examples
İngilizce olarak object linking and embedding database kelimelerinin baş harflerinden oluşan bir veritabanı terimidir. Aslında OLE terimi (object linking and embedding) Microsoft firması tarafından kazandırılan ve uygulama geliştirme sırasında bir varlığı programa bağlamaya yarayan mantıktır.
Örneğin bir programda bulunan bir resim, programın içerisine dahil edilip çalıştırılabilir kodun (executable code) bir parçası olabileceği gibi, bir resim dosyası olarak hariçte tutulup programın çalışması sırasında bu harici dosyadan okunabilir. Bir varlığın bu şekilde programa dahil edilmesine microsoft terminolojisinde COM (component object model) ismi verilirken hariçte tutulmasına ole ismi verilir.
OLEDB terimi ise bu açıklamadan anlaşılacağı üzere program ile veri tabanı arasındaki bağlantının hariçte tutulması anlamındadır. Aslında MDAC ismi verilen ve microsoft tarafından geliştirilen microsoft data access components (microsoft veri erişim bileşenleri) ailesinin bir üyesi olan OLEDB, kısaca microsoft ortamlarında veritabanına (DB database) erişmek için kullanılan çeşitli yöntemlerden birisidir. Yapı olarak ODBC bağlantısına benzetilebilir.
OLEDB kullanımını daha net anlayabilmek için C# dilinde yazılmış aşağıdaki kodu anlamaya çalışalım:
using System;
using System.Data.OleDb;
public class ReadFromOleDb
{
  [STAThread]
  static void Main(string[] args)
  {
    String sConn = "provider=sqloledb;server=(local)\SQLEXPRESS;database=veritabani;Integrated Security=SSPI";
    String sSQL = "select id, firstname, lastname from Employee";
    OleDbConnection oConn = new OleDbConnection(sConn);
    oConn.Open();
    OleDbCommand oCmd = new OleDbCommand(sSQL, oConn);
    OleDbDataReader oReader = oCmd.ExecuteReader();

    int idxID = oReader.GetOrdinal("id");
    int idxFirstName = oReader.GetOrdinal("firstname");
    int idxLastName = oReader.GetOrdinal("lastname");

    while(oReader.Read()) {
      Console.WriteLine("{0} {1} {2}",
        oReader.GetValue(idxID),
        oReader.GetValue(idxFirstName),
        oReader.GetValue(idxLastName));
    }
  }
}
Yukarıdaki kodda dikkat edileceği üzere System.Data.OleDb kütüphanesi kullanılmıştır. OleDb ile veritabanı işlemini 3 adımda inceleyebiliriz:
1. Veritabanı bağlantısının kurulması
2. Bağlantı üzerinde bir sorgunun (query) çalıştırılması
3. Sorgu sonucunda (şayet sorgu bir sonuç kümesi döndürüyorsa (result set) ) sonuç kümesi üzerinde işlem yapılması
Bu sıralanan adımları yukarıdaki kodda belirleyecek olursak ilk adım olan veritabanı bağlantısı
String sConn = "provider=sqloledb;server=(local)\SQLEXPRESS;database=veritabani;Integrated Security=SSPI";
OleDbConnection oConn = new OleDbConnection(sConn);
oConn.Open();
Şeklinde 3 ayrı satırda kurulmuştur.  Buradaki provider terimi sqlodedb olarak belirlenmiştir. Bunun anlamı bilgisayarımızda bu bağlantıyı sağlamaya yarayan bir sürücü (driver) bulunması gerektiği ve bağlantının isminin tam olarak sqloledb olmasıdır.
Ardından server= ile başlayan kısımda (local) yazılmıştır. Burada da aynı bilgisayar üzerinde bulunan veri tabanına bağlanılacağı anlaşılıyor. Şayet farklı bir sunucudaki bir veritabanına bağlanılması istenirse bu durumda sunucunun internet addresi ( IP address ) veya ağ kurulumuna göre  ismi yazılmalıdır.
Bu yazı şadi evren şeker tarafından yazılmış ve bilgisayarkavramlari.com sitesinde yayınlanmıştır. Bu içeriğin kopyalanması veya farklı bir sitede yayınlanması hırsızlıktır ve telif hakları yasası gereği suçtur.
database= ile başlayan kısma ise veri tabanı sunucumuzda bulunan ve bağlanılmak istenen veritabanının ismi yazılır.
Integrated Security ile SSPI ayarlanmıştır. SSPI aslında detaylıca anlatılabilecek bir güvenlik modülü olmasına karşılık bu yazıda sadece Security Support Provider Interface kelimelerinin baş harfinden oluştuğunu ve bağlantı sırasında gidip gelen verinin güvenli bir şekilde saklanmasının hedeflendiğini söylemek ile yetinelim.
Yukarıdaki bağlantıyı gösteren ikinci satırda OleDbConnection sınıfından (class) bir nesne (object) oluşturuyoruz ve ismi de oConn olarak veriliyor. Burada OleDbConnection yapıcısına (constructor) parametre olarak bir önceki saıtrda tanımladığımız dizgiyi (string) veriyoruz.
Son olarak oConn.open() metodunu çağırarak bağlantıyı açıyoruz.
Bağlantı açıldıktan sonra, bu bağlantı üzerinden artık sorgu (query) çalıştırılabilir. Sorgu çalıştıran satırlarımız aşağıda verilmiştir:
String sSQL = "select id, firstname, lastname from Employee";
OleDbCommand oCmd = new OleDbCommand(sSQL, oConn);
OleDbDataReader oReader = oCmd.ExecuteReader();
Yukarıdaki 3 satırda sırasıyla bir sorgu cümlesi (query string) girilmiş, bu cümle daha önceden açılmış olan bağlantı üzerinde çalıştırılmış ve son olarak sorgunun sonucunu okumak üzere OleDbDataReader sınıfından (class) bir nesne üretilmiştir.
Üçüncü kısımda bu sorgunun sonucu işlenmektedir.
oReader nesnesinden getOrdinal fonksiyonu ile veritabanımızda bulunan Employee tablosundaki ilgili kolonları (column) bağlayabiliyoruz:
oReader.GetOrdinal("id");
Örneğin yukarıdaki kodda “id” isimli kolon alınmıştır. Daha sonra bu bağlantılar üzerinden satır satır okuma yaparak verileri çekmemiz mümkün. while döngüsünün (loop) içindeki satırlarda da bu şekilde veri çekilerek Console.write komutu ile ekrana basılmaktadır.
-
-
-
NoSQL	  (GITHUB)  (test) 
Nosql nedir
İlişkisiz veritabanıdır. NoSQL is not a relational database.
Bir NoSQL veritabanında, kayıtlar genellikle bir JSON belgesi olarak saklanır.
NoSQL (ilişkisel olmayan) Veritabanları Nasıl Çalışır?
NoSQL is a whole new way of thinking about a database. NoSQL is not a relational database. The reality is that a relational database model may not be the best solution for all situations. The easiest way to think of NoSQL, is that of a database which does not adhering to the traditional relational database management system (RDMS) structure. Sometimes you will also see it revered to as 'not only SQL'.
Basit bir kitap veritabanının şemasını modelleme örneğini ele alalım:
•	İlişkisel bir veritabanında genellikle bir kitap kaydı gizlenerek (veya “normalleştirilerek”) ayrı tablolarda depolanırken, ilişkiler birincil ve yabancı anahtar kısıtlamaları tarafından tanımlanır. Bu örnekte Books (Kitaplar) tablosu ISBN, Book Title (Kitap Başlığı) ve Edition Number (Baskı Sayısı) sütunlarına; Authors (Yazarlar) tablosu AuthorID (Yazar Kimliği) ve Author Name (Yazar Adı) sütunlarına; Author-ISBN (Yazar-ISBN) tablosu ise AuthorID (Yazar Kimliği) ve ISBN sütunlarına sahiptir. İlişkisel model, veritabanının yedekliliği azaltacak şekilde normalleştirilmiş ve genel olarak depolama için optimize edilmiş tablolar arasında başvurusal bütünlük uygulamasına imkan tanıyacak şekilde tasarlanmıştır.
•	Bir NoSQL veritabanında, kitap kaydı genellikle bir JSON belgesi olarak saklanır. Her kitap için öğe, ISBN, Book Title (Kitap Başlığı), Edition Number (Baskı Sayısı), Author Name (Yazar Adı) ve AuthorID (Yazar Kimliği) bilgileri tek bir belgede öznitelikler olarak depolanır. Bu modelde, veriler sezgisel yazılım geliştirme ve yatay ölçeklenebilirlik için optimize edilir.

Neden NoSQL veritabanı kullanmalısınız?
NoSQL veritabanları, harika kullanıcı deneyimleri sunulması amacıyla esnek, ölçeklenebilir, yüksek performanslı ve yüksek oranda işlevsel veritabanları gerektiren mobil, web ve oyun gibi birçok modern uygulama için idealdir.
•	Esneklik: NoSQL genellikle daha hızlı ve daha fazla yinelemeli yazılım geliştirmeyi mümkün kılan esnek şemalar sağlar. Esnek veri modeli sayesinde NoSQL veritabanları yarı yapılandırılmış ve yapılandırılmamış veriler için idealdir.
•	Ölçeklenebilirlik: NoSQL veritabanları genellikle pahalı ve kalıcı sunucular eklenerek ölçeği artırılabilecek şekilde değil, dağıtılmış donanım kümeleri kullanılarak ölçeği genişletilebilecek şekilde tasarlanır. Bazı bulut sağlayıcıları bu işlemleri arka planda, tam olarak yönetilen bir hizmet olarak gerçekleştirir.
•	Yüksek performans: NoSQL veritabanları, benzer işlevlerin ilişkisel veritabanlarıyla gerçekleştirilmesi ile karşılaştırıldığında daha yüksek performansı mümkün kılan belirli veri modelleri (belge, anahtar-değer, grafik gibi) ve erişim desenleri için optimize edilmiştir.
•	Yüksek oranda işlevsel: NoSQL veritabanları, her biri ilgili veri modeli için özel olarak tasarlanmış yüksek oranda işlevsel API'ler ve veri türleri sağlar.
BEST NOSQL DATABASES 2020
•	MongoDB
•	It is an open-source NoSQL database which is document-oriented. MongoDB uses JSON like documents to store any data. It is written in C++.
•	Cassandra
•	It was developed at Facebook for inbox search. Cassandra is a distributed data storage system for handling very large amounts of structured data.
•	Redis
•	Redis is the most famous key-value store. Redis is composed in C language. It is authorized under BSD.
•	HBase
•	It is a distributed and non-relational database which is designed for the BigTable database by Google.
•	Neo4j
•	Neo4j is referred to as a native graph database because it effectively implements the property graph model down to the storage level.
•	Oracle NoSQL
•	Oracle NoSQL Database implements a map from user-defined keys to opaque data items.
•	Amazon DynamoDB
•	DynamoDB uses a NoSQL database model, which is nonrelational, allowing documents, graphs and columnar among its data models.
•	Couchbase
•	Couchbase Server is a NoSQL document database for interactive web applications. It has a flexible data model, is easily scalable, provides consistently high performance.
•	Memcached
•	It is an open source, high-performance, distributed memory caching system intended to speed up dynamic web applications by reducing the database load.
•	CouchDB
•	It is an Open Source NoSQL Database which utilizes JSON to store information and JavaScript as its query language.
-
-
-
MongoDb	(GITHUB)  (test) 
Mongodb nedir

MongoDB is the most well known among NoSQL Databases. It is an Open-Source database which is Document-oriented. MongoDB is a scalable and accessible database. It is in C++. MongoDB can likewise be utilized as the file system. In MongoDB, JavaScript can be utilized as the query language. By utilizing sharding MongoDB scales horizontally. It is very useful in Popular JavaScript Frameworks. People really enjoying sharding, advanced text searching, gridFS, map-reduce features for the 2019 year. Amazing performance and new features promoted this NoSQL database to 1st place in our list. 
FEATURES
•	Provides high performance
•	Auto-sharding
•	Run over multiple servers
•	Supports Master-Slave replication
•	Data is stored in the form of JSON style documents
•	index any field in a document
•	It has an automatic load balancing configuration because of data placed in shards
•	Supports regular expression searches
•	Easy to administer in the case of failures
PROS OF MONGODB
•	Easy to setup MongoDB
•	MongoDB Inc. provides professional support to its clients
•	Support ad-hoc query
•	High-Speed Database
•	Schema-less database
•	Horizontally scalable database
•	Performance is very high
CONS OF MONGODB
•	Doesn’t support joins
•	Data Size is High
•	Nesting of documents is limited
•	Increase unnecessary usage of memory


Bir veritabanıdır, ilişkisiz veritabanıdır -> MongoDB is a general purpose, document-based, distributed database built for modern application developers and for the cloud era. No database makes you more productive.
-
-
-
Cassandra	(GITHUB)  (test) 
Cassandra nedir
 
Cassandra NoSQL database in top 10
Cassandra was developed at Facebook for inbox search. Cassandra is a distributed data storage system for handling very large amounts of structured data. Generally, these data are spread out across many commodity servers. You can also add storage capacity of your data keeping your service online and you can do this task easily. As all the nodes in a cluster are same, there is no complex configuration to deal with. Cassandra is written in Java. (CQL nedir) Cassandra Query Language (CQL) is a SQL-like language for querying Cassandra Database. As a result, Cassandra stands 2nd in best open source databases. Cassandra is being used by some of the biggest companies such as Facebook, Twitter, Cisco, Rackspace, eBay, Twitter, Netflix, and more.
•	Developed by: Apache Software Foundation
FEATURES
•	Linearly scalable
•	Maintains a quick response time
•	Supports properties like Atomicity, Consistency, Isolation, and Durability (ACID)
•	Supports MapReduce with Apache Hadoop
•	Maximal flexibility to distribute the data
•	Highly scalable
•	Peer-to-peer architecture
PROS OF CASSANDRA
•	Highly scalable
•	No single point of failure
•	Multi-DC Replication
•	Integrate tightly with other JVM based applications
•	More suitable for multiple data-center deployments, redundancy, failover and disaster recovery
CONS OF CASSANDRA
•	Limited support for aggregations
•	Unpredictable Performance
•	Doesn’t Support ad-hoc query
-
-
-
Redis (Remote Dictionary Server)  	(GITHUB)  (test) 
Redis nedir
İsmine dikkat, remote Sözlük server’ı
 
Redis NoSQL database
Redis(Remote Dictionary Server) is a key-value store. Furthermore, it is the most famous key-value store. Redis has support for some C++, PHP, Ruby, Python, Perl, Scala and so forth. Redis is composed in C language. Furthermore, it is authorized under BSD. Some fun facts about Redis NoSQL Database – It can handle up to 2 ³² keys and was tested in practice to handle at least 250 million keys per instance. It is an in-memory but persistent on-disk database. It means it will store all data in RAM only for backup only use disk(HDD or SSD).
FEATURES
•	Automatic failover
•	Holds its database entirely in the memory
•	Transactions
•	Lua scripting
•	Replicate data to any number of slaves
•	Keys with a limited time-to-live
•	LRU eviction of keys
•	Supports Publish/Subscribe
PROS OF REDIS
•	Supports a huge variety of data types
•	Easy to install
•	Very fast(perform about 110000 SETs per second, about 81000 GETs per second)
•	Operations are atomic
•	Multi-utility tool(used in a number of use cases)
•	Redis Sentinel is featured provided by Redis to create replication into a distributed system.
Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes with radius queries and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.

Redis (/ˈrɛdɪs/; Remote Dictionary Server) is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. The project is mainly developed by Salvatore Sanfilippo and as of 2019, is sponsored by Redis Labs.[8] It is open-source software released under a BSD 3-clause license.

History
The Redis project began when Salvatore Sanfilippo, nicknamed antirez, the original developer of Redis, was trying to improve the scalability of his Italian startup, developing a real-time web log analyzer. After encountering significant problems in scaling some types of workloads using traditional database systems, Sanfilippo began to prototype a first proof of concept version of Redis in Tcl.[9] Later Sanfilippo translated that prototype to the C language and implemented the first data type, the list. After a few weeks of using the project internally with success, Sanfilippo decided to open source it, announcing the project on Hacker News. The project began to get traction, more so among the Ruby community, with GitHub and Instagram being among the first companies adopting it. 
-
-
-
acid için basit anlatım. venkat.
https://www.youtube.com/watch?v=VLc4ewu6lUI

-

Atomicity: The atomicity acid property in SQL. It means either all the operations (insert, update, delete) inside a transaction take place or none. Or you can say, all the statements (insert, update, delete) inside a transaction are either completed or rolled back.
-

A transaction can be defined as a group of tasks. A single task is the minimum processing unit which cannot be divided further.
Let’s take an example of a simple transaction. Suppose a bank employee transfers Rs 500 from A's account to B's account. This very simple and small transaction involves several low-level tasks.
A’s Account
Open_Account(A)
Old_Balance = A.balance
New_Balance = Old_Balance - 500
A.balance = New_Balance
Close_Account(A)
B’s Account
Open_Account(B)
Old_Balance = B.balance
New_Balance = Old_Balance + 500
B.balance = New_Balance
Close_Account(B)
ACID Properties
A transaction is a very small unit of a program and it may contain several lowlevel tasks. A transaction in a database system must maintain Atomicity, Consistency, Isolation, and Durability − commonly known as ACID properties − in order to ensure accuracy, completeness, and data integrity.
•	Atomicity − This property states that a transaction must be treated as an atomic unit, that is, either all of its operations are executed or none. There must be no state in a database where a transaction is left partially completed. States should be defined either before the execution of the transaction or after the execution/abortion/failure of the transaction.
•	Consistency − The database must remain in a consistent state after any transaction. No transaction should have any adverse effect on the data residing in the database. If the database was in a consistent state before the execution of a transaction, it must remain consistent after the execution of the transaction as well.
•	Durability − The database should be durable enough to hold all its latest updates even if the system fails or restarts. If a transaction updates a chunk of data in a database and commits, then the database will hold the modified data. If a transaction commits but the system fails before the data could be written on to the disk, then that data will be updated once the system springs back into action.
•	Isolation − In a database system where more than one transaction are being executed simultaneously and in parallel, the property of isolation states that all the transactions will be carried out and executed as if it is the only transaction in the system. No transaction will affect the existence of any other transaction.
Serializability
When multiple transactions are being executed by the operating system in a multiprogramming environment, there are possibilities that instructions of one transactions are interleaved with some other transaction.
•	Schedule − A chronological execution sequence of a transaction is called a schedule. A schedule can have many transactions in it, each comprising of a number of instructions/tasks.
•	Serial Schedule − It is a schedule in which transactions are aligned in such a way that one transaction is executed first. When the first transaction completes its cycle, then the next transaction is executed. Transactions are ordered one after the other. This type of schedule is called a serial schedule, as transactions are executed in a serial manner.
In a multi-transaction environment, serial schedules are considered as a benchmark. The execution sequence of an instruction in a transaction cannot be changed, but two transactions can have their instructions executed in a random fashion. This execution does no harm if two transactions are mutually independent and working on different segments of data; but in case these two transactions are working on the same data, then the results may vary. This ever-varying result may bring the database to an inconsistent state.
To resolve this problem, we allow parallel execution of a transaction schedule, if its transactions are either serializable or have some equivalence relation among them.
Equivalence Schedules
An equivalence schedule can be of the following types −
Result Equivalence
If two schedules produce the same result after execution, they are said to be result equivalent. They may yield the same result for some value and different results for another set of values. That's why this equivalence is not generally considered significant.
View Equivalence
Two schedules would be view equivalence if the transactions in both the schedules perform similar actions in a similar manner.
For example −
•	If T reads the initial data in S1, then it also reads the initial data in S2.
•	If T reads the value written by J in S1, then it also reads the value written by J in S2.
•	If T performs the final write on the data value in S1, then it also performs the final write on the data value in S2.
Conflict Equivalence
Two schedules would be conflicting if they have the following properties −
•	Both belong to separate transactions.
•	Both accesses the same data item.
•	At least one of them is "write" operation.
Two schedules having multiple transactions with conflicting operations are said to be conflict equivalent if and only if −
•	Both the schedules contain the same set of Transactions.
•	The order of conflicting pairs of operation is maintained in both the schedules.
Note − View equivalent schedules are view serializable and conflict equivalent schedules are conflict serializable. All conflict serializable schedules are view serializable too.
States of Transactions
A transaction in a database can be in one of the following states −
 
•	Active − In this state, the transaction is being executed. This is the initial state of every transaction.
•	Partially Committed − When a transaction executes its final operation, it is said to be in a partially committed state.
•	Failed − A transaction is said to be in a failed state if any of the checks made by the database recovery system fails. A failed transaction can no longer proceed further.
•	Aborted − If any of the checks fails and the transaction has reached a failed state, then the recovery manager rolls back all its write operations on the database to bring the database back to its original state where it was prior to the execution of the transaction. Transactions in this state are called aborted. The database recovery module can select one of the two operations after a transaction aborts −
o	Re-start the transaction
o	Kill the transaction
•	Committed − If a transaction executes all its operations successfully, it is said to be committed. All its effects are now permanently established on the database system.

-
Wikipediden, güzel görünüyo baya:
In computer science, ACID (atomicity, consistency, isolation, durability) is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.
In 1983,[1] Andreas Reuter and Theo Härder coined the acronym ACID, building on earlier work by Jim Gray[2] who named atomicity, consistency, and durability, but not isolation, when characterizing the transaction concept. These four properties are the major guarantees of the transaction paradigm, which has influenced many aspects of development in database systems.
According to Gray and Reuter, the IBM Information Management System supported ACID transactions as early as 1973 (although the acronym was created later).[3]
Characteristics
The characteristics of these four properties as defined by Reuter and Härder are as follows:
Atomicity
	Main article: Atomicity (database systems)

Transactions are often composed of multiple statements. Atomicity guarantees that each transaction is treated as a single "unit", which either succeeds completely, or fails completely: if any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged. An atomic system must guarantee atomicity in each and every situation, including power failures, errors and crashes.[4] A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).
An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a consistent state, that is, money is neither debited nor credited if either of those two operations fail.[5]
Consistency
	Main article: Consistency (database systems)

Consistency ensures that a transaction can only bring the database from one valid state to another, maintaining database invariants: any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers, and any combination thereof. This prevents database corruption by an illegal transaction, but does not guarantee that a transaction is correct. Referential integrity guarantees the primary key – foreign key relationship. [6]
Isolation
	Main article: Isolation (database systems)

Transactions are often executed concurrently (e.g., multiple transactions reading and writing to a table at the same time). Isolation ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially. Isolation is the main goal of concurrency control; depending on the method used, the effects of an incomplete transaction might not even be visible to other transactions. [7]
Durability
	Main article: Durability (database systems)

Durability guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash). This usually means that completed transactions (or their effects) are recorded in non-volatile memory.[citation needed]
Examples
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2018) (Learn how and when to remove this template message)
The following examples further illustrate the ACID properties. In these examples, the database table has two columns, A and B. An integrity constraint requires that the value in A and the value in B must sum to 100. The following SQL code creates a table as described above:
CREATE TABLE acidtest (A INTEGER, B INTEGER, CHECK (A + B = 100));
Atomicity
Atomicity is the guarantee that series of database operations in an atomic transaction will either all occur (a successful operation), or none will occur (an unsuccessful operation). The series of operations cannot be separated with only some of them being executed, which makes the series of operations "indivisible". A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. In other words, atomicity means indivisibility and irreducibility.[8] Alternatively, we may say that a logical transaction may be made of, or composed of, one or more (several), physical transactions. Unless and until all component physical transactions are executed, the Logical transaction will not have occurred – to the effects of the database. Say our Logical transaction consists of transferring funds from account A to account B. This logical transaction may be composed of several physical transactions consisting of first removing the amount from account A as a first physical transaction and then, as a second transaction, depositing said amount in account B. We would not want to see the amount removed from account A before we are sure it has been transferred into account B. Then, unless and until both transactions have happened and the amount has been transferred to account B, the transfer has not, to the effects of the database, occurred.
Consistency failure
Consistency is a very general term, which demands that the data must meet all validation rules. In the previous example, the validation is a requirement that A + B = 100. All validation rules must be checked to ensure consistency. Assume that a transaction attempts to subtract 10 from A without altering B. Because consistency is checked after each transaction, it is known that A + B = 100 before the transaction begins. If the transaction removes 10 from A successfully, atomicity will be achieved. However, a validation check will show that A + B = 90, which is inconsistent with the rules of the database. The entire transaction must be cancelled and the affected rows rolled back to their pre-transaction state. If there had been other constraints, triggers, or cascades, every single change operation would have been checked in the same way as above before the transaction was committed. Similar issues may arise with other constraints. We may have required the data types of both A and B to be integers. If we were then to enter, say, the value 13.5 for A, the transaction will be cancelled, or the system may give rise to an alert in the form of a trigger (if/when the trigger has been written to this effect). Another example would be with integrity constraints, which would not allow us to delete a row in one table whose primary key is referred to by at least one foreign key in other tables.
Isolation failure
To demonstrate isolation, we assume two transactions execute at the same time, each attempting to modify the same data. One of the two must wait until the other completes in order to maintain isolation.
Consider two transactions: T1 transfers 10 from A to B. T2 transfers 20 from B to A.
Combined, there are four actions:
1.	T1 subtracts 10 from A.
2.	T1 adds 10 to B.
3.	T2 subtracts 20 from B.
4.	T2 adds 20 to A.
If these operations are performed in order, isolation is maintained, although T2 must wait. Consider what happens if T1 fails halfway through. The database eliminates T1's effects, and T2 sees only valid data.
By interleaving the transactions, the actual order of actions might be:
1.	T1 subtracts 10 from A.
2.	T2 subtracts 20 from B.
3.	T2 adds 20 to A.
4.	T1 adds 10 to B.
Again, consider what happens if T1 fails while modifying B in Step 4. By the time T1 fails, T2 has already modified A; it cannot be restored to the value it had before T1 without leaving an invalid database. This is known as a write-write conflict,[citation needed] because two transactions attempted to write to the same data field. In a typical system, the problem would be resolved by reverting to the last known good state, canceling the failed transaction T1, and restarting the interrupted transaction T2 from the good state.
Durability failure
Consider a transaction that transfers 10 from A to B. First it removes 10 from A, then it adds 10 to B. At this point, the user is told the transaction was a success. However, the changes are still queued in the disk buffer waiting to be committed to disk. Power fails and the changes are lost, but the user assumes (understandably) that the changes persist.
Implementation
This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2015) (Learn how and when to remove this template message)
Processing a transaction often requires a sequence of operations that is subject to failure for a number of reasons. For instance, the system may have no room left on its disk drives, or it may have used up its allocated CPU time. There are two popular families of techniques: write-ahead logging and shadow paging. In both cases, locks must be acquired on all information to be updated, and depending on the level of isolation, possibly on all data that may be read as well. In write ahead logging, durability is guaranteed by copying the original (unchanged) data to a log before changing the database.[dubious – discuss] That allows the database to return to a consistent state in the event of a crash. In shadowing, updates are applied to a partial copy of the database, and the new copy is activated when the transaction commits.
Locking vs multiversioning
Many databases rely upon locking to provide ACID capabilities. Locking means that the transaction marks the data that it accesses so that the DBMS knows not to allow other transactions to modify it until the first transaction succeeds or fails. The lock must always be acquired before processing data, including data that is read but not modified. Non-trivial transactions typically require a large number of locks, resulting in substantial overhead as well as blocking other transactions. For example, if user A is running a transaction that has to read a row of data that user B wants to modify, user B must wait until user A's transaction completes. Two phase locking is often applied to guarantee full isolation.
An alternative to locking is multiversion concurrency control, in which the database provides each reading transaction the prior, unmodified version of data that is being modified by another active transaction. This allows readers to operate without acquiring locks, i.e., writing transactions do not block reading transactions, and readers do not block writers. Going back to the example, when user A's transaction requests data that user B is modifying, the database provides A with the version of that data that existed when user B started his transaction. User A gets a consistent view of the database even if other users are changing data. One implementation, namely snapshot isolation, relaxes the isolation property.
Distributed transactions
	Main article: Distributed transaction

Guaranteeing ACID properties in a distributed transaction across a distributed database, where no single node is responsible for all data affecting a transaction, presents additional complications. Network connections might fail, or one node might successfully complete its part of the transaction and then be required to roll back its changes because of a failure on another node. The two-phase commit protocol (not to be confused with two-phase locking) provides atomicity for distributed transactions to ensure that each participant in the transaction agrees on whether the transaction should be committed or not.[9] Briefly, in the first phase, one node (the coordinator) interrogates the other nodes (the participants) and only when all reply that they are prepared does the coordinator, in the second phase, formalize the transaction.
-
-
-
DML (Data Manipulation Language)  (GITHUB)  (test)
Dml nedir
Veritabanındaki veri üzerinde değişiklik yapmanı sağlayan yapı. 

SELECT, INSERT, UPDATE, DELETE'DEN OLUŞUR (WIKI ENGLISH)

Bilgiyi çağırma, bilgiye yeni bir şeyler ekleme, bilgiden bir şeyler silme, bilgiyi güncelleştirme işlemlerini yapar. Temel dört komutu vardır:
Select: Veritabanındaki nesneden alan çağırmayı ve gereken bilgiyi görebilmeyi sağlar.
İnsert: Nesneye alan eklemeyi sağlar.
Update: Nesnede bilgi değiştirmeyi sağlar.
Delete: Nesneden alan silmeyi sağlar.
-
A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language.[1] Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing.
A popular data manipulation language is that of Structured Query Language (SQL), which is used to retrieve and manipulate data in a relational database.[2] Other forms of DML are those used by IMS/DLI, CODASYL databases, such as IDMS and others.
SQL
In SQL, the data manipulation language comprises the SQL-data change statements,[3] which modify stored data but not the schema or database objects. Manipulation of persistent database objects, e.g., tables or stored procedures, via the SQL schema statements,[3] rather than the data stored within them, is considered to be part of a separate data definition language (DDL). In SQL these two categories are similar in their detailed syntax, data types, expressions etc., but distinct in their overall function.[3]
The SQL-data change statements are a subset of the SQL-data statements; this also contains the SELECT query statement,[3] which strictly speaking is part of the DQL, not the DML. In common practice though, this distinction is not made and SELECT is widely considered to be part of DML,[4] so the DML consists of all SQL-data statements, not only the SQL-data change statements. The SELECT ... INTO ... form combines both selection and manipulation, and thus is strictly considered to be DML because it manipulates (i.e. modifies) data.
Data manipulation languages have their functional capability organized by the initial word in a statement, which is almost always a verb. In the case of SQL, these verbs are:
•	SELECT ... FROM ... WHERE ... (strictly speaking DQL)
•	SELECT ... INTO ...
•	INSERT INTO ... VALUES ...
•	UPDATE ... SET ... WHERE ...
•	DELETE FROM ... WHERE ...
For example, the command to insert a row into table employees:
INSERT INTO employees (first_name, last_name, fname) VALUES ('John', 'Capita', 'xcapit00');
-
-
-
-
MVC örneğinde: class / Property / Entity / Attribute 

   public class Book
    {
        public int Id { get; set; }

        [Required]
        [DisplayName("Kitap Adı"), DisplayFormat(NullDisplayText = "Veri yok"), StringLength(30, ErrorMessage = "30 haneden fazla hane girdiniz."), MinLength(2, ErrorMessage = "2 hanedan az hane girdiniz.")]
        public string BookName { get; set; }
    }

Yukarıdaki örnekte Book class’ını oluşturduk. 
Bu class’ın herhangi bir örneğine Entity denir.
Property’nin üzerine yazdığımız öğelere Attribute denir. Bu durumda [Required], [DisplayName] bir Attribute’dir. (ayrıca Data Annotation)
-
-
-
-
Six Benefits Of Using MVC Model For Effective Web Application Development
 
Over the years, software development has gone through many changes. One of the biggest changes that happened in recent years, is the use of MVC patterns for developing software or web application. The Model–view–controller shortly known as MVC is a software architectural design for implementing user interfaces on computers. The MVC pattern is a great architecture no matter whatever the language you are using for the development.
How Does MVC Pattern Work?
MVC patterns separate the input, processing, and output of an application. This model divided into three interconnected parts called the model, the view, and the controller. All of the three above given components are built to handle some specific development aspects of any web or .net application development.
In the MVC application development, the controller receives all requests for the application and then instructs the model to prepare any information required by the view. The view uses that data prepared by the controller to bring the final output.
Three Levels Of MVC Model:
Model:
This level is very important as it represents the data to the user. This level defines where the application’s data objects are stored. The model doesn’t know anything about views and controllers. So, whenever there are changes done in the model it will automatically notify observers that the changes are made. The model may be a single object or a structure of objects.
Views:
A view is a visual representation of the MVC model. This level creates an interface to show the actual output to the user. However, a view will not display anything itself. It is the controller or model that tells view of what to display to the user. It also handles requests from the user and informs the controller. A view is connected to its model and gets the data necessary for the presentation by asking certain questions. Sometimes, it also updates the model by sending appropriate messages. All these questions and messages are sent back to the model in such an easy terminology that can easily understand the information sent by a model or a controller.
Controller:
The controller is a level that acts as the brain of the entire MVC system. A controller also acts as a link between a user and the system. It provides the user with the input by providing appropriate views to present it appropriately on the screen. The controller understands user output, converts it into the appropriate messages and passes the same to views.
Advantages Of Using MVC Framework
1. Faster Development Process:
MVC supports rapid and parallel development. If an MVC model is used to develop any particular web application then it is possible that one programmer can work on the view while the other can work on the controller to create the business logic of the web application. Hence this way, the application developed using the MVC model can be completed three times faster than applications that are developed using other development patterns.
2. Ability To Provide Multiple Views:
In the MVC Model, you can create multiple views for a model. Today, there is an increasing demand for new ways to access your application and for that MVC development is certainly a great solution. Moreover, in this method, Code duplication is very limited because it separates data and business logic from the display.
3. Support For Asynchronous Technique:
The MVC architecture can also integrate with the JavaScript Framework. This means that MVC applications can be made to work even with PDF files, site-specific browsers, and also with desktop widgets. MVC also supports an asynchronous technique, which helps developers to develop an application that loads very fast.
4. The Modification Does Not Affect The Entire Model:
For any web application, the user interface tends to change more frequently than even the business rules of the .net development company. It is obvious that you make frequent changes in your web application like changing colors, fonts, screen layouts, and adding new device support for mobile phones or tablets. Moreover, Adding a new type of view are very easy in the MVC pattern because the Model part does not depend on the views part. Therefore, any changes in the Model will not affect the entire architecture.
5. MVC Model Returns The Data Without Formatting:
MVC pattern returns data without applying any formatting. Hence, the same components can be used and called for use with any interface. For example, any kind of data can be formatted with HTML, but it could also be formatted with Macromedia Flash or Dream viewer.
6. SEO Friendly Development Platform:
MVC platform supports the development of SEO friendly web pages or web applications. Using this platform, it is very easy to develop SEO-friendly URLs to generate more visits from a specific application. This development architecture is commonly used in Test-Driven Development applications. Moreover, Scripting languages like JavaScript and jQuery can be integrated with MVC to develop feature-rich web applications.
Thus, the MVC design pattern is surely a great approach to building software applications. The MVC framework is easy to implement as it offers above given numerous advantages. Projects that are developed with the help of the MVC model can be easily developed with lesser expenditure and within less time too. Above all, its power to manage multiple views makes MVC the best architecture pattern for developing web applications.
As a result, today organizations are looking for the .net development of web applications based on MVC architecture for cost and time benefits. There are many web development companies providing MVC development services to develop web applications that satisfy every requirement of the clients. Brainvire is one such .net development company that provides the most desired output to its clients by offering fast and highly interactive web applications using MVC 6 development architecture. Contact us today
-
-
-
-What is Programming Language?
A programming language is a notation designed to connect instructions to a machine or a computer. Programming languages are mainly  used to control the performance of a machine or to express algorithms. At present, thousand programming languages have been implemented. In the computer field, many languages need to be stated in an imperative form, while other programming languages utilize declarative form. The program can be divided into two forms such as syntax and semantics. Some languages are defined by an SO standard like C language.
Types of Programming Languages
The different types of programming languages are discussed below.
 
Procedural Programming Language
The procedural programming language is used to execute a sequence of statements which lead to a result. Typically, this type of programming language uses multiple variables, heavy loops and other elements, which separates them from functional programming languages. Functions of procedural language may control variables, other than function’s value  returns. For example, printing out information.
Functional Programming Language
Functional programming language typically uses stored data, frequently avoiding loops in favor of recursive functions.The functional programing’s  primary focus is on the return values of functions, and side effects and different suggests that storing state are powerfully discouraged. For example, in an exceedingly pure useful language, if a function is termed, it’s expected that the function not modify or perform any o/p. It may, however, build algorithmic calls and alter the parameters of these calls. Functional languages are usually easier and build it easier to figure on abstract issues, however, they’ll even be “further from the machine” therein their programming model makes it difficult to know precisely, but the code is decoded into machine language (which are often problematic for system programming).
Object-oriented Programming Language
This programming language  views the world as a group of objects that have internal data and external accessing parts of that data. The aim this programming language  is to think about the fault by separating it into a collection of objects that offer services which can be used to solve a specific problem. One of the main principle of object oriented programming language  is encapsulation that everything an object will need must be inside of the object. This language also emphasizes reusability through inheritance and the capacity to spread current implementations without having to change a great deal of code by using polymorphism.
Scripting Programming Language
These programming languages are often procedural and may comprise object-oriented language elements, but they fall into their own category as they are normally not full-fledged programming languages with support for development of large systems. For example, they may not have compile-time type checking. Usually, these languages require tiny syntax to get started.
Logic Programming Language
These types of languages let programmers  make declarative statements and then allow the machine to reason about the consequences of those statements. In a sense, this language doesn’t tell the computer how to do something, but employing restrictions on what it must consider doing.
To call these groups ” types  of language ” is really a bit confusing. It’s easy to program in an object-oriented style in C language. In truth, most of the languages include ideas and features from various domains, which only helps to increase the  usefulness of these types of languages. Nevertheless, most of the programming languages do not best in all styles of programming.
The Difference Between Different Programming Languages
C++ Language
The C++ language has an object oriented structure which is used in large projects. Programmers can collaborate one program into different parts or even one individual work on each part of the program. The structure of object oriented also permit code to be reused many times.This language is an efficient language. But, many programmers will disagree
 
C++ Language
C Language
The C language is a  basic programming language and it is a very popular language, particularly used in game programming, Because C language includes the additional packing of the C++, Every programmer uses this language because it makes programs faster . However the value of this language gives the reusability of C++ to get the slight increase in performance with C language.
 
C Language
Pascal Language
Pascal language is mostly a teaching language and few industries uses this language to write the programs. This language tends to use keywords instead of symbols and braces in C language. So this language is very easy for beginners to understand than a programming language like C, C++. Borland is a compiler software company, which is using Delphi programming language for industrial strength. Delphi is an object oriented language of Pascal, and presently  Borland compilers only use it.
 
Pascal Language
Fortran Language
Fortran language is a number crunching language and still it is used by scientists. This language allows different sizes of variables up to the memory limit in the machine. This language is suitable for engineers, who have to calculate values with high precision. Program in Fortran is inflexible and sometimes it makes difficult to read.
 
Fortran Language
Java Language
The Java language is a multi platform language that’s particularly helpful in networking. Of course, mostly this language is used on the web with Java applets. However, this language is used to design cross platform programs, Since it similar to C++ in structure and syntax. For C++ programmers, Java language is very easy to learn and it offers some advantages provided by object oriented  programming. Like reusability and it can be difficult to write efficient code in Java. But, nowadays the speed of the Java language has increased and 1.5 version offers some good features for easy program making.
 
Java Language
Perl Language
Perl language is a file management language for UNIX. But it is more popular for its common gateway interface programming (CGI). It is a term for programs that web servers can perform to allow additional capabilities of web pages. Pearl language is a method for searching text and it is used for useful server functions and other databases, and it is very easy to pick up the fundamentals if you have any experience in any language. As a CGi language, web hosting services select Perl language over C++ language. Because, the web hosts can review Perl script files. Since they are text files, when C++ is compiled.

PHP Language
The PHP language is used to design web pages and sometimes it is also used as scripting language. This language is designed to develop a rapid website, and as a result comprises features which make it easy generate HTTP headers and link to databases. As a scripting language, it includes a set of components permit the programmer to easily get up to speed. However, it has more  sophisticated object oriented features.
 
PHP Language
LISP Language
LISP language is mostly used in computer science research and it stores all data in lists such as arrays. The syntax of list is very simple and easy to make for developers to implement structures.
 
LISP Language
Scheme Language
The scheme language is an alternate of LISP language, and it has a simpler syntax and features. Any project under the scheme language will result in the re implementation of most of the  LISP language. But, this is very popular introductory language in the computer science department of MIT’s. This language easily solves the problems instead of worrying about syntaxes of programming language.
 
Scheme Logo
This is all about the differences between programming languages and few major programming languages are discussed. And, the remaining languages like Tcl, Python, Smalltalk, COBOL, C# and Prolog are similar to the above languages which are discussed. But selecting the suitable language for developing a program or application is very important
-
-
-
-
Method = Function

Both are same, there is no difference its just a different term for the same thing in C#.
Method:
In object-oriented programming, a method is a subroutine (or procedure or function) associated with a class.
With respect to Object Oriented programming the term "Method" is used, not functions.
Detay II: 
When a function is a part of a class, it's called a method.
C# is an OOP language and doesn't have functions that are declared outside of classes, that's why all functions in C# are actually methods.
Though, beside this formal difference, they are the same...
-
-
update-database -TargetMigration:0
..
Note: It's possible to get other errors when migrating data and making schema changes. If
you get migration errors you can't resolve, you can either change the database name in the
connection string or delete the database. The simplest approach is to rename the database
in Web.config file. The following example shows the name changed to CU_Test:
<add name="SchoolContext" connectionString="Data
Source=(LocalDb)\v11.0;Initial Catalog=CU_Test;Integrated
Security=SSPI;"
providerName="System.Data.SqlClient" />
With a new database, there is no data to migrate, and the update-database command is
much more likely to complete without errors. For instructions on how to delete the
database, see How to Drop a Database from Visual Studio 2012.
If that fails, another thing you can try is re-initialize the database by entering the
following command in the PMC:
update-database -TargetMigration:0
Open the database in Server Explorer as you did earlier, and expand the Tables node to see that
all of the tables have been created. (If you still have Server Explorer open from the earlier time,
click the Refresh button.)
..
-
-
-
-
Oracle Database	(GITHUB) (test)
Oracle nedir
Oracle Database (commonly referred to as Oracle RDBMS or simply as Oracle) is a proprietary multi-model database[4] management system produced and marketed by Oracle Corporation.
It is a database commonly used for running online transaction processing (OLTP), data warehousing (DW) and mixed (OLTP & DW) database workloads. The latest generation, Oracle Database 19c, is available on-prem, on-cloud, or in a hybrid-Cloud environment. 19c may also be deployed on Oracle Engineered Systems (e.g. Exadata) on-prem, on Oracle (public) cloud or (private) cloud at customer.[5] At Openworld 2017 in San Francisco, Executive Chairman of the Board and CTO, Larry Ellison announced the next database generation, Oracle Autonomous Database.[6]

Oracle Database, Oracle firması tarafından geliştirilen ve pazarlaması yapılan,zaman zaman Oracle RDBMS (relational database management system), Oracle ilişkisel veritabanı yönetim sistemi (Oracle İVTYS) veya sadece Oracle olarak anılan, gelişmiş bir ilişkisel veritabanı yönetim sistemidir. Tüm ilişkisel veritabanı sistemleri gibi büyük miktarda verinin çok kullanıcılı ortamda depolanmasını ve güvenli bir şekilde erişimini yönetir.

Oracle veritabanı yazılımları özellikle kurumsal alanda kullanılan yaygın bir veritabanı sistemidir. Oracle çok sayıda araçtan oluşur ve uygulama geliştiricilerinin kolay ve esnek uygulamalar geliştirmesini sağlar. Oracle’de diğer veritabanı yönetim sistemlerinde olduğu gibi, stored procedure’lar, paketler, trigger’ler gibi bileşenler yer alır.
-
-
-
-
ORM   (Object Relational Mapping) / (Object-relational Mapper) (GITHUB) (test)
Orm nedir?

Entity Framework'ün tanımı şudur: Entity Framework is an Object Relational Mapper (ORM) which is a type of tool that simplifies mapping between objects in your software to the tables and columns of a relational database. Entity Framework (EF) is an open source ORM framework for ADO.NET which is a part of .NET Framework.
ORM, veritabanında oluşturulan her bir nesneye (tabloya) karşılık uygulama tarafında bir nesne oluşturma işidir. Bu işlem bazı Frameworklerde ara yazılımlar sayesinde (ORM Tools), bazı frameworklerde ise elle gerçekleştirilmektedir.
-
ORM (Object relational Mapping)  (GITHUB)

İlişkisel veritabanı (RDBMS) ile nesneye yönelik programlanın arasında bir tür köprü özelliği gören ve ilişkisel veritabanındaki bilgilerimizi yönetmek için, nesne modellerimizi kullandığımız bir tekniktir”. Basite indirgemek istersek: “Nesnelerimizi ilişkisel veritabanındaki tablomuza bağlayan ve veri alış-verişini bizim için yapan bir tekniktir/metodtur”. ORM tekniği belli bir programlama diline bağlı değildir ve her OO dilinde yazılabilir/kullanılabilir.
-
-
-
Jira 	(GITHUB)  (test)
jira nedir
Jira Software is part of a family of products designed to help teams of all types manage work. 
Originally, Jira was designed as a bug and issue tracker. 
But today, Jira has evolved into a powerful work management tool for all kinds of use cases, from requirements and test case management to agile software development. In this guide, you'll learn which features and functionalities of Jira can help your team with your unique needs. 

JIRA is a tool developed by Australian Company Atlassian. This software is used for bug tracking, issue tracking, and project management. The JIRA full form is actually inherited from the Japanese word "Gojira" which means "Godzilla". The basic use of this tool is to track issue and bugs related to your software and Mobile apps.
It is also used for project management. The JIRA dashboard consists of many useful functions and features which make handling of issues easy. Some of the key features are listed below. Now in these Jira tutorials, let's learn JIRA Defect and Project tracking software with this Training Course.
-
-
-
Ad Hoc Query	(GITHUB)  (test) 
Ad hoc nedir, ad hoc query nedir, ad-hoc query nedir
(ad hoc = amaca özel, niyete mahsus, latince) (Ad hoc is a Latin phrase meaning literally 'to this')
Anlam ad hoc:
adverb
1.	when necessary or needed.
"the group was constituted ad hoc"
adjective
1.	created or done for a particular purpose as necessary.
"the discussions were on an ad hoc basis"
-
-
In SQL, an ad hoc query is a loosely typed command/query whose value depends upon some variable
-
-
Ad hoc is latin for "for this purpose". You might call it an "on the fly" query, or a "just so" query. It's the kind of SQL query you just loosely type out where you need it
var newSqlQuery = "SELECT * FROM table WHERE id = " + myId;

...which is an entirely different query each time that line of code is executed, depending on the value of myId. The opposite of an ad hoc query is a predefined query such as a Stored Procedure, where you have created a single query for the entire generalized purpose of selecting from that table (say), and pass the ID as a variable.
-
-
An Ad-Hoc Query is a query that cannot be determined prior to the moment the query is issued. It is created in order to get information when need arises and it consists of dynamically constructed SQL which is usually constructed by desktop-resident query tools.
Check: http://www.learn.geekinterview.com/data-warehouse/dw-basics/what-is-an-ad-hoc-query.html
-
-
-
In SQL, an ad hoc query is a loosely typed command/query whose value depends upon some variable. Each time the command is executed, the result is different, depending on the value of the variable. It cannot be predetermined and usually comes under dynamic programming SQL query. An ad hoc query is short lived and is created at runtime.
Techopedia Explains Ad Hoc Query
As the word "ad hoc" suggests, this type of query is designed for a "particular purpose,"which is in contrast to a predefined query, which has the same output value on every execution. An ad hoc query does not reside in the system for a long time and is created dynamically on demand by the user. It is more efficient to use an ad hoc query in programming as it saves system resources, but, at the same time complex, ad hoc queries (have multiple variables) also challenge the processing speed and runtime memory of the system.
-
-

Also want to add that ad hoc query is vulnerable to SQL injection attacks. We should try to avoid using it and use parameterized SQLs instead (like PreparedStatement in Java).
Share
Improve this answer
Follow
answered Jun 5 '13 at 16:04

 

xli
2,14422 gold badges1616 silver badges2525 bronze badges
Is this answer outdated?
Yes | No
•	7
An ad-hoc query is not vulnerable to SQL Injection. An unparametized query that accepts user-input is vulnerable to SQL Injection. – Ben Dec 25 '13 at 10:00
•	@Ben An ad-hoc query is always unparameterized. It cannot be SQL injection attacked if we don't expose the variables to users; but the point is ad-hoc query is risky to SQL Injection. – xli Dec 31 '13 at 15:06
-
-
In SQL, an ad hoc query is a loosely typed command/query whose value depends upon some variable. 

Each time the command is executed, the result is different, depending on the value of the variable. It cannot be predetermined and usually comes under dynamic programming SQL query. An ad hoc query is short lived and is created at runtime.
Techopedia explains Ad Hoc Query
As the word "ad hoc" suggests, this type of query is designed for a "particular purpose,"which is in contrast to a predefined query, which has the same output value on every execution. An ad hoc query does not reside in the system for a long time and is created dynamically on demand by the user. It is more efficient to use an ad hoc query in programming as it saves system resources, but, at the same time complex, ad hoc queries (have multiple variables) also challenge the processing speed and runtime memory of the system. 
-
-
https://solutioncenter.apexsql.com/monitoring-sql-server-ad-hoc-query-use-and-abuse/
Monitoring SQL Server ad-hoc query use and abuse
This article will provide an overview of ad-hoc query use in SQL Server, the potential resulting problems as well as how to detect and remediate them.
Background
Despite the many warnings about ad-hoc queries vulnerability to SQL injection attacks and the potential performance degradation compared to use of stored procedures, ad-hoc queries are still used quite a bit. For developers, the ad-hoc query is more flexible and more familiar than compiling the stored procedures since it allows them to create the string using concatenation easily, and execute it against the database. For example:
var adhocQuery = "SELECT * FROM test_table WHERE id = " + UserId ;
Ad-hoc queries are dynamic aka “on the fly” queries that are treated entirely different queries by SQL Server for every execution. The side effect of ad-hoc queries is that SQL Server does nor reuse such statements and instead it adds those to the procedure cache. As a consequence, on a system with significant traffic intensity, this can cause that procedure cache became bloated causing flushing the data from the buffer cache and thus forcing the SQL Server to read more data straight from the physical disk instead of from memory. The result could be a severe performance problem that can slow down SQL Server severely due to a created I/O bottleneck
Now, an ad-hoc query is not something that have to be avoided in all cases, and occasional use can be legitimate, but one should avoid to use them in some iterative cursor or RBAR (Row By Agonizing Row) processes

DECLARE       @tablesCount INT,
    @rowsCount INT,
    @tablename VARCHAR(100),
    @EMP_ID integer,
    @BIRTH_DATE date,
    @WAGE integer,
    @CREDIT_CARD integer,
    @EMP_NAME varchar(10)

SET @tablesCount = 0

WHILE (@tablesCount < 2) -- set the number of tables you want to be created
BEGIN
  SET @tablesCount = @tablesCount + 1 
  SET @tablename = 'Table' + cast(@tablesCount as CHAR)
  
  PRINT 'Created table: ' + @tableName
  
  EXEC('CREATE TABLE ' + @tablename + '(
      EMP_ID integer,
      BIRTH_DATE date,
      WAGE integer,
      CREDIT_CARD integer,
      EMP_NAME varchar(10))')   
    
  SET @rowsCount = 0
  
  WHILE (@rowsCount < 5) -- set the number of rows to be populated in each table
  BEGIN 
    SET @rowsCount = @rowsCount + 1
    
    SET @EMP_ID = @rowsCount
    SET @BIRTH_DATE = DATEADD(d,-((18 * 365) + ABS(CONVERT(int,CONVERT(binary(4),NEWID())))/2147483648. * (47 * 365)),DATEADD(d,0,DATEDIFF(d,0,GETDATE())))
    SET @WAGE = CONVERT(int,50000 + ABS(CONVERT(int,CONVERT(binary(4),NEWID())))/2147483648.*90000)
    SET @CREDIT_CARD = CONVERT(int,5000000 + ABS(CONVERT(int,CONVERT(binary(4),NEWID())))/21474648. * 9000000)
    SET @EMP_NAME = REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(CONVERT(varchar(10), ABS(CONVERT(int,CONVERT(binary(4),NEWID())))) ,'1','a'),'2','b'),'3','c'),'4','d'),'5','e'),'6','f'),'7','g'),'8','h'),'9','i'),'0','j')

    EXEC ('INSERT INTO ' + @tablename + ' VALUES (' + 
      @EMP_ID + ', ' + 
      '''' + @BIRTH_DATE + ''', ' +
      @WAGE + ', ' +
      @CREDIT_CARD + ', ' +
      '''' + @EMP_NAME + ''')')
  END     
END

The query itself creates the desired number of tables in the database and populate each table with the desired number of rows with randomly generated data
The query uses dynamic SQL strings as for each variable in this query it hardcodes the generated value into that string. So, when the string (highlighted parts of the query) is executed using the EXEC command, the SQL Server must create a new query plan for every EXEC iteration because the string and values passed to the EXEC command are always different
So after executing the query (it creates 2 tables and then populates each with 5 rows of data), and as it can be seen it created 10 execution plans
 
This query could be easily set to generate tens or hundreds of thousands of data rows (which will be demonstrated later in the article), and as a consequence ad-hoc queries could flood the execution plan cache with hundreds of thousands single-use copies of the very same query execution plan, where SQL Server has to involve additional work to compile and store each plan. That could severely affect the performance of SQL Server
Analyzing ad-hoc query cost and the impact on SQL Server performance
Generating a significant number of cached execution plans plan that are used only once takes up much space, wastes SQL Server resources and often results in SQL Server contention. Use of Performance Monitor counters could be the way to investigate the source of such issues.
Performance Monitor is a Windows performance monitoring tool suited with a broad range of metrics for monitoring performance of CPU, disk, memory, and network, but it is also equipped with a variety of SQL Server performance metrics.
Performance Monitor is a “live” tool that does not store the performance metric data anywhere, and therefore it is not always easy to track and compare performance data. One way is to create Performance Monitor snapshots at specified intervals and then to compare the results. So unless during the testing or in known situations, it is not a practical and reliable way of performing the analysis of the ad-hoc query issues
Another approach is using DMVs to analyze the impact of ad-hoc query use via analysis of the cached query execution plans and associated queries. All ad-hoc query execution plans are captured and stored in the plan cache, and by rule, the ad-hoc query creates the so-called single-use query execution plan.
One approach to analyze ad-hoc query cost is determining the ratio between the multi-use and single-use query execution plans cached in SQL Server per each database
SELECT Db_Name(QueryText.dbid) AS database_name,
  Sum(CASE WHEN ExecPlans.usecounts = 1 THEN 1 ELSE 0 END) AS Single,
  Sum(CASE WHEN ExecPlans.usecounts > 1 THEN 1 ELSE 0 END) AS Reused,
  Sum(ExecPlans.size_in_bytes) / (1024) AS KB
   FROM sys.dm_exec_cached_plans AS ExecPlans
    CROSS APPLY sys.dm_exec_sql_text(ExecPlans.plan_handle) AS QueryText
   WHERE ExecPlans.cacheobjtype = 'Compiled Plan' AND QueryText.dbid IS NOT NULL
     GROUP BY QueryText.dbid;
 
Another way that can provide some insight into the potential issues related to the ad-hoc query use is to determine the percent of ad-hoc queries that are executed on the target SQL Server.
The following query can be used for that, and it returns the
SELECT Convert(INT,Sum
        (
        CASE a.objtype 
        WHEN 'Adhoc' 
        THEN 1 ELSE 0 END)
        * 1.00/ Count(*) * 100
              ) as 'Ad-hoc query %'
  FROM sys.dm_exec_cached_plans AS a
The example of the query result:
 
A single execution of this query indicates that 82% of queries executed on SQL Server are an ad-hoc query, so depending on the expected value and application design, such result could indicate that there is a need for closer inspection of the application design.
However, for getting some more reliable information regarding the potential ad-hoc query impact on SQL Server, this query should be scheduled and results collected and stored so that they can be analyzed and compared over the time. But none of those allow relevant and measurable analysis of the actual impact that ad-hoc query use imposes on SQL Server performance

-
-
-
-
Default Parameters / Optional Parameters / Required Parameters

static void Main()
        {
            MyMethod(3,3);
            Console.ReadKey();
        }
        static void MyMethod(int x, int y = 2)
        {
            Console.WriteLine(x);
            Console.WriteLine(y);       
        }
Yukarıdaki MyMethod metodumuzda iki tane parametre var: x ve y.
Bunlardan:
 int x ==> required parameter
int y = 2  ==> optional parameter (default parameter)
ikinci parametreye opsiyonel deniyor; çünkü bu parametreye değer vermesek de olur.
Not: Metod tanımlanırken, optional parametreler, required parametrelerden sonra yazılmalıdır. 
Yani şu durumda:
static void MyMethod(int x = 2, int y)
gibi bir şey yazarsak hata verecektir.
-
-
Bi örnek daha:
using System;

class Program
{
    static void Main()
    {
        MyMethod(1, 2);
        MyMethod(5);
        Console.ReadKey();
    }
    static void MyMethod(int x, int y = 0)
    {
        Console.WriteLine(x + ", " + y);
        //1, 2
        //5,0   (x'i 5 verip y yi boş bıraktık. y'nin default value'su da 0 idi. O yüzden 0 döndü)
    }

}
-
-
-
-
-
using System;

namespace ConsoleAppForExercises
{
    class Program
    {
        static void Main()
        {
            MyMethod(6, 7);
            MyMethod(1);
            Console.ReadKey();
        }
        static void MyMethod(int x, int y = 2)
        {
            Console.WriteLine(x);
            Console.WriteLine(y);
            Console.WriteLine("---");
        }

        /*
           SONUÇ:
           6
           7
           ---
           1
           2
           ---
         */

    }
}
-
-
-
-
Uzun anlatım:
Merhaba Arkadaşlar,
2004 ve  2005 yıllarında uzun bir süre editörlüğünü yaptığım C#Nedir? topluluğunun düzenlediği C# Akademi eğitimlerinde, yarı zamanlı eğitmen olarak görev yapmıştım. Genellikle C# programlama dilinin basit ve temel konularını, ayrıca Object Oriented özelliklerini aktarmaya çalışırdım. Elbette sınıfımdaki öğrencilerim yanda görüldüğü gibi her zaman pür neşe olmazlardı.
Ancak insan zaman içerisinde profesyonelleşme yolunda ilerledikçe konuları çok daha farklı açılardan ele alması gerektiğini de öğreniyor. Profesyonel bir eğitmenin en iyi yaptığı işlerin başında, en zor konuları çöp adam kullanarak anlatmak gelmektedir. Tabi eğitmenin gerçek hayat tecrübelerini ve ip uçlarını da aktarıyor olması, profesyonelliğinin diğer bir göstergesidir. Böyle bir eğitmenin vereceği önerileri pür dikkat dinlemekte yarar vardır.
Ben eğitmenliği bırakalı uzun bir süre oldu ama makale yazarken veya görsel ders çekerken, konunun anlatımı sırasında yukarıdaki hususlara dikkat etmeye çalışıyorum. Bu anlamda bazen çok basit olarak görünen bir konunun, aslında derinlere inildiğinde dikkat edilmesi gereken noktalar içerdiğini sürekli vurgulamaya çalışan yazıları da hazırlama uğraşısı içerisindeyim. İşte bu yazımızın konusu da; C# 4.0 ile birlikte gelen yeni dil özelliklerden birisi olan Default Parameters ile ilişkili tuzaklar. Öncelikli olarak konuya aşağıdaki hazır kod parçası ile başlayalım.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52	using System;
 
namespace DefaultAndOptionalParametersCase 
{ 
    class Program 
    { 
        static void Main(string[] args) 
        { 
            Connection myConn = new Connection(); 
            Console.WriteLine(myConn.ToString()); 
            myConn = new Connection("localhost", 
"AdventureWorks"); 
            Console.WriteLine(myConn.ToString()); 
        } 
    }
 
    class Connection 
    { 
        public string Server { get; set; } 
        public string Database { get; set; } 
        public int Timeout { get; set; } 
        public int PacketSize { get; set; }
 
        #region Constructors
 
        public Connection(string server,string databaseName,
int timeout,int packetSize) 
        { 
            Server = server; 
            Database = databaseName; 
            Timeout = timeout; 
            PacketSize = packetSize; 
        } 
        public Connection(string server, string databaseName, 
int timeout) 
            : this(server, databaseName, timeout, 4096) 
        { 
        } 
        public Connection(string server, string databaseName) 
            : this(server, databaseName, 45, 4096) 
        { 
        } 
        public Connection() 
            : this(".", "master", 45, 4096) 
        { 
        }
 
        #endregion
 
        public override string ToString() 
        { 
            return String.Format("server={0};
database={1};timeout={2},
packetSize={3}", Server, Database, Timeout, PacketSize); 
        } 
    } 
}
 
Bu kod parçasında dikkat etmemiz gereken nokta Constructor metodlarıdır. Görüldüğü üzere en fazla sayıda parametre alan yapıcı metod, diğer yapıcı metodlar tarafından kullanılmaktadır. Burada this anahtar kelimesini takip eden ifadeler içerisinde gerekli aktarma işlemlerinin yapıldığı görülebilir.

Eski bilgilerimizi bir hatırlayalım. Bilindiği üzere yapıcı metodlarda(Constructors) this yerine base anahtar kelimesini kullanarak, metod parametrelerinin bir üst sınıftaki versiyonuna gönderilmesi de sağlanabilir.
Tabi burada C# 4.0 ile gelen Default Parameters yeteneğinin devreye girmesi ile n sayıda metod yerine tek bir metodun kullanılması söz konusu olabilir. Nitekim ele aldığımız örnek senaryoda yapıcı metodların tek yaptığı, uygun olan versiyona parametre değerlerini taşımaktır. Dikkat edileceği üzere sadece tek bir yapıcı metod içerisinde özellik değer atama işlemleri yapılmaktadır. Diğer yapıcı metodlar sadece parametre değerlerini taşımak için kullanılmaktadır. Aşağıdaki şekilde bu durum ifade edilmeye çalışılmaktadır.
 
Aslında Constructor kullanımının buradaki amacı, Connection tipine ait nesne örneklerinin oluşturulması sırasında alternatif versiyonları varsayılan parametre değerlerine göre sunabilmektir. Bu amaç düşünüldüğünde Default Parameters yeteneği önemli bir avantaj sağlamaktadır. Gelin kodumuzu Default Parameters kabiliyetini kullanarak aşağıdaki hale getirelim.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38	using System;
 
namespace DefaultAndOptionalParametersCase 
{ 
    class Program 
    { 
        static void Main(string[] args) 
        { 
            Connection myConn = new Connection(); 
            Console.WriteLine(myConn.ToString()); 
            myConn = new Connection("localhost", "AdventureWorks"); 
            Console.WriteLine(myConn.ToString()); 
            myConn = new Connection("localhost", "AdventureWorks",20,512); 
            Console.WriteLine(myConn.ToString()); 
        } 
    }
 
    class Connection 
    { 
        public string Server { get; set; } 
        public string Database { get; set; } 
        public int Timeout { get; set; } 
        public int PacketSize { get; set; }
 
        public Connection(string server=".", string databaseName="master", int timeout=45, int packetSize=4096) 
        { 
            Server = server; 
            Database = databaseName; 
            Timeout = timeout; 
            PacketSize = packetSize; 
        }
 
        public override string ToString() 
        { 
            return String.Format("server={0};database={1};timeout={2},packetSize={3}", Server, Database, Timeout, PacketSize); 
        } 
    }    
}
Dikkat edileceği üzere tek bir yapıcı metod kullanımı söz konusudur. Bir başka deyişle kod kısalmıştır. Yapıcı metodun parametrelerinde verilen varsayılan değerler sayesinde, Connection tipine ait nesne örneklerinin oluşturulması şekillendirilmiştir. Örneğin, çalışma zamanı çıktısı aşağıdaki gibi olacaktır.
 
Aslında işin içerisine Named Parameters kullanımını da katmamız yerinde olacaktır. Neden? Main metodu içerisindeki aşağıdaki kod satırını göz önüne alalım.
1	myConn = new Connection("localhost", "AdventureWorks",20,512);
Geliştirici kodu yazarken parametrelerin ne anlama geldiğini, isimlerinden veya varsa eğer XML Comment’ lerden çıkartabilir. Ancak tamamlanmış kodun okunması sırasında 20 ve 512 rakamlarının en anlama geldiği kolayca anlaşılamayabilir. İşte bu noktada parametreleri isimlendirerek kullanmak aşağıdaki okunurluğu sağlayacaktır.
1	myConn = new Connection(server:"localhost", databaseName:"AdventureWorks", timeout:20, packetSize:512);
Parametre Sayısının Arttırılması
Gelelim default parameters kullanımında dikkatli olmamız gereken hususlara. İlk olarak parametre sayısının arttırılması durumunu göz önüne alacağız. Ancak senaryonun oluşumunda Named Parameters kullanmadığımızı varsayıyoruz. Bu amaçla Connection tipine ait yapıcı metodu aşağıdaki gibi değiştirdiğimizi düşünelim.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40	using System;
 
namespace DefaultAndOptionalParametersCase 
{ 
    class Program 
    { 
        static void Main(string[] args) 
        { 
            Connection myConn = new Connection(); 
            Console.WriteLine(myConn.ToString()); 
            myConn = new Connection("localhost", "AdventureWorks"); 
            Console.WriteLine(myConn.ToString()); 
            myConn = new Connection("localhost", "AdventureWorks",20,512);            
            Console.WriteLine(myConn.ToString()); 
        } 
    }
 
    class Connection 
    { 
        public string Server { get; set; } 
        public string Database { get; set; } 
        public int Timeout { get; set; } 
        public int PacketSize { get; set; } 
        public int ProcessId { get; set; }
 
        public Connection(string server=".", string databaseName="master", int timeout=45,int processId=10, int packetSize=4096) 
        { 
            Server = server; 
            Database = databaseName; 
            Timeout = timeout; 
            PacketSize = packetSize; 
            ProcessId = processId; 
        }
 
        public override string ToString() 
        { 
            return String.Format("server={0};database={1};timeout={2},packetSize={3},PId:{4}", Server, Database, Timeout, PacketSize,ProcessId); 
        } 
    }    
}
Kodda sadece processId isimli bir metod parametresi eklendiğini görmekteyiz. Bu aslında sonradan yapılan bir değişiklik olarak düşünülmelidir. Bir başka deyişle geliştirdiğimiz projelerde sonradan varsayılan parametre eklenmesi söz konusu olabilir. Buna göre çalışma zamanı çıktısı aşağıdaki gibi olacaktır.
 
Dikkatinizi çeken bir nokta var mı?
Son çıktıya göre ProcessId değerinin 512 olduğu görülmektedir. Oysaki 512 değeri daha önceki kodlamaya göre PacketSize özelliği için atanan bir değerdir. Bir başka deyişle yanlış bir değer ataması söz konusudur. İşin kötü yanı bu senaryoda derleme zamanında bir hata veya uyarı mesajı alınmamaktadır. Dolayısıyla kodun hatalı çalışması olasıdır.
Öyleyse varsayılan parametre kullanımı gibi senaryolarda, metodlara yeni parametrelerin eklenmesi söz konusu ise, bu parametrelerin en sona eklenmesi daha doğru olacaktır. Named Parameters aslında köklü çözüm olsa da, ilgili tip metodlarını kullanan diğer geliştiricilerin bu kullanımı göz ardı etmesi ihtimali vardır.
Yani metod yapısını aşağıdaki gibi değiştirmemiz doğru bir çalışma zamanı çıktısı elde etmemizi sağlayacaktır.
1	public Connection(string server = ".", string databaseName = "master", int timeout = 45, int packetSize = 4096,int processId = 10)
,sonucu çalışma zamanı çıktısı aşağıdaki gibidir.
 
Türetme(Inheritance) ve Varsayılan Parametreler
Gelelim diğer bir vakaya. Bu vaka çok daha kritik ve önemlidir. Nitekim işin içerisinde türetme(Inheritance) kavramı vardır. Konuyu netleştirmek için aşağıdaki sınıf şemasına sahip örnek kod parçasını göz önüne alarak ilerleyelim.
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43	using System;
 
namespace DefaultAndOptionalParametersCase 
{ 
    class Program 
    { 
        static void Main(string[] args) 
        { 
            MyCommand myCmd = new MyCommand(); 
            ICommand iCmd = myCmd; 
            Command cmd = myCmd;
 
            Console.WriteLine(myCmd.PrepareSelectTop("Product")); 
            Console.WriteLine(iCmd.PrepareSelectTop("Product")); 
            Console.WriteLine(cmd.PrepareSelectTop("Product")); 
        } 
    }
 
    interface ICommand 
    { 
        string PrepareSelectTop(string tableName, int topNumber = 3); 
    } 
    class Command 
       : ICommand 
    { 
        #region ICommand Members
 
        public virtual string PrepareSelectTop(string tableName, int topNumber = 10) 
        { 
            return String.Format("Select top {0} from {1}",topNumber,tableName);            
        }
 
        #endregion 
    } 
    class MyCommand 
       : Command 
    { 
        public override string PrepareSelectTop(string tableName, int topNumber = 50) 
        { 
            return String.Format("Select top {0} from {1}",topNumber,tableName); 
        } 
    }   
}
Aslında bu senaryo Temeller Kolay Unutulur (C# – Implicitly Name Hiding Sorunsalı) başlıklı yazımızdan size tanıdık gelecektir.
Sınıf şemasından da görüleceği üzere ICommand arayüzünü(Interface) uygulayan Command isimli bir tip ve bundan türeyen MyCommand sınıfı söz konusudur. MyCommand sınıfı, Command tipinde virtual olarak tanımlanmış ve aslında ICommand arayüzü tarafından zorunlu hale getirilmiş PrepareSelectTop metodunu ezmektedir(Overriding).
Kritik olan yer Main metodu içerisindeki değişken atamalardır. Dikkat edileceği üzere ICommand ve Command tipinden olan değişkenlere aynı MyCommand nesne örneği atanmıştır. Eğer çok biçimlilik ilkesini biliyorsak, iCmd ve cmd isimli nesne örnekleri üzerinden yapılan PrepareSelectTop çağrılarının aslında MyCommand tipindeki metod içeriğine doğru yapılması gerektiğini biliriz. Buna göre de tüm Select sorgularında Top 50 değerinin kullanılıyor olması gerekmektedir. Oysaki çalışma zamanı çıktısı aşağıdaki gibi olacaktır.
 
Görüldüğü gibi son iki çağrıda topNumber için Default Parameter değerleri tanımlandıkları yerdekiler olmuştur. ICommand için 3 iken Command için 10 olarak ele alınmıştır. Tam bu noktada “Amanın! Yoksa ICommand ve Command tipleri çok biçimlilik göstermiyorlarmış!” diye haykırabilirsiniz. Ama dereyi görmeden paçaları sıvamamak lazım. Nitekim uygulamayı debug modda değerlendirdiğimizde, aslında tüm PrepareSelectTop çağrılarının, MyCommand içinden yapıldığı görülecektir.
Sorun tamamen Default Parameter’ lar ile alakalıdır. Söz gelimi ICommand üzerinden yapılan çağrı sonucu topNumber değeri aşağıdaki gibi olacaktır.
 
veya Command tipi için şu şekilde olacaktır.
 
Böyle bir vakanın oluşmasının sebebi Defaul Parameter’ ların çalışma zamanı(Runtime) yerine derleme zamanında(Compile Time) çözümleniyor olmalarıdır. Bu durum IL(Intermediate Language) kodunda açık bir şekilde görülebilir ve ispatlanabilir.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38	.method private hidebysig static void  Main(string[] args) cil managed 
{ 
  .entrypoint 
  // Code size       68 (0x44) 
  .maxstack  3 
  .locals init ([0] class DefaultAndOptionalParametersCase.MyCommand myCmd, 
           [1] class DefaultAndOptionalParametersCase.ICommand iCmd, 
           [2] class DefaultAndOptionalParametersCase.Command cmd) 
  IL_0000:  nop 
  IL_0001:  newobj     instance void DefaultAndOptionalParametersCase.MyCommand::.ctor() 
  IL_0006:  stloc.0 
  IL_0007:  ldloc.0 
  IL_0008:  stloc.1 
  IL_0009:  ldloc.0 
  IL_000a:  stloc.2 
  IL_000b:  ldloc.0 
  IL_000c:  ldstr      "Product" 
  IL_0011:  ldc.i4.s   50 
  IL_0013:  callvirt   instance string DefaultAndOptionalParametersCase.Command::PrepareSelectTop(string, 
                                                                                                  int32) 
  IL_0018:  call       void [mscorlib]System.Console::WriteLine(string) 
  IL_001d:  nop 
  IL_001e:  ldloc.1 
  IL_001f:  ldstr      "Product" 
  IL_0024:  ldc.i4.3 
  IL_0025:  callvirt   instance string DefaultAndOptionalParametersCase.ICommand::PrepareSelectTop(string, 
                                                                                                   int32) 
  IL_002a:  call       void [mscorlib]System.Console::WriteLine(string) 
  IL_002f:  nop 
  IL_0030:  ldloc.2 
  IL_0031:  ldstr      "Product" 
  IL_0036:  ldc.i4.s   10 
  IL_0038:  callvirt   instance string DefaultAndOptionalParametersCase.Command::PrepareSelectTop(string, 
                                                                                                  int32) 
  IL_003d:  call       void [mscorlib]System.Console::WriteLine(string) 
  IL_0042:  nop 
  IL_0043:  ret 
} // end of method Program::Main
IL kodunda yer alan ldc komutlaraına bakıldığında Defualt Parameter değerlerinin, tip tanımlamaları sırasında yazıldığı gibi set edildiği açık bir şekilde görülebilmektedir.
Kolayca gözden kaçabilecek bir durum olduğu için tehlikeli bir vaka olduğunu ifade edebiliriz. Dolayısıyla en azından bu senaryoya göre Default Parameter kullanımını aslında Interface seviyesinde bırakmak çözüm olarak düşünülebilir.
Böylece geldik bir yazımızın daha sonuna. Tekrardan görüşünceye dek hepinize mutlu günler dilerim.
-
-
-
One of the major annoyances when starting with C# was the lack of default arguments. Generating two or three overloads manually is ok, but looking at the 28 overloads for a MessageBox makes my head tingle (in an ugly way).
Now I can't hack the C# compiler to magically allow this. I can, however, provide a code generator that creates a set of overloads from a declaration containing default arguments.
Using the Tool
Enter your declaration in the first edit box. You can add default arguments like you would in C++, and the generated code appears immediately in the text field below. Click "Copy" to copy the generated code to the clipboard. Example:
Hide   Copy Code
void Add(string name = "(none)", int age = 22)
The above generates the following implementations:
Hide   Copy Code
// $overload: void Add(string name = "(none)", int age = 22){
 // do your worst! - implement the actual method here
}

#region overloads for: Add

public void Add(string name)
{
 Add(name, 22);
}

public void Add()
{
 Add("(none)", 22);
}

#endregion // overloads
public void Add(string name, int age)
The first overload is the method you actually implement. The other overloads have a reduced argument list and call the first.
The first line is for integration with Linkify (see below).
It Gets Better (Optionally)
C++ default arguments allow to omit parameters at the end of the argument list. However, in the above example where argument types are distinct, you may want to omit the name but still specify the age. I've added a custom "keyword", optional, that can drop arguments from the middle of the argument list:
Hide   Copy Code
void Add(optional string name = "(none)", int age = 22)
This generates the following signatures:
Hide   Copy Code
public void Add(string name, int age) { } // the one to implement
public void Add(string name)
public void Add()
public void Add(int age)
The last one omits the string name parameter.
It is your responsibility to make sure the resulting overloads aren't ambiguous. If you don't feel up to the job, the compiler will catch you!
Integration with Visual Studio - Linkify
I've updated my Linkify article that allows to visit URLs and run external tools from source code comments. Also, DefaultOverloader now generates a declaration line that Linkify can understand:
Hide   Copy Code
// $overload: void Add(string name = "(none)", int age = 22)
void Add(string name, int age)
Note: You need to set the path to the DefaultOverloader executable first in the Linkify configuration dialog. See that article for details.
Place the caret on the "$overload" and click "Linkify". Default Overloader opens with your declaration already in place.
You can also copy the two declaration lines separately by clicking "copy head", and the overloads region by clicking "copy body". This allows to preserve the body of the main functions implementation. Auto-Paste makes that even easier.
Auto-Paste
Auto-Paste makes that even easier. It works without Linkify installed (but then, the first step is more complicated). To modify an existing block with overloads:
1.	Place caret on "$overload", and start Linkify
» DefaultOverloader will open, with your declaration ready to be edited.
2.	After making your modifications, click "Auto-Paste"
» DefaultOverloader minimizes, and displays "Paste Head" as title
3.	Select the two head lines (comment and main function header), and paste
» The declarations are replaced with the new contents
» DefaultOverloader (still minimized) displays "Paste Body" as title
4.	Collapse the #region Overloads for..., select the entire line, and paste
» New overloads replace the previous ones
» DefaultOverloader closes
Restore the DefaultOverloader from minimized state to cancel the paste sequence. There is a small delay between the first paste and the second data arriving on clipboard, so don't paste too fast in succession!
Limitations
The parser is fairly simple. So you might be able to create declarations that aren't interpreted correctly. One thing is a function name with multiple template parameters: don't put spaces in the template list, otherwise, I get the function name wrong!
The parser doesn't validate your declaration - that's left to your compiler.
This is unfortunately no full replacement for default parameters. One thing where some language and IDE support would be helpful is Intellisense - you still get listed all overloads separately. Also, when you modify the declaration, you have to use the Overloader again.
The Debate
There is a reason there are no default parameters in MSIL or the C# language - to learn about the reasons, read Eric Gunnersons response [^] or start at Google [^].
In my opinion, Eric has a valid argument why default arguments aren't in MSIL - but it doesn't hold up for the C# language itself. After all, C# could do what I do.:)
Points of Interest
Permutation of Bools
For implementing the optional keyword, I needed to permute over all variations of having them in. With N parameters tagged optional, I need to generate all possible combinations of N bool values. While some approaches can be found on the 'net, none was plug-and-play-enough for me. Thinking how to implement it myself, a "background thread" associated the idea of a bool array with a bit array - bools are bits! This allows a very simple solution: just count from 0 to 2<sup>N</sup>-1, and treat each bit as a flag.
Parsing the Argument List
The sources contain a (still somewhat experimental) Tokenizer that respects single- and double-quoted strings as well as round, square and curly brackets. This is required to parse declarations like the following correctly:
Hide   Copy Code
SendMessage(string X = "Hello, World", int[,] targetCoords) 
It's barely tested and needs some extensions to be reusable, but it works well enough for the application given.
Thank You
A big thank you to everyone who helped me at the C# forums here - especially Judah Himango who often went an extra mile.
I am currently working on my first "real world" C# project (complete with time pressure, feature creep, big plans, and get-it-done-while-everybody-is-out-to-distract you), and you have been really helpful in getting my stubborn head through this wall - or at least into it.
-
-
-
What’s the Difference Between a URI and a URL?

The terms “URI” and “URL” are often used interchangeably, but they are not exactly the same.
1.	A URI is an identifier of a specific resource. Like a page, or book, or a document.
2.	A URL is special type of identifier that also tells you how to access it, such as HTTPs, FTP, etc.—like https://www.google.com.
3.	If the protocol (https, ftp, etc.) is either present or implied for a domain, you should call it a URL—even though it’s also a URI.
All URLs are URIs, but not all URIs are URLs.
 
DOMAINS AREN’T EITHER URIS OR URLS BECAUSE ALL URLS ARE ALSO URIS
When most people talk about a given URI, they’re also talking about a URL because the protocol is implied.
That’s really it.
TL;DR: When communicating, being more specific is usually better, so “URL” is better than “URI” when talking about web addresses.
That’s all you probably need to know, but if you want to see how the sausage is made (I warn you, it’s gross), feel free to read on!
________________________________________
A deeper explanation (let’s get technical)
This is one of the most common Nerd Fight debates in tech history, and that’s saying a lot.
One obstacle to getting to the bottom of things is that the relevant RFCs are extremely dense, confusing, and even contradictory. For example, RFC 3986 says a URI can be a name, locator, or both…
My emphasis.
A URI can be further classified as a locator, a name, or both. The term “Uniform Resource Locator” (URL) refers to the subset of URIs that, in addition to identifying a resource, provide a means of locating the resource by describing its primary access mechanism (e.g., its network “location”).
-
-
-
rfc 3986, section 1.1.3
Related
multi-dimensional vulnerability hierarchies
But just a little further down that same RFC says…
My emphasis.
The URI itself only provides identification; access to the resource is neither guaranteed nor implied by the presence of a URI.

rfc 3986, section 1.2.2
And then, if you’re not yet completely confused, it also says…
My emphasis.
Each URI begins with a scheme name, as defined in Section 3.1, that refers to a specification for assigning identifiers within that scheme.

rfc 3986, section 1.1.1
And it goes on to give examples:
Notice how they all their examples have schemes.
ftp://ftp.is.co.za/rfc/rfc1808.txt
http://www.ietf.org/rfc/rfc2396.txt
ldap://[2001:db8::7]/c=GB?objectClass?one
mailto:John.Doe@example.com
news:comp.infosystems.www.servers.unix
tel:+1-816-555-1212
telnet://192.0.2.16:80/
urn:oasis:names:specification:docbook:dtd:xml:4.1.2
Related
an ngrok tutorial and primer
Wait…what?
These three contradictions are the source of this entire long-lived debate.
The same RFC just told us that a URI can be a name, a locator, or both—but a URI only provides identification, and a way to access isn’t guaranteed or implied—oh and also each URI begins with a scheme name (which in many cases tells you exactly how to access the resource).
It’s no wonder everyone is confused!
The reason the internet’s been fighting about this for over a decade is that the RFC is poorly written.
salvaging practical rules from all this
Being the top search result for this topic means I have the conversation a lot.
Ok, so given the fact that the RFC adds to confusion rather than eliminating it, what—if anything—can we use from them?
In the vein of language being here for communication rather than pedantry, here are my own practical interpretations of the RFCs that will hopefully synchronize people and result in fewer swordfights.
Related
the internet, the deep web, and the dark web
All butterflies fly, but not everything that flies is a butterfly.
1.	A Uniform Resource Identifier (URI) provides a simple and extensible means for identifying a resource (straight from RFC 3986). It’s just an identifier; don’t overthink it.
2.	For most debates about this that matter, URI is the superset, so the question is just whether a given URI is formally a URL or not. All URLs are URIs, but not all URIs are URLs. In general, if you see http(s)://, it’s a URL.
3.	URIs technically do require a scheme (see above), but the RFC also says they can be a name, locator, or both, so YOLO! My advice for anyone saying URIs do or do not require a scheme is to show them this article, because it’s the only thing I know of that highlights the contradictions in the RFC.
4.	Fragments like file.htm actually are not URNs, because URNs are required to use a special notation with urn: in the beginning.
5.	A little-known section of RFC 3986 actually speaks directly to the religious part of the argument, and seems to say we should say URI instead of URL.
RFC 3986 is from 2005, so presumably they’re saying URI is the preferred term after that point.
Future specifications and related documentation should use the general term “URI” rather than the more restrictive terms “URL” and “URN”

rfc 3986, section 1.1.3
So that’s support for the “URI” denomination, but in my opinion it’s even more support for those who say, “stop looking for the answers in 15-year-old RFCs”.
It’s like another widely-read text in this way.
There’s just so much contradictory content that there’s partial backing for multiple conclusions.
summary
What a mess. Here’s the tl;dr…
1.	The RFCs are ancient, poorly written, and not worth debating until they’re updated.
2.	A URI is an identifier.
3.	A URL is an identifier that tells you how to get to it.
4.	Use the term that is best understood by the recipient.
I’d welcome a new version of the RFC that simplifies and clarifies the distinction, with modern examples.
These RFCs were written a very long time ago, and they’re written with the academic weakness of not being optimized for reading.
The best thing I can possibly tell you about this debate is not to over-index on it. I’ve not once in 20 years seen a situation where the confusion between URI or URL actually mattered.
The irony is that RFCs are supposed to remove confusion, not add to it.
So while there is some direct support that “URI” is preferred by the RFCs, and “URL” seems most accurate for full addresses with http(s) schemes (because it’s most specific), I’ve chosen to prioritize the Principle of Communication Clarity higher than that of pedantic nuance.
It’s taken me a long time to get to this point.
As a result, I personally use “URL” in most cases because it’s least likely to cause confusion, but if I hear someone use “URI” I’ll often switch immediately to using that instead.
-
-
-

Array class
C# provides numerous built-in classes to store and manipulate data.
One example of such a class is the Array class.
An array is a data structure that is used to store a collection of data. You can think of it as a collection of variables of the same type.
For example, consider a situation where you need to store 100 numbers. Rather than declare 100 different variables, you can just declare an array that stores 100 elements.
To declare an array, specify its element types with square brackets:
int[] myArray;
Since arrays are objects, we need to instantiate them with the new keyword:
int[] myArray = new int[5];
After creating the array, you can assign values to individual elements by using the index number:int[ ] 

            int[] myArray;
            myArray = new int[5];
            myArray[0] = 23;

This will assign the value 23 to the first element of the array.
Arrays in C# are zero-indexed meaning the first member has index 0, the second has index 1, and so on.
We can provide initial values to the array when it is declared by using curly brackets:

            string[] names = new string[3] { "John", "Mary", "Jessica" };
            double[] prices = new double[4] { 3.6, 9.8, 6.4, 5.9 };

We can omit the size declaration when the number of el It's occasionally necessary to iterate through the elements of an arrayements are provided in the curly braces:

            string[] names = new string[] { "John", "Mary", "Jessica" };
            double[] prices = new double[] { 3.6, 9.8, 6.4, 5.9 };

We can even omit the new operator. The following statements are identical to the ones above:

            string[] names = { "John", "Mary", "Jessica" };
            double[] prices = { 3.6, 9.8, 6.4, 5.9 };

-
Array sample:

        static void Main(string[] args)
        {
            int[] a = new int[11];
            for (int k = 0; k <= 10; k++)
            {
                a[k] = k * 2;
            }
            foreach (int k in a)
            {
                Console.WriteLine(k);
            }
            Console.ReadKey();
        }
The foreach loop iterates through the array a and assigns the value of the current element to the variable k at each iteration of the loop. So, at the first iteration, k=a[0], at the second, k=a[1], etc.
The data type of the variable in the foreach loop should match the type of the arrayelements.
-
-

-
-
ilk değeri sıfır olan listeler şunlar bunlar vb.

örneğe bkz:

This will assign the value 23 to the first element of the array.
Arrays in C# are zero-indexed meaning the first member has index 0, the second has index 1, and so on.
-
-
Zero-based numbering  
Connected to:
 NumberingDerivativeComputer science
From Wikipedia, the free encyclopedia	
Zero-based numbering is a way of numbering in which the initial element of a sequence is assigned the index 0, rather than the index 1 as is typical in everyday non-mathematical or non-programming circumstances. 

Under zero-based numbering, the initial element is sometimes termed the zeroth element,[1] rather than the first element; zeroth is a coined ordinal number corresponding to the number zero. In some cases, an object or value that does not (originally) belong to a given sequence, but which could be naturally placed before its initial element, may be termed the zeroth element. There is not wide agreement regarding the correctness of using zero as an ordinal (nor regarding the use of the term zeroth) as it creates ambiguity for all subsequent elements of the sequence when lacking context.
Numbering sequences starting at 0 is quite common in mathematics notation, in particular in combinatorics, though programming languages for mathematics usually index from 1.[2][3][4] In computer science, array indices usually start at 0 in modern programming languages, so computer programmers might use zeroth in situations where others might use first, and so forth. In some mathematical contexts, zero-based numbering can be used without confusion, when ordinal forms have well established meaning with an obvious candidate to come before first; for instance a zeroth derivative of a function is the function itself, obtained by differentiating zero times. Such usage corresponds to naming an element not properly belonging to the sequence but preceding it: the zeroth derivative is not really a derivative at all. However, just as the first derivative precedes the second derivative, so also does the zeroth derivative (or the original function itself) precede the first derivative.
Computer programming
Origin
Martin Richards, creator of the BCPL language (a precursor of C), designed arrays initiating at 0 as the natural position to start accessing the array contents in the language, since the value of a pointer p used as an address accesses the position p + 0 in memory.[5][6] Canadian systems analyst Mike Hoye asked Richards the reasons for choosing that convention. BCPL was first compiled for the IBM 7094; the language introduced no run time indirection lookups, so the indirection optimization provided by these arrays was done at compile time.[6] The optimization was nevertheless important.[6][7]
Edsger W. Dijkstra later wrote a pertinent note Why numbering should start at zero[8] in 1982, analyzing the possible designs of array indices by enclosing them in a chained inequality, combining sharp and standard inequalities to four possibilities, demonstrating that to his conviction zero-based arrays are best represented by non-overlapping index ranges, which start at zero, alluding to open, half-open and closed intervals as with the real numbers. Dijkstra's criteria for preferring this convention are in detail that it represents empty sequences in a more natural way (a ≤ i < a ?) than closed "intervals" (a ≤ i ≤ (a−1) ?), and that with half-open "intervals" of naturals, the length of a sub-sequence equals the upper minus the lower bound (a ≤ i < b gives (b−a) possible values for i, with a, b, i all integers).
-
-
-
Intranet    	(GITHUB)  (test)
Intranet nedir, intranet nedir
 
İntranet, sadece belirli bir kuruluş içindeki bilgisayarları, yerel ağları (LAN) ve geniş alan ağlarını (WAN) birbirine bağlayan, çoğunlukla TCP/IP tabanlı bir ağdır. İntranet'ler Ağ geçitleri (İng: gateways) ile diğer ağlara bağlanabilir. Temel oluşturulma amaçları, kuruluş bünyesinde bilgileri ve bilgi işlem kapasitesini paylaşmaktır.
İntranet'ler, şirket(ler) içi tele-konferans uygulamalarında ve farklı birimlerdeki kişilerin bir araya gelebildiği iş gruplarının oluşturulmasında da kullanılırlar. İntranet'ler üzerinden HTTP, FTP gibi pek çok protokol uygulamaları çalıştırılabilir. Günümüzde, İntranet'ler içinde, Web erişimi ile kaynakların kullanımı oldukça yaygındır.
Bazı şirketlerdeki intranet'lerden, ateş duvarı (İng: Firewall) sistemleri üzerinden (bazı emniyet tedbirleri ile), İnternet çıkışı da yapılmaktadır. Bu sayede, her iki yönde de ileti trafiği kontrol edilebilmekte ve güvenlik sağlanmaktadır.
İntranet üzerinde; muhasebe, insan kaynakları, üretim otomasyon yazılımları çalıştırmak mümkün olduğu gibi çeşitli veri tabanlarını tutmak ve belge dağıtımı gibi işleri gerçekleştirmek mümkündür. Özünde İnternet teknolojisinin şirket içinde kullanılmasıdır.
İntranet dağıtık bilişim stratejilerini destekler. İntranetin üzerinde kuruluşun bütün faaliyetleriyle ilgili modüller çalıştırılıyorsa ve uygun bir modelleme yapılmışsa kuruluş içinde her şeyin bütünleşik çalıştığı, sistemde kendini denetleme mekanizmaları bulunur. Küresel erişim, multimedya olanakları ve düşük maliyet sağlanmış ve bütün bunların bir araya gelmesi İntranet'i güçlü kılmıştır. Türkiyed'de ki en büyük intranet ağı "Polnet" ile Emniyet Genel Müdürlüğü'ne aittir.
Kullanım alanları
Belge Dağıtımı, insan kaynakları, eğitim ve oryantasyon, çalışma grupları, ortak iş programlarının kullanılması, üretim, yeni belgeleme sistemlerinin oluşturulması, tedarikçiler ile ilişkiler, sanal alış-veriş, servis ve destek vs.vs.
-
Race Condition	(GITHUB) (testset)
Race condition nedir
Kısaca: 
A race condition or race hazard is the condition of an electronics, software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when one or more of the possible behaviors is undesirable.

-
It is also called race hazard. It is a behavior of software or system where output is based on the sequence or timing of other uncontrolled events. It becomes painful or bug when events do not happen in the order in which functionality is required.

Race condition occurs when two threads access a shared variable at the same time. The first thread reads the variable, and the second thread writes to the same variable at the same time.
 Symptoms of Race Condition
The most common symptom of a race condition is unexpected values of variable that are shared between multiple threads. In this case, sometimes one thread wins, and sometimes the other thread wins. At the other times, the result of execution may be correct. Also, if each thread executes separately, the result of variable comes expected.








