




WWW     	(GITHUB)  (empty) 
[www nedir, world wide web nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
World Wide Web, Dünya Çapında Ağ (kısaca WWW veya Web), İnternet üzerinde yayınlanan birbirleriyle bağlantılı hiper-metin dokümanlarından oluşan bir bilgi sistemidir. Bu dokümanların her birine Web sayfası adı verilir ve Web sayfalarına İnternet kullanıcısının bilgisayarında çalışan Web tarayıcısı adı verilen bilgisayar programları aracılığıyla erişilir. Web sayfalarında metin, imaj, video ve diğer multimedya ögeleri bulunabilir ve diğer bağlantı ya da link adı verilen hiper-bağlantılar ile başka Web sayfalarına geçiş yapılabilir.

İnternet ve Web terimleri aynı olguyu tanımlamaz. Zira Web sadece İnternet üzerinde çalışan bir servistir. Web kavramı, CERN'de bir bilgisayar programcısı olan Tim Berners-Lee'nin HTML adlı metin işaretleme dilini geliştirmesiyle oluşmuştur. Bugün de kendisinin başkanı olduğu W3C (World Wide Web Consortium) tarafından standartları belirlenmektedir.

Yapısı
Web’in temeli İnternet'tir. Web İnternet üzerinde kurulmuştur ve İnternet'in sunduğu mekanizmalardan çoğunun kullanılmasını sağlar. İnternet'in fiziksel görünüşleri –bilgisayarlar, ağlar ve servisler– Dünya üzerindeki diğer binlerce bilgisayara bağlanmamıza izin verir. Web, İnternet'in en tepesindeki soyutlanmış genel servisler kümesidir. World Wide Web (W3), insanların fikir ve projelerinin paylaşılmasını sağlayan bir bilgi ve kültür havuzudur. İstemci-sunucu uygulamaları ile yapılan birçok organizasyon üzerinde Web tarayıcıları istemci olarak çalışabilirler. Web yürütümü standart İstemci-sunucu modelini izler. Aşağıdaki şekilde gösterildiği gibi "Web tarayıcısı" adı verilen programı çalıştıran bir istemci bilgisayar ile Web sunucu yazılımı çalıştıran bir sunucu bilgisayar arasındaki etkileşime "istemci-sunucu" etkileşimi adı verilir. İstemci bilgisayar sunucudan HTTP'yi (Hypertext Transfer Protocol) ve İnternet mesaj standardı TCP/IP'yi kullanarak bir doküman ister ve sunucu istemcinin göstereceği dokümanı geri döndürür.-
-
WebPack             (GITHUB)  (test) 
Webpack nedir
Webpack is an open-source JavaScript module bundler. It is made primarily for JavaScript, but it can transform front-end assets such as HTML, CSS, and images if the corresponding loaders are included. webpack takes modules with dependencies and generates static assets representing those modules.
Webpack takes the dependencies and generates a dependency graph allowing web developers to use a modular approach for their web application development purposes. It can be used from the command line, or can be configured using a config file which is named webpack.config.js. This file is used to define rules, plugins, etc., for a project. (webpack is highly extensible via rules which allow developers to write custom tasks that they want to perform when bundling files together.)
Node.js is required for using webpack.
webpack provides code on demand using the moniker code splitting. The Technical Committee 39 for ECMAScript is working on standardization of a function that loads additional code: "proposal-dynamic-import".
-
-
-
Webpack is a tool that has got a lot of attention in the last few years, and it is now seen used in almost every project. Learn about it.

Using webpack allows you to use import or require statements in your JavaScript code to not just include other JavaScript, but any kind of file, for example CSS.
Webpack aims to handle all our dependencies, not just JavaScript, and loaders are one way to do that.
For example, in your code you can use:
import 'style.css'
-
What is webpack?
Webpack is a tool that lets you compile JavaScript modules, also known as module bundler.
Given a large number of files, it generates a single file (or a few files) that run your app.
It can perform many operations:
•	helps you bundle your resources.
•	watches for changes and re-runs the tasks.
•	can run Babel transpilation to ES5, allowing you to use the latest JavaScript features without worrying about browser support.
•	can transpile CoffeeScript to JavaScript
•	can convert inline images to data URIs.
•	allows you to use require() for CSS files.
•	can run a development webserver.
•	can handle hot module replacement.
•	can split the output files into multiple files, to avoid having a huge js file to load in the first page hit.
•	can perform tree shaking.
Webpack is not limited to be use on the frontend, it’s also useful in backend Node.js development as well.
Predecessors of webpack, and still widely used tools, include:
•	Grunt
•	Broccoli
•	Gulp
There are lots of similarities in what those and Webpack can do, but the main difference is that those are known as task runners, while webpack was born as a module bundler.
It’s a more focused tool: you specify an entry point to your app (it could even be an HTML file with script tags) and webpack analyzes the files and bundles all you need to run the app in a single JavaScript output file (or in more files if you use code splitting).
Installing webpack
Webpack can be installed globally or locally for each project.
Global install
Here’s how to install it globally with Yarn:
yarn global add webpack webpack-cli
with npm:
npm i -g webpack webpack-cli
once this is done, you should be able to run
webpack-cli
 
Local install
Webpack can be installed locally as well. It’s the recommended setup, because webpack can be updated per-project, and you have less resistance to using the latest features just for a small project rather than updating all the projects you have that use webpack.
With Yarn:
yarn add webpack webpack-cli -D
with npm:
npm i webpack webpack-cli --save-dev
Once this is done, add this to your package.json file:
{
  //...
  "scripts": {
    "build": "webpack"
  }
}
once this is done, you can run webpack by typing
yarn build
in the project root.
Webpack configuration
By default, webpack (starting from version 4) does not require any config if you respect these conventions:
•	the entry point of your app is ./src/index.js
•	the output is put in ./dist/main.js.
•	Webpack works in production mode
You can customize every little bit of webpack of course, when you need. The webpack configuration is stored in the webpack.config.js file, in the project root folder.
The entry point
By default the entry point is ./src/index.js This simple example uses the ./index.js file as a starting point:
module.exports = {
  /*...*/
  entry: './index.js'
  /*...*/
}
The output
By default the output is generated in ./dist/main.js. This example puts the output bundle into app.js:
module.exports = {
  /*...*/
  output: {
    path: path.resolve(__dirname, 'dist'),
    filename: 'app.js'
  }
  /*...*/
}
Loaders
Using webpack allows you to use import or require statements in your JavaScript code to not just include other JavaScript, but any kind of file, for example CSS.
Webpack aims to handle all our dependencies, not just JavaScript, and loaders are one way to do that.
For example, in your code you can use:
import 'style.css'
by using this loader configuration:
module.exports = {
  /*...*/
  module: {
    rules: [
      { test: /\.css$/, use: 'css-loader' },
    ]
  }
  /*...*/
}
The regular expression targets any CSS file.
A loader can have options:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.css$/,
        use: [
          {
            loader: 'css-loader',
            options: {
              modules: true
            }
          }
        ]
      }
    ]
  }
  /*...*/
}
You can require multiple loaders for each rule:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.css$/,
        use:
          [
            'style-loader',
            'css-loader',
          ]
      }
    ]
  }
  /*...*/
}
In this example, css-loader interprets the import 'style.css' directive in the CSS. style-loader is then responsible for injecting that CSS in the DOM, using a <style> tag.
The order matters, and it’s reversed (the last is executed first).
What kind of loaders are there? Many! You can find the full list here.
A commonly used loader is Babel, which is used to transpile modern JavaScript to ES5 code:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /(node_modules|bower_components)/,
        use: {
          loader: 'babel-loader',
          options: {
            presets: ['@babel/preset-env']
          }
        }
      }
    ]
  }
  /*...*/
}
This example makes Babel preprocess all our React/JSX files:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.(js|jsx)$/,
        exclude: /node_modules/,
        use: 'babel-loader'
      }
    ]
  },
  resolve: {
    extensions: [
      '.js',
      '.jsx'
    ]
  }
  /*...*/
}
See the babel-loader options here.
Plugins
Plugins are like loaders, but on steroids. They can do things that loaders can’t do, and they are the main building block of webpack.
Take this example:
module.exports = {
  /*...*/
  plugins: [
    new HTMLWebpackPlugin()
  ]
  /*...*/
}
The HTMLWebpackPlugin plugin has the job of automatically creating an HTML file, adding the output JS bundle path, so the JavaScript is ready to be served.
There are lots of plugins available.
One useful plugin, CleanWebpackPlugin, can be used to clear the dist/ folder before creating any output, so you don’t leave files around when you change the name of the output file:
module.exports = {
  /*...*/
  plugins: [
    new CleanWebpackPlugin(['dist']),
  ]
  /*...*/
}
The webpack mode
This mode (introduced in webpack 4) sets the environment on which webpack works. It can be set to development or production (defaults to production, so you only set it when moving to development)
module.exports = {
  entry: './index.js',
  mode: 'development',
  output: {
    path: path.resolve(__dirname, 'dist'),
    filename: 'app.js'
  }
}
Development mode:
•	builds very fast
•	is less optimized than production
•	does not remove comments
•	provides more detailed error messages and suggestions
•	provides a better debugging experience
Production mode is slower to build, since it needs to generate a more optimized bundle. The resulting JavaScript file is smaller in size, as it removes many things that are not needed in production.
I made a sample app that just prints a console.log statement.
Here’s the production bundle:
 
Here’s the development bundle:
 
Running webpack
Webpack can be run from the command line manually if installed globally, but generally you write a script inside the package.json file, which is then run using npm or yarn.
For example this package.json scripts definition we used before:
"scripts": {
  "build": "webpack"
}
allows us to run webpack by running
npm run build
or
yarn run build
or
yarn build
Watching changes
Webpack can automatically rebuild the bundle when a change in your app happens, and keep listening for the next change.
Just add this script:
"scripts": {
  "watch": "webpack --watch"
}
and run
npm run watch
or
yarn run watch
or
yarn watch
One nice feature of the watch mode is that the bundle is only changed if the build has no errors. If there are errors, watch will keep listening for changes, and try to rebuild the bundle, but the current, working bundle is not affected by those problematic builds.
Handling images
Webpack allows us to use images in a very convenient way, using the file-loader loader.
This simple configuration:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.(png|svg|jpg|gif)$/,
        use: [
          'file-loader'
        ]
      }
    ]
  }
  /*...*/
}
Allows you to import images in your JavaScript:
import Icon from './icon.png'

const img = new Image()
img.src = Icon
element.appendChild(img)
(img is an HTMLImageElement. Check the Image docs)
file-loader can handle other asset types as well, like fonts, CSV files, xml, and more.
Another nice tool to work with images is the url-loader loader.
This example loads any PNG file smaller than 8KB as a data URL.
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.png$/,
        use: [
          {
            loader: 'url-loader',
            options: {
              limit: 8192
            }
          }
        ]
      }
    ]
  }
  /*...*/
}
Process your SASS code and transform it to CSS
Using sass-loader, css-loader and style-loader:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.scss$/,
        use: [
          'style-loader',
          'css-loader',
          'sass-loader'
        ]
      }
    ]
  }
  /*...*/
}
Generate Source Maps
Since webpack bundles the code, Source Maps are mandatory to get a reference to the original file that raised an error, for example.
You tell webpack to generate source maps using the devtool property of the configuration:
module.exports = {
  /*...*/
  devtool: 'inline-source-map',
  /*...*/
}
devtool has many possible values, the most used probably are:
•	none: adds no source maps
•	source-map: ideal for production, provides a separate source map that can be minimized, and adds a reference into the bundle, so development tools know that the source map is available. Of course you should configure the server to avoid shipping this, and just use it for debugging purposes
•	inline-source-map: ideal for development, inlines the source map as a Data URL
-
-
-
SonarQube (formerly Sonar)[2] is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security vulnerabilities.[3][4]
SonarQube can record metrics history and provides evolution graphs. SonarQube provides fully automated analysis and integration with Maven, Ant, Gradle, MSBuild and continuous integration tools (Atlassian Bamboo, Jenkins, Hudson, etc.).
 
Bir önceki yazımda ( Yazılım Ekiplerinde Kod Kalitesi ve Kalitede Sürekliliğin Sağlanması ) Continuous Code Quality kavramından, DevOps kültüründeki, ve bir CI pipleline’ındaki yerinden bahsetmiştim. Özellikle büyük veya büyüme kapasitesi olan ekiplerde her commit’in ana branch’e merge edilmeden önce mutlaka code-review işlemine tabi tutulması gerektiğini ve bu işlemden önce bir statik kod analiz aracı kullanımının, kod kalitesi, hatasızlık ve zaman tasarrufu açısından faydalı olacağından bahsetmiştim. Bu yazımda SonarQube statik kod analiz aracından bahsedeceğim.
 
SonarQube (eski ismiyle Sonar), Sonarsource firmasının iki ana ürününden birisidir. Diğer ürün SonarLint’de yine bir code quality tool’udur, ancak SonarQube’den farklı olarak yazılımın geliştirme aşamasında, yani henüz kodumuzu commit’lemeden önce bizi uyarır. Bir çok popüler IDE için desteği mevcuttur. “Fix issues before they exist” diye de bir sloganları var. Bu yazıda SonarLint’den bahsetmeyeceğim. Merak edenleriniz buradan inceleyebilirler.
SonarQube’ü açık kaynak projeleriniz için cloud üzerinden ücretsiz olarak kullanabileceğiniz gibi, private projeler için on-premise kurulum yaparak da kullanabilirsiniz. Cloud üzerinden private projeleriniz için ücret ödemeniz gerekiyor.
Amacım SonarQube kullanımı hakkında bilgi vermek olduğu için, kurulum kısmını atlayıp, konuyu cloud üzerinden yani https://sonarcloud.io uygulaması üzerinden anlatacağım. SonarQube’ü aşağıda listelediğim 6 ana başlıkta inceleyeceğiz;
•	https://sonarcloud.io uygulamasına giriş ve boş bir proje oluşturma
•	Kişisel bilgisayarımıza SonarScanner kurulumu
•	SonarScanner ile örnek bir .Net projesinin analiz edilmesi ve raporun sonarcloud.io ya gönderilmesi
•	sonarcloud.io ya gönderilen raporun yorumlanması
•	Quality Gates, Quality Profiles ve Rules kavramları hakkında
•	SonarQube’ün bir CI pipeline’ına dahil edilmesi
sonarcloud.io SaaS Uygulamasına Giriş
SonarQube’ü açık kaynak projeler için cloud üzerinden ücretsiz olarak kullanabiliyoruz. Giriş ekranında sizi 3 OAuth seçeneği karşılıyor. Ben github hesabımla giriş yaptım.
 
Ekran 1: OAuth login seçenekleri
Sağ üst köşedeki menüden “Create new organization” diyerek bir organizasyon oluşturarak başlıyoruz. SonarQube de her proje bir organizasyona bağlıdır. Daha sonra yine sağ üst köşeden “Analyze new project” diyoruz ve aşağıdaki ekranla karşılaşıyoruz.
 
 
Ekran 2: Mevcut organizasyona proje ekleme
Önceki adımda oluşturduğumuz organizasyonumuz seçili geliyor. Devam diyoruz ve sonraki adımda bir token oluşturuyoruz. Oluşan token değeri lazım olacak, hemen bir yere kopyalayıp devam ediyoruz ve aşağıdaki ekrandan projemizin ana programlama dilini seçiyoruz ve projemiz için bir anahtar kelime belirliyoruz.( Projede birden fazla dil olabileceğinden ötürü (c#, html, js) ana dil ifadesi kullanılmış )
 
 
Ekran 3: Projenin ana programlama dili seçimi
Done butonuna tıkladıktan sonra aşağıdaki ekranla karşılaşacaksınız. Bu ekranda gördüğünüz üzere Scanner’ı çalıştırmak ve projenizi analiz edebilmek için komut satırından çalıştırmanız gereken 3 adet komut yer almakta. Bu 3 komutu bir yere kopyalayıp ekranı kapatabilirsiniz.
 
 
Ekran 4: Proje ekleme son adım
SonarScanner Kurulumu
Yukarıdaki ekran görüntüsünde dikkat ederseniz ”Download and unzip the Scanner for MSBuild” ifadesi yer alıyor. Peki neden MSBuild Scanner? Dil olarak C# seçtiğimiz için SonarQube ilgili Scanner hangisiyse onu indirmemiz gerektiğini söylüyor. Download butonu ile yönlendirileceğiniz ekrandaki talimatları yerine getirerek MSBuild Scanner’ını bilgisayarınıza kurmuş olacaksınız. (%PATH% güncellemesini unutmayınız) Bu arada hangi tür Scanner kurarsanız kurun sisteminizde mutlaka java’nın kurulu olması gerektiğini belirteyim. JRE hakkındaki detayları da yine download butonuyla yönlendirileceğiniz sayfadan görebilirsiniz.
MSBuild SonarScanner İle Projenin Kod Analizi
Önceki adımda kurduğumuz Scanner’ı kullanarak C# dili ile yazılmış olan projemiz için static kod analizini yapabiliriz artık. Bunun için projenin ana dizinindeyken sırasıyla aşağıdaki 3 komutu çalıştırmamız yeterli. Son komuttan sonra 5 nolu ekran görüntüsüyle karşılaşmanız raporun oluştuğu ve sonarcloud’a gönderildiği anlamına gelir. (rabbitmq sonarcloud da projemi oluştururken verdiğim key’in ismi)
 
Ekran 5: Kod analizinin başarıyla yapılması
SonarScanner.MSBuild.exe begin /k:"<proje unique key buraya>" /d:sonar.organization="<organizasyon_adi_buraya>" /d:sonar.host.url="https://sonarcloud.io" /d:sonar.login="<olusturulan_token_adi_buraya>"MsBuild.exe /t:RebuildSonarScanner.MSBuild.exe end /d:sonar.login="<olusturulan_token_degeri_buraya>"
Bu 3 komutta koyulaştırdığım kısımlar sizin proje ve organizasyon isminize göre değişecektir. 4 nolu ekran görüntüsünde yer alan bu 3 komutu bir yere kopyaladıysanız, hızlıca komut satırından çalıştırabilirsiniz. Komut satırında “Post-processing succeded” ifadesini gördükten sonra artık sonarcloud’a geçiş yaparak projenizin analiz edildiğini görebilmelisiniz.
 
 
Ekran 6: “Passed” :)
106 sayısı kod satır sayısını ifade etmekte. Hemen altında da C# ibaresi, programlama dilini işaret ediyor. Evet, çok küçük ve XS (X Small) kategorisinde bir proje olmasına rağmen yeşil renkli “Passed” ifadesini görmek güzel hissettiriyor :) Buradan inceleyebilirsiniz.
Oluşan Raporun Yorumlanması
Raporu yorumlamadan önce hemen belirteyim, örnek proje zamanında RabbitMQ yü kurcalamak için oluşturduğum bir Asp.Net MVC uygulaması.
Proje public bir proje olduğu için sonarcloud hesabınız olmasa bile, yani sonarcloud a giriş yapmadan da oluşan son raporu buradan görebilirsiniz.
Linke tıkladıysanız aşağıdaki ekranla karşılaşmış olmanız gerekiyor.
 
Ekran 7: rabbitmq key’i ile oluşturduğum projemin sonarcloud dashboard’u
Dikkat ederseniz rapor 5 ana başlıktan oluşuyor. Bunlar;
Bugs
Projenizde bu kategoriye alınan kodlarınız varsa bir şeylerin yanlış veya eksik olduğundan emin olabilirsiniz. Eğer düzeltilmezse ileride başınız ağrıyabilir, dolayısıyla bu kategoriyi önemseyin. Bizim 109 satırlık küçük uygulamamızda Bug olarak nitelendirilecek bir şey bulamamış sevgili SonarQube. A alarak geçmişiz bu dersten :) Eğer ne tür durumların bug olarak ele alındığını merak ediyorsanız buraya tıklayarak public projeleri keşfedebilir, raporları dilediğinizce inceleyerek bir çok şey öğrenebilirsiniz.
Vulnerabilities
Güvenlik zafiyetine sebebiyet verecek olan kod parçacıkları bu kategoride raporlanmaktadır. Yani bu kategoride en az Bug kategorisi kadar önemli. Göz ardı etmemekte fayda var, bir örnek olması açısından rastgele public bir projeden aldığım aşağıda ekran görüntüsünü paylaşmak istedim.
 
 
Ekran 8: SonarQube’ün zafiyet olarak gördüğü alert kullanımı
Burada bir js dosyası içerisinde kullanılan alert metodu zafiyet olarak görülmüş. Üç nokta (…) icon’una tıklayarak açılan aşağıdaki bilgilendirme ekranında SonarQube’ün gerekçesini de görebilmeniz mümkün. Özetle, debug modda alert’e tamam ama production da hassas veri gösterme riski taşıdığından burada bir zafiyet var arkadaş, diyor SonarQube.
 
 
Ekran 9: SonarQube: Production ortamında unutulan alert can yakabilir
Code Smells
İlk 2 maddemiz kadar risk teşkil etmeyen ancak, kod okunabilirliği ve bakım maliyetleri açısından negatif etki yapabilecek kod parçacıkları bu kategoride değerlendirilmekte. Raporumuzda 3 adet code smells olduğu görülüyor. Hemen yanında yazan 14 min ifadesi ise, bu 3 adet sorunun tahmini çözüm süresi :) Evet SonarQube sağ olsun onu da hesaplıyor ama tabi bu değerde bir hata payı olabileceğini söylememe gerek yok sanırım. Şimdi bu 3 rakamına tıklayarak aşağıdaki detay ekranına erişiyoruz.
 
 
Ekran 10: Code Smell liste ekranı
Listede koyu renkle yazılı olan özet bilgisi aslında gerekli detayı veriyor. Örneğin ilk maddede sadece constructor içerisinden değer atanan _hostName adındaki field’ın readonly olarak işaretlenmesi gerektiğinden bahsediyor. Eğer bu maddenin üzerine tıklarsanız aşağıdaki detay ekranında ilgili kod dosyasının ilgili satırını görebiliyorsunuz.
 
 
Ekran 11: Code Smell detayı
Burada yine Make _hostName readonly ifadesinin yanındaki üç nokta (…) ya tıklarsanız ekranın en altında o maddeyle alakalı bilgilendirici açıklamayı görebilirsiniz. SonarQube sadece açıklarınız bulmuyor aynı zamanda eğitiyor da :) Gördüğünüz gibi içerikte örnek kod bile var.

Coverage
Buradan yazılan testlerin projenizin ne kadarını cover ettiğiniz görebilirsiniz. Bu küçük projede test yazılmadığı için değerimiz %0 ve kırmızı renk bir uyarı manasında, yani unit test yazılmadığını ifade ediyor.
Duplications
Proje genelinde tekrarlı kod satırlarının toplam kod satırına oranını ifade eder. Örneğin 1000 satır kod içerisinde toplamda 100 satır eğer tekrarlı ise bu değer %10 olacaktır. Örnek projemiz az sayıda kod içerdiğinden ötürü hiç tekrarlı kod yoktur. Ben orta ve büyük ölçekli projelerde bu değerin %40- 50 'lerde olabildiğini görmüştüm ki çok ciddi bir oran. SonarQube detaylı olarak aşağıdaki ekran görüntüsünde gördüğünüz gibi dosya bazında tekrarlı kod oranlarını başarılı şekilde raporlayabiliyor.
 
 
Ekran 13: Kod dosyası bazında rapor detayları
Quality Gates, Quality Profiles ve Rules Kavramları Hakkında
Quality Gate’ler projemizin production ortamı için hazır olup olmadığına karar veren kural dizileridir. Bir başka değişle, kod analiz raporunun başarılı mı (Passed), başarısız mı (Failed) olacağının belirlenmesi için kullanılırlar. 6 nolu ekran görüntüsünde yer alan yeşil renkli “Passed” ifadesi, kod analiz işlemi esnasında mevcut Quality Gate’lerin hepsinden geçildiği ve rapor sonucunun başarılı olduğu anlamına geliyor.
SonarQube desteklediği her programlama dili için default (ön tanımlı) olarak, Quality Profile, Rules ve Quality Gate tanımlamalarına sahip. Örneğin C# dili için buraya tıklayarak ulaşabileceğiniz, benim oluşturduğum public organizasyonun aşağıdaki ekranında ön tanımlı olarak gelen Quality Gate’leri görebilirsiniz.
Örneğin; en altta belirtilen kuraldan, Security notunun A’dan daha kötü olması raporun başarısız sonuçlanacağını anlamalıyız. Ve en önemlisi, sol üst köşedeki Create butonuyla, kendi custom Quality Gate’inizi oluşturabilirsiniz. Örneğin siz projenizde Security notunun B den daha kötü olması durumunda raporun başarısız olmasını sağlayabilirsiniz.
 
Ekran 14: Ön tanımlı Quality Gate’ler
Peki Security notu nasıl belirleniyor? Burada Rules kavramına değinmemiz gerekiyor. Yukarıdaki ekran görüntüsünde Rules sekmesine tıkladığınızda aşağıdaki ekrana erişeceksiniz. Sol taraftaki filtreleme özelliğini kullanarak yalnızca C# için tanımlı olan kuralları listeledim. Kurallara tıklayarak örnek kod parçacıklarının da yer verildiği bilgilendirici detay sayfalarına göz atabilirsiniz. Dikkat ederseniz C# dili için kural havuzunda toplam 358 adet (21.08.2018 tarihli kural sayısı ) kural tanımlı. Bu kurallar C# ve diğer diller için sürekli olarak artmakta, eklenen yeni kurallarla SonarQube geliştirilmeye devam etmektedir.
 
 
Ekran 15: C# dili için tanılı kurallar listesi
Quality Profile içinse en basit haliyle, çeşitli kuralları içeren gruplardır diyebiliriz. Yani C# dili için kendinize özel oluşturacağınız bir Quality Profile, en az 1 en fazla 358 adet kuraldan oluşabilir.
Özetle SonarQube’ün ön tanımlı Quality Gate ve Quality Profile ını kullanmayıp, kendi tanımlamalarınızı yapabilirsiniz. Üstelik bunları proje bazlı da yapabilirsiniz. Aşağıdaki ekrandan rabbitmq projemiz için Administrator sekmesinde tıkladığımızda açılan menüden bu değişikliği yapabiliriz.
 
 
Ekran 16: Proje bazlı admin ekranları için menü
Bu menüden Quality Gate sekmesinde tıkladığımızda gelen ekrandan, projemiz için istediğimiz Quality Gate’i seçebiliriz. Buradaki sample gate isimli Quality Gate benim örnek olması için yaptığım tanımlama. Dikkat ederseniz Sonar way için, default ibaresi var yani tüm projeler ilk oluşturulduklarında ön tanımlı olarak Sonar way Quality Gate’ine tabi tutulmaktalar.
 
 
Ekran 17: Projenin Quality Gate’inin değiştirilmesi
Aynı şekilde Quality Profiles linkine tıklayarak SonarQube’ün projenize atadığı ön tanımlı profili kullanmayıp kendi tanımladığınız profilin kullanılmasını sağlayabilirsiniz. Ön tanımlı gelen Sonar way isimli Quality Profile tüm kuralları içerirken siz kendi profiliniz için kapsamı daraltmak isteyebilirsiniz mesela.
Örnek vermek gerekirse; kendi özel tanımlayacağınız Quality Profile’ınızdan projemizde 2 adet Code Smell olarak değerlendirilen, readonly field kuralını çıkarırsanız, Code Smell sayımız bire inecektir. Bu yolla siz, “readonly field kuralı benim için code smell değildir”, demiş olursunuz aslında.
SonarQube’ü CI Pipeline’ınıza Dahil Edin
Eğer hali hazırda bir CI yazılımı kullanmaktaysanız, SonarQube veya benzeri bir statik kod analiz aracını CI pipeline’ınıza dahil edebilirsiniz. Bu noktada kullandığınız CI yazılımına göre eforunuz değişecektir. Jenkins, TeamCtiy, Travis CI vs. gibi bazı CI araçları SonarQube için sahip oldukları plugin’ler sayesinde bu eforu minimuma indirmekteler. Diğer bir ifadeyle SonarQube için doğal desteğe sahipler.
Doğal entegrasyona sahip CI araçlarında SonarQube rapor sonucuna göre pipeline’ınızı kırıp, ilgili yerlere bildirim yapabilirsiniz. Örneğin build adımından sonra SonarQube’ü tetikler, raporu sonucunu bekletir ve rapor başarısız olursa bir sonraki adıma geçmeyebilirsiniz.
Plugin ile doğal entegrasyon imkanı olmayan CI araçlarında SonarQube’ü tetiklemek için ise, yml scriptinize MSBuild SonarScanner İle Projenin Kod Analizi bölümünde belirttiğimiz komutları ekleyerek ilerleyebilirsiniz. Bu noktada rapor sonucunu bekleyerek, gelen sonucuna göre bir aksiyon almak mümkün müdür açıkçası bilmiyorum. Yorumlarınızı alabilirim.

-
-
-
UNIX		(GITHUB)
Unix nedir
Unix (/ˈjuːnɪks/; trademarked as UNIX) is a family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix, development starting in the 1970s at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[3]
-Unix vs linux yazılar var: https://chandigarhinfo.in/unix-vs-linux-whats-the-difference/
-
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
Linux 		(GITHUB)
Linux nedir

The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Bilgisayar işletim sistemlerinin en temel parçası olan çekirdek yazılımlarından bir tanesidir.verilmiştir.[1]Günümüzde süper bilgisayarlarda, akıllı cihazların ve internet altyapısında kullanılan cihazların işletim sistemlerinde yaygın olarak kullanılmaktadır. Bunlardan en popüler olanı Google tarafından geliştirilen Android işletim sistemidir.
Android, Linux çekirdeği üzerine inşa edilmiş bir mobil işletim sistemidir.
Android işletim sistemi beş kısımdan oluşur.
1.	Çekirdek: Linux kernelidir. Güvenlik, hafıza yönetimi, süreç yönetimi, ağ yığınları ve sürücü modellerini içermektedir.
2.	Android Runtime: Sanal makinedir. Dalvik Sanal Makinesini de içermektedir. 5.0 ile Dalvik kaldırılmış ve ART'ye geçilmiştir.
3.	Kütüphaneler: Veritabanı kütüphaneleri, web tarayıcı kütüphaneleri, grafik ve arayüz kütüphanelerini içermektedir.
4.	Uygulama Çatısı: Uygulama geliştiricilere geniş bir platform sunan kısımdır.
5.	Uygulama Katmanı: Doğrudan Java (programlama dili) ile geliştirilmiş uygulamaları içermektedir.
-
-
-
-
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
UNIX		(bosch)	 (bosch)
unix nedir
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
-
Combining webpack with ASP.NET MVC 5
 

Jonathan Harrison


Sep 20, 2017·4 min read

I am currently working on an ASP.NET MVC 5 web application in Visual Studio 2017 with my colleague matthewygf; on other recent projects we have been using a typical JavaScript Single Page Application (SPA) stack e.g. npm, gulp, etc.
Even though ASP.NET MVC 5 and Visual Studio includes everything needed for development i.e. package management with NuGet, TypeScript and LESS compiling, bundling and minification etc; we wanted a development pipeline such that if we ever decided to move to a SPA, reuse it on other new projects or replace gulp on others we could.
This lead us to build a “hybrid” combining webpack with ASP.NET MVC 5; it suits our needs very well so we thought we would share our template.
The complete project is over on GitHub if you just want to download it and start using it, otherwise you can create it from scratch by following these steps.
Create an ASP.NET Web Application project
First of all create an ASP.NET MVC Web application project in Visual Studio. Specifically we used:
•	.NET Framework 4.6.1
•	MVC
•	Authentication: No Authentication
Uninstall JavaScript and CSS NuGet packages
By default the ASP.NET MVC template includes Bootstrap and jQuery downloaded from NuGet. We want to use npm instead, so uninstall the following NuGet packages:
•	bootstrap
•	jQuery
•	jQuery.Validation
•	Microsoft.jQuery.Unobstrusive.Validation
•	Modernizr
•	Respond
After uninstalling the packages, make sure the Fonts and Scripts folders are deleted and also,the Content folder only contains Site.css.
Create package.json and install dependencies
Now that we have a clean starting point, in the project folder open a terminal and run.
npm init
Just accept all the default values and then you will have a package.json file created. After that its time to use:
npm install --save
To install the following packages:
•	bootstrap
•	jquery
•	jquery-validation
•	jquery-validation-unobtrusive
With the main dependencies installed, we need to install the development dependencies which includes webpack and other to support TypeScript and LESS. Use:
npm install -D
To install the following packages:
•	@types/bootstrap
•	@types/jquery
•	@types/jquery-validation-unobtrusive
•	@types/jquery.validation
•	awesome-typescript-loader
•	clean-webpack-plugin
•	css-loader
•	extract-text-webpack-plugin
•	file-loader
•	html-loader
•	html-webpack-plugin
•	less
•	less-loader
•	source-map-loader
•	style-loader
•	typescript
•	webpack
•	webpack-merge
Create src folder
All the dependencies are installed so create a folder called src in the root of the ASP.NET MVC project; this folder will contain all the TypeScript and LESS source files.
After creating the folder, move the Site.css file from the Content folder to src and rename it index.less. Open the index.less file and at the top add this:
@import "../node_modules/bootstrap/less/bootstrap.less";
This will make the bootstrap classes and styles available across our application.
Now create a index.ts file and add the following to it:
import "./index.less";
import "bootstrap";
This imports the LESS file we just created and also bootstrap in order that bootstrap components such as the navigation bar work as expected.
Modify _Layout.cshtml
As per webpack’s best practices for production, our production webpack configuration will generate multiple bundles each having a unique name every time they build. webpack can inject tags for these generated files into a HTML file, we will use the MVC shared layout file for this.
First of all rename the _Layout.cshtml file in Views/Shared to _Layout_Template.cshtml; this is because webpack will generate _Layout.cshtml from _Layout_Template.cshtml with the script and link tags appended at the end of the body element.
Create tsconfig.json
To support TypeScript compilation, you will need to create a tsconfig.json file at the root of the ASP .NET project. This file specifies the root files and the compiler options required to compile the TypeScript files. Simply copy the file from over on the GitHub project: tsconfig.json.
Create webpack configuration files
Now its time to create the webpack configuration files, create these at the root of the ASP .NET project. Simply copy the files from over on the GitHub project: webpack.common.js, webpack.dev.js, webpack.prod.js
Running webpack
Now that everything is setup, we just need to run webpack. We can use npm scripts to do this, so in your package.json add the following:
"scripts": {
"build:dev": "webpack --config webpack.dev.js",
"build:prod": "webpack --config webpack.prod.js"
}
You will need to be run build:dev for example every time you build your ASP.NET project. This can be done by using the NPM Task Runner extension for Visual Studio, where you can setup a binding for BeforeBuild to run build:dev.
After you have configured the extension, every time you build it runs the selected script. Note — this will only work in Visual Studio not on a build server; for a build server you will need to run these npm scripts manually.
If you followed all the steps above or grabbed the code from GitHub, just run the project and you will see the same old ASP.NET bootstrap starter site.
-
-
-
A single-page application (SPA) is a web application or website that interacts with the user by dynamically rewriting the current web page with new data from the web server, instead of the default method of a web browser loading entire new pages. The goal is faster transitions that make the website feel more like a native app.
In a SPA, a page refresh never occurs; instead, all necessary HTML, JavaScript, and CSS code is either retrieved by the browser with a single page load,[1] or the appropriate resources are dynamically loaded and added to the page as necessary, usually in response to user actions. The page does not reload at any point in the process, nor does it transfer control to another page, although the location hash or the HTML5 History API can be used to provide the perception and navigability of separate logical pages in the application.[2]
History
The origins of the term single-page application are unclear, though the concept was discussed at least as early as 2003.[3] Stuart Morris, a programming student at Cardiff University, Wales, wrote the Self-Contained website at slashdotslash.com with the same goals and functions in April 2002,[4] and later the same year Lucas Birdeau, Kevin Hakman, Michael Peachey and Clifford Yeh described a single-page application implementation in US patent 8,136,109.[5]
JavaScript can be used in a web browser to display the user interface (UI), run application logic, and communicate with a web server. Mature open-source libraries are available that support the building of a SPA, reducing the amount of JavaScript code developers have to write.
Technical approaches
There are various techniques available that enable the browser to retain a single page even when the application requires server communication.
Document Hashes
HTML authors can leverage element IDs to show or hide different sections of the HTML document. Then, using CSS, authors can use the `#target` selector to only show the section of the page which the browser navigated to.
JavaScript frameworks
Web browser JavaScript frameworks and libraries, such as AngularJS, Ember.js, ExtJS, Knockout.js, Meteor.js, React, Vue.js, and Svelte have adopted SPA principles. Aside from ExtJS, all of these are open-source.
•	AngularJS is a fully client-side framework. AngularJS's templating is based on bidirectional UI data binding. Data-binding is an automatic way of updating the view whenever the model changes, as well as updating the model whenever the view changes. The HTML template is compiled in the browser. The compilation step creates pure HTML, which the browser re-renders into the live view. The step is repeated for subsequent page views. In traditional server-side HTML programming, concepts such as controller and model interact within a server process to produce new HTML views. In the AngularJS framework, the controller and model states are maintained within the client browser. Therefore, new pages are capable of being generated without any interaction with a server.
•	Ember.js is a client-side JavaScript web application framework based on the model–view–controller (MVC) software architectural pattern. It allows developers to create scalable single-page applications by incorporating common idioms and best practices into a framework that provides a rich object model, declarative two-way data binding, computed properties, automatically updating templates powered by Handlebars.js, and a router for managing application state.
•	ExtJS is also a client side framework that allows creating MVC applications. It has its own event system, window and layout management, state management (stores) and various UI components (grids, dialog windows, form elements etc.). It has its own class system with either dynamic or static loader. The application built with ExtJS can either exist on its own (with state in the browser) or with the server (e.g. with REST API that is used to fill its internal stores). ExtJS has only built in capabilities to use localStorage so larger applications need a server to store state.
•	Knockout.js is a client side framework which uses templates based on the Model-View-ViewModel pattern.
•	Meteor.js is a full-stack (client-server) JavaScript framework designed exclusively for SPAs. It features simpler data binding than Angular, Ember or ReactJS,[6] and uses the Distributed Data Protocol[7] and a publish–subscribe pattern to automatically propagate data changes to clients in real-time without requiring the developer to write any synchronization code. Full stack reactivity ensures that all layers, from the database to the templates, update themselves automatically when necessary. Ecosystem packages such as Server Side Rendering[8] address the problem of Search Engine Optimization.
•	React is a JavaScript library for building user interfaces. It is maintained by Facebook, Instagram and a community of individual developers and corporations. React uses a new language which is a mix of JS and HTML (a subset of HTML). Several companies use React with Redux (JavaScript library) which adds state management capabilities, which (with several other libraries) lets developers create complex applications.[9]
•	Vue.js is a JavaScript framework for building user interfaces. Vue developers also provide Vuex for state management.
•	Svelte is a framework for building user interfaces that compiles Svelte code to JavaScript DOM manipulations, avoiding the need to bundle a framework to the client, and allowing for simpler application development syntax.
Ajax
As of 2006, the most prominent technique used was Ajax.[1] Ajax involves using asynchronous requests to a server for XML or JSON data, such as with JavaScript's XMLHttpRequest or more modern fetch() (since 2017), or the deprecated ActiveX Object. In contrast to the declarative approach of most SPA frameworks, with Ajax the website directly uses JavaScript or a JavaScript library such as jQuery to manipulate the DOM and edit HTML elements. Ajax has further been popularized by libraries like jQuery, which provides a simpler syntax and normalizes Ajax behavior across different browsers which historically had varying behavior.
WebSockets
WebSockets are a bidirectional real-time client-server communication technology that are part of the HTML5 specification. For real-time communication, their use is superior to Ajax in terms of performance[10] and simplicity.
Server-sent events
Server-sent events (SSEs) is a technique whereby servers can initiate data transmission to browser clients. Once an initial connection has been established, an event stream remains open until closed by the client. SSEs are sent over traditional HTTP and have a variety of features that WebSockets lack by design such as automatic reconnection, event IDs, and the ability to send arbitrary events.[11]
Browser plugins
Although this method is outdated, asynchronous calls to the server may also be achieved using browser plug-in technologies such as Silverlight, Flash, or Java applets.
Data transport (XML, JSON and Ajax)
Requests to the server typically result in either raw data (e.g., XML or JSON), or new HTML being returned. In the case where HTML is returned by the server, JavaScript on the client updates a partial area of the DOM (Document Object Model).[12] When raw data is returned, often a client-side JavaScript XML / (XSL) process (and in the case of JSON a template) is used to translate the raw data into HTML, which is then used to update a partial area of the DOM.
Server architecture
Thin server architecture
A SPA moves logic from the server to the client, with the role of the web server evolving into a pure data API or web service. This architectural shift has, in some circles, been coined "Thin Server Architecture" to highlight that complexity has been moved from the server to the client, with the argument that this ultimately reduces overall complexity of the system.
Thick stateful server architecture
The server keeps the necessary state in memory of the client state of the page. In this way, when any request hits the server (usually user actions), the server sends the appropriate HTML and/or JavaScript with the concrete changes to bring the client to the new desired state (usually adding/deleting/updating a part of the client DOM). At the same time, the state in server is updated. Most of the logic is executed on the server, and HTML is usually also rendered on the server. In some ways, the server simulates a web browser, receiving events and performing delta changes in server state which are automatically propagated to client.
This approach needs more server memory and server processing, but the advantage is a simplified development model because a) the application is usually fully coded in the server, and b) data and UI state in the server are shared in the same memory space with no need for custom client/server communication bridges.
Thick stateless server architecture
This is a variant of the stateful server approach. The client page sends data representing its current state to the server, usually through Ajax requests. Using this data, the server is able to reconstruct the client state of the part of the page which needs to be modified and can generate the necessary data or code (for instance, as JSON or JavaScript), which is returned to the client to bring it to a new state, usually modifying the page DOM tree according to the client action that motivated the request.
This approach requires that more data be sent to the server and may require more computational resources per request to partially or fully reconstruct the client page state in the server. At the same time, this approach is more easily scalable because there is no per-client page data kept in the server and, therefore, Ajax requests can be dispatched to different server nodes with no need for session data sharing or server affinity.
Running locally
Some SPAs may be executed from a local file using the file URI scheme. This gives users the ability to download the SPA from a server and run the file from a local storage device, without depending on server connectivity. If such a SPA wants to store and update data, it must use browser-based Web Storage. These applications benefit from advances available with HTML5.[13]
Challenges with the SPA model
Because the SPA is an evolution away from the stateless page-redraw model that browsers were originally designed for, some new challenges have emerged. Possible solutions (of varying complexity, comprehensiveness, and author control) include:[14]
•	Client-side JavaScript libraries.
•	Server-side web frameworks that specialize in the SPA model.[15][16][17]
•	The evolution of browsers and the HTML5 specification,[18] designed for the SPA model.
Search-engine optimization
Because of the lack of JavaScript execution on crawlers of some popular Web search engines,[19] SEO (Search engine optimization) has historically presented a problem for public facing websites wishing to adopt the SPA model.[20]
Between 2009 and 2015, Google Webmaster Central proposed and then recommended an "AJAX crawling scheme"[21][22] using an initial exclamation mark in fragment identifiers for stateful AJAX pages (#!). Special behavior must be implemented by the SPA site to allow extraction of relevant metadata by the search engine's crawler. For search engines that do not support this URL hash scheme, the hashed URLs of the SPA remain invisible. These "hash-bang" URIs have been considered problematic by a number of writers including Jeni Tennison at the W3C because they make pages inaccessible to those who do not have JavaScript activated in their browser. They also break HTTP referer headers as browsers are not allowed to send the fragment identifier in the Referer header.[23] In 2015, Google deprecated their hash-bang AJAX crawling proposal.[24]
Alternatively, applications may render the first page load on the server and subsequent page updates on the client. This is traditionally difficult, because the rendering code might need to be written in a different language or framework on the server and in the client. Using logic-less templates, cross-compiling from one language to another, or using the same language on the server and the client may help to increase the amount of code that can be shared.
In 2018, Google introduced dynamic rendering as another option for sites wishing to offer crawlers a non-JavaScript heavy version of a page for indexing purposes.[25] Dynamic rendering switches between a version of a page that is rendered client-side and a pre-rendered version for specific user agents. This approach involves your web server detecting crawlers (via the user agent) and routing them to a renderer, from which they are then served a simpler version of HTML content.
Because SEO compatibility is not trivial in SPAs, it is worth noting that SPAs are commonly not used in a context where search engine indexing is either a requirement, or desirable. Use cases include applications that surface private data hidden behind an authentication system. In the cases where these applications are consumer products, often a classic "page redraw" model is used for the applications landing page and marketing site, which provides enough meta data for the application to appear as a hit in a search engine query. Blogs, support forums, and other traditional page redraw artifacts often sit around the SPA that can seed search engines with relevant terms.
Another approach used by server-centric web frameworks like the Java-based ItsNat is to render any hypertext on the server using the same language and templating technology. In this approach, the server knows with precision the DOM state on the client, any big or small page update required is generated in the server, and transported by Ajax, the exact JavaScript code to bring the client page to the new state executing DOM methods. Developers can decide which page states must be crawlable by web spiders for SEO and be able to generate the required state at load time generating plain HTML instead of JavaScript. In the case of the ItsNat framework, this is automatic because ItsNat keeps the client DOM tree in the server as a Java W3C DOM tree; rendering of this DOM tree in the server generates plain HTML at load time and JavaScript DOM actions for Ajax requests. This duality is very important for SEO because developers can build with the same Java code and pure HTML-based templating the desired DOM state in server; at page load time, conventional HTML is generated by ItsNat making this DOM state SEO-compatible.
As of version 1.3,[26] ItsNat provides a new stateless mode, and the client DOM is not kept on the server because, with the stateless mode client, DOM state is partially or fully reconstructed on the server when processing any Ajax request based on required data sent by the client informing the server of the current DOM state; the stateless mode may be also SEO-compatible because SEO compatibility happens at load time of the initial page unaffected by stateful or stateless modes. Another possible choice is frameworks like PreRender, Puppeteer, Rendertron which can be easily integrated into any website as a middleware with web server configuration enabling bot requests (google bot and others) to be served by the middleware while non-bot requests are served as usual. These frameworks cache the relevant website pages periodically to allow latest versions be available to search engines. These frameworks have been officially approved by google.[27]
There are a couple of workarounds to make it look as though the web site is crawlable. Both involve creating separate HTML pages that mirror the content of the SPA. The server could create an HTML-based version of the site and deliver that to crawlers, or it's possible to use a headless browser such as PhantomJS to run the JavaScript application and output the resulting HTML.
Both of these do require quite a bit of effort, and can end up giving a maintenance headache for the large complex sites. There are also potential SEO pitfalls. If server-generated HTML is deemed to be too different from the SPA content, then the site will be penalized. Running PhantomJS to output the HTML can slow down the response speed of the pages, which is something for which search engines – Google in particular – downgrade the rankings.[28]
Client/server code partitioning
One way to increase the amount of code that can be shared between servers and clients is to use a logic-less template language like Mustache or Handlebars. Such templates can be rendered from different host languages, such as Ruby on the server and JavaScript in the client. However, merely sharing templates typically requires duplication of business logic used to choose the correct templates and populate them with data. Rendering from templates may have negative performance effects when only updating a small portion of the page—such as the value of a text input within a large template. Replacing an entire template might also disturb a user's selection or cursor position, where updating only the changed value might not. To avoid these problems, applications can use UI data bindings or granular DOM manipulation to only update the appropriate parts of the page instead of re-rendering entire templates.
Browser history
With a SPA being, by definition, "a single page", the model breaks the browser's design for page history navigation using the "forward" or "back" buttons. This presents a usability impediment when a user presses the back button, expecting the previous screen state within the SPA, but instead, the application's single page unloads and the previous page in the browser's history is presented.
The traditional solution for SPAs has been to change the browser URL's hash fragment identifier in accord with the current screen state. This can be achieved with JavaScript, and causes URL history events to be built up within the browser. As long as the SPA is capable of resurrecting the same screen state from information contained within the URL hash, the expected back-button behavior is retained.
To further address this issue, the HTML5 specification has introduced pushState and replaceState providing programmatic access to the actual URL and browser history.
Analytics
Analytics tools such as Google Analytics rely heavily upon entire new pages loading in the browser, initiated by a new page load. SPAs do not work this way.
After the first page load, all subsequent page and content changes are handled internally by the application, which should simply call a function to update the analytics package. Failing to call said function, the browser never triggers a new page load, nothing gets added to the browser history, and the analytics package has no idea who is doing what on the site.
Security Scanning
Similarly to the problems encountered with search engine crawlers, DAST tools may struggle with these JavaScript-rich applications. Problems can include the lack of hypertext links, memory usage and resources loaded by the SPA typically being made available by an Application Programming Interface or API. Single Page Applications are still subject to the same security risks as traditional web pages such as Cross-Site Scripting (XSS), but also a host of other unique vulnerabilities such as Data Exposure via API and Client Side Logic & Client-Side Enforcement of Server-Side Security.[29] In order to effectively scan a Single Page Application, a DAST scanner must be able to navigate the client-side application in a reliable and repeatable manner to allow discovery of all areas of the application and interception of all requests that the application sends to remote servers (e.g. API requests). There are few commercial tools capable of such actions but such tools definitely exist.
Adding page loads to a SPA
It is possible to add page load events to a SPA using the HTML5 history API; this will help integrate analytics. The difficulty comes in managing this and ensuring that everything is being tracked accurately – this involves checking for missing reports and double entries. Some frameworks provide open source analytics integrations addressing most of the major analytics providers. Developers can integrate them into the application and make sure that everything is working correctly, but there is no need to do everything from scratch.[28]
Speeding up the page load
There are some ways of speeding up the initial load of a SPA, such as a heavy approach to caching and lazy-loading modules when needed. But it's not possible to get away from the fact that it needs to download the framework, at least some of the application code, and will most likely hit an API for data before displaying something in the browser.[28] This is a "pay me now, or pay me later" trade-off scenario. The question of performance and wait-times remains a decision that the developer must make.
Page lifecycle
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Single-page application" – news · newspapers · books · scholar · JSTOR (October 2020) (Learn how and when to remove this template message)
A SPA is fully loaded in the initial page load and then page regions are replaced or updated with new page fragments loaded from the server on demand. To avoid excessive downloading of unused features, a SPA will often progressively download more features as they become required, either small fragments of the page, or complete screen modules.
In this way an analogy exists between "states" in a SPA and "pages" in a traditional website. Because "state navigation" in the same page is analogous to page navigation, in theory, any page-based web site could be converted to single-page replacing in the same page only the changed parts.
The SPA approach on the web is similar to the single-document interface (SDI) presentation technique popular in native desktop applications.
-
-
-
IoT / Internet Of Things / 
Nesnelerin İnterneti     	(GITHUB) (test)
internet of things nedir, Internet of things nedir, nesnelerin interneti nedir, iot nedir, IoT nedir, iot nedir
Birbiriyle konuşan makinelerin sistemi diyebiliriz… İnsansız…
Amazon videosu, güzel açıklıyor: https://aws.amazon.com/tr/iot/?nc2=h_ql_prod_it
The Internet of things (IoT) is a system of interrelated computing devices, mechanical and digital machines provided with unique identifiers (UIDs) and the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction. 
The definition of the Internet of things has evolved due to the convergence of multiple technologies, real-time analytics, machine learning, commodity sensors, and embedded systems.[1] Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), and others all contribute to enabling the Internet of things. In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the "smart home", including devices and appliances (such as lighting fixtures, thermostats, home security systems and cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers.
There are a number of serious concerns about dangers in the growth of IoT, especially in the areas of privacy and security, and consequently industry and governmental moves to address these concerns have begun.
-
-
videoyu izle: https://www.youtube.com/watch?v=rrT6v5sOwJg

Material Design   (GITHUB)  (test) 
Material Design nedir
Material Design (codenamed Quantum Paper)[1] is a design language that Google developed in 2014. Expanding on the "card" motifs that debuted in Google Now, Material Design uses more grid-based layouts, responsive animations and transitions, padding, and depth effects such as lighting and shadows.
Google announced Material Design on June 25, 2014, at the 2014 Google I/O conference.
Overview
Designer Matías Duarte explained that, "unlike real paper, our digital material can expand and reform intelligently. Material has physical surfaces and edges. Seams and shadows provide meaning about what you can touch." Google states that their new design language is based on paper and ink but implementation takes place in an advanced manner.
Material Design will gradually be extended throughout Google's array of web and mobile products, providing a consistent experience across all platforms and applications. Google has also released application programming interfaces (APIs) for third-party developers to incorporate the design language into their applications.[5][6][7] The main purpose of material design is creation of new visual language that combines principles of good design with technical and scientific innovation.
In 2018, Google detailed a revamp of the language, with a focus on providing more flexibility for designers to create custom "themes" with varying geometry, colors, and typography. Google released Material Theme Editor exclusively for the macOS design application Sketch.[8][9]
Implementation
As of 2020, most of Google's mobile applications for Android as well as its web app counterparts had applied the new design language, including Gmail, YouTube, Google Drive, Google Docs, Google Sheets, Google Slides, Google Photos, Google Maps, Google Classroom, Google Translate, Google Chrome, Google Keep, Google Play, and most other Google products. It is also the primary design language of Android and Chrome OS.
In 2018, with the introduction of the ability to create custom themes, Google also began redesigning most of their apps into a customized and adapted version of Material Design called the Google Material Theme[10], also dubbed "Material Design 2"[11], which heavily emphasized white space, rounded corners, colorful icons, bottom navigation bars, and utilized a special size-condensed version of Google's proprietary Product Sans font called Google Sans.[12] As of 2020, most Google applications have also applied the new Google Material Theme design, with the notable exception of YouTube.
The canonical implementation of Material Design for web application user interfaces is called Polymer.[13] It consists of the Polymer library, a shim that provides a Web Components API for browsers that do not implement the standard natively, and an elements catalog, including the "paper elements collection" that features visual elements of the Material Design.[14]
-
-
-

- 
Cross-platform software  / Multi-platform software  (GITHUB)  (test)
Cross-platform nedir, cross platform nedir, multi platform nedir, multi-platform nedir
For example, a cross-platform application may run on Microsoft Windows, Linux, and macOS. Cross-platform programs may run on as many as all existing platforms, or on as few as two platforms. Cross-platform frameworks (such as Qt, Flutter, NativeScript, Xamarin, Phonegap, Ionic, and React Native) exist to aid cross-platform development.
Platform can refer to the type of processor (CPU) or other hardware on which a given operating system or application runs, the type of operating system on a computer or the combination of the type of hardware and the type of operating system running on it.[4] An example of a common platform is Microsoft Windows running on the x86 architecture. Other well-known desktop computer platforms include Linux/Unix and macOS - both of which are themselves cross-platform.
-
-
- 
-






