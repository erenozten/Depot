




WWW     	(GITHUB)  (empty) 
[www nedir, world wide web nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
World Wide Web, Dünya Çapında Ağ (kısaca WWW veya Web), İnternet üzerinde yayınlanan birbirleriyle bağlantılı hiper-metin dokümanlarından oluşan bir bilgi sistemidir. Bu dokümanların her birine Web sayfası adı verilir ve Web sayfalarına İnternet kullanıcısının bilgisayarında çalışan Web tarayıcısı adı verilen bilgisayar programları aracılığıyla erişilir. Web sayfalarında metin, imaj, video ve diğer multimedya ögeleri bulunabilir ve diğer bağlantı ya da link adı verilen hiper-bağlantılar ile başka Web sayfalarına geçiş yapılabilir.

İnternet ve Web terimleri aynı olguyu tanımlamaz. Zira Web sadece İnternet üzerinde çalışan bir servistir. Web kavramı, CERN'de bir bilgisayar programcısı olan Tim Berners-Lee'nin HTML adlı metin işaretleme dilini geliştirmesiyle oluşmuştur. Bugün de kendisinin başkanı olduğu W3C (World Wide Web Consortium) tarafından standartları belirlenmektedir.

Yapısı
Web’in temeli İnternet'tir. Web İnternet üzerinde kurulmuştur ve İnternet'in sunduğu mekanizmalardan çoğunun kullanılmasını sağlar. İnternet'in fiziksel görünüşleri –bilgisayarlar, ağlar ve servisler– Dünya üzerindeki diğer binlerce bilgisayara bağlanmamıza izin verir. Web, İnternet'in en tepesindeki soyutlanmış genel servisler kümesidir. World Wide Web (W3), insanların fikir ve projelerinin paylaşılmasını sağlayan bir bilgi ve kültür havuzudur. İstemci-sunucu uygulamaları ile yapılan birçok organizasyon üzerinde Web tarayıcıları istemci olarak çalışabilirler. Web yürütümü standart İstemci-sunucu modelini izler. Aşağıdaki şekilde gösterildiği gibi "Web tarayıcısı" adı verilen programı çalıştıran bir istemci bilgisayar ile Web sunucu yazılımı çalıştıran bir sunucu bilgisayar arasındaki etkileşime "istemci-sunucu" etkileşimi adı verilir. İstemci bilgisayar sunucudan HTTP'yi (Hypertext Transfer Protocol) ve İnternet mesaj standardı TCP/IP'yi kullanarak bir doküman ister ve sunucu istemcinin göstereceği dokümanı geri döndürür.-
-
WebPack             (GITHUB)  (test) 
Webpack nedir
Webpack is an open-source JavaScript module bundler. It is made primarily for JavaScript, but it can transform front-end assets such as HTML, CSS, and images if the corresponding loaders are included. webpack takes modules with dependencies and generates static assets representing those modules.
Webpack takes the dependencies and generates a dependency graph allowing web developers to use a modular approach for their web application development purposes. It can be used from the command line, or can be configured using a config file which is named webpack.config.js. This file is used to define rules, plugins, etc., for a project. (webpack is highly extensible via rules which allow developers to write custom tasks that they want to perform when bundling files together.)
Node.js is required for using webpack.
webpack provides code on demand using the moniker code splitting. The Technical Committee 39 for ECMAScript is working on standardization of a function that loads additional code: "proposal-dynamic-import".
-
-
-
Webpack is a tool that has got a lot of attention in the last few years, and it is now seen used in almost every project. Learn about it.

Using webpack allows you to use import or require statements in your JavaScript code to not just include other JavaScript, but any kind of file, for example CSS.
Webpack aims to handle all our dependencies, not just JavaScript, and loaders are one way to do that.
For example, in your code you can use:
import 'style.css'
-
What is webpack?
Webpack is a tool that lets you compile JavaScript modules, also known as module bundler.
Given a large number of files, it generates a single file (or a few files) that run your app.
It can perform many operations:
•	helps you bundle your resources.
•	watches for changes and re-runs the tasks.
•	can run Babel transpilation to ES5, allowing you to use the latest JavaScript features without worrying about browser support.
•	can transpile CoffeeScript to JavaScript
•	can convert inline images to data URIs.
•	allows you to use require() for CSS files.
•	can run a development webserver.
•	can handle hot module replacement.
•	can split the output files into multiple files, to avoid having a huge js file to load in the first page hit.
•	can perform tree shaking.
Webpack is not limited to be use on the frontend, it’s also useful in backend Node.js development as well.
Predecessors of webpack, and still widely used tools, include:
•	Grunt
•	Broccoli
•	Gulp
There are lots of similarities in what those and Webpack can do, but the main difference is that those are known as task runners, while webpack was born as a module bundler.
It’s a more focused tool: you specify an entry point to your app (it could even be an HTML file with script tags) and webpack analyzes the files and bundles all you need to run the app in a single JavaScript output file (or in more files if you use code splitting).
Installing webpack
Webpack can be installed globally or locally for each project.
Global install
Here’s how to install it globally with Yarn:
yarn global add webpack webpack-cli
with npm:
npm i -g webpack webpack-cli
once this is done, you should be able to run
webpack-cli
 
Local install
Webpack can be installed locally as well. It’s the recommended setup, because webpack can be updated per-project, and you have less resistance to using the latest features just for a small project rather than updating all the projects you have that use webpack.
With Yarn:
yarn add webpack webpack-cli -D
with npm:
npm i webpack webpack-cli --save-dev
Once this is done, add this to your package.json file:
{
  //...
  "scripts": {
    "build": "webpack"
  }
}
once this is done, you can run webpack by typing
yarn build
in the project root.
Webpack configuration
By default, webpack (starting from version 4) does not require any config if you respect these conventions:
•	the entry point of your app is ./src/index.js
•	the output is put in ./dist/main.js.
•	Webpack works in production mode
You can customize every little bit of webpack of course, when you need. The webpack configuration is stored in the webpack.config.js file, in the project root folder.
The entry point
By default the entry point is ./src/index.js This simple example uses the ./index.js file as a starting point:
module.exports = {
  /*...*/
  entry: './index.js'
  /*...*/
}
The output
By default the output is generated in ./dist/main.js. This example puts the output bundle into app.js:
module.exports = {
  /*...*/
  output: {
    path: path.resolve(__dirname, 'dist'),
    filename: 'app.js'
  }
  /*...*/
}
Loaders
Using webpack allows you to use import or require statements in your JavaScript code to not just include other JavaScript, but any kind of file, for example CSS.
Webpack aims to handle all our dependencies, not just JavaScript, and loaders are one way to do that.
For example, in your code you can use:
import 'style.css'
by using this loader configuration:
module.exports = {
  /*...*/
  module: {
    rules: [
      { test: /\.css$/, use: 'css-loader' },
    ]
  }
  /*...*/
}
The regular expression targets any CSS file.
A loader can have options:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.css$/,
        use: [
          {
            loader: 'css-loader',
            options: {
              modules: true
            }
          }
        ]
      }
    ]
  }
  /*...*/
}
You can require multiple loaders for each rule:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.css$/,
        use:
          [
            'style-loader',
            'css-loader',
          ]
      }
    ]
  }
  /*...*/
}
In this example, css-loader interprets the import 'style.css' directive in the CSS. style-loader is then responsible for injecting that CSS in the DOM, using a <style> tag.
The order matters, and it’s reversed (the last is executed first).
What kind of loaders are there? Many! You can find the full list here.
A commonly used loader is Babel, which is used to transpile modern JavaScript to ES5 code:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /(node_modules|bower_components)/,
        use: {
          loader: 'babel-loader',
          options: {
            presets: ['@babel/preset-env']
          }
        }
      }
    ]
  }
  /*...*/
}
This example makes Babel preprocess all our React/JSX files:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.(js|jsx)$/,
        exclude: /node_modules/,
        use: 'babel-loader'
      }
    ]
  },
  resolve: {
    extensions: [
      '.js',
      '.jsx'
    ]
  }
  /*...*/
}
See the babel-loader options here.
Plugins
Plugins are like loaders, but on steroids. They can do things that loaders can’t do, and they are the main building block of webpack.
Take this example:
module.exports = {
  /*...*/
  plugins: [
    new HTMLWebpackPlugin()
  ]
  /*...*/
}
The HTMLWebpackPlugin plugin has the job of automatically creating an HTML file, adding the output JS bundle path, so the JavaScript is ready to be served.
There are lots of plugins available.
One useful plugin, CleanWebpackPlugin, can be used to clear the dist/ folder before creating any output, so you don’t leave files around when you change the name of the output file:
module.exports = {
  /*...*/
  plugins: [
    new CleanWebpackPlugin(['dist']),
  ]
  /*...*/
}
The webpack mode
This mode (introduced in webpack 4) sets the environment on which webpack works. It can be set to development or production (defaults to production, so you only set it when moving to development)
module.exports = {
  entry: './index.js',
  mode: 'development',
  output: {
    path: path.resolve(__dirname, 'dist'),
    filename: 'app.js'
  }
}
Development mode:
•	builds very fast
•	is less optimized than production
•	does not remove comments
•	provides more detailed error messages and suggestions
•	provides a better debugging experience
Production mode is slower to build, since it needs to generate a more optimized bundle. The resulting JavaScript file is smaller in size, as it removes many things that are not needed in production.
I made a sample app that just prints a console.log statement.
Here’s the production bundle:
 
Here’s the development bundle:
 
Running webpack
Webpack can be run from the command line manually if installed globally, but generally you write a script inside the package.json file, which is then run using npm or yarn.
For example this package.json scripts definition we used before:
"scripts": {
  "build": "webpack"
}
allows us to run webpack by running
npm run build
or
yarn run build
or
yarn build
Watching changes
Webpack can automatically rebuild the bundle when a change in your app happens, and keep listening for the next change.
Just add this script:
"scripts": {
  "watch": "webpack --watch"
}
and run
npm run watch
or
yarn run watch
or
yarn watch
One nice feature of the watch mode is that the bundle is only changed if the build has no errors. If there are errors, watch will keep listening for changes, and try to rebuild the bundle, but the current, working bundle is not affected by those problematic builds.
Handling images
Webpack allows us to use images in a very convenient way, using the file-loader loader.
This simple configuration:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.(png|svg|jpg|gif)$/,
        use: [
          'file-loader'
        ]
      }
    ]
  }
  /*...*/
}
Allows you to import images in your JavaScript:
import Icon from './icon.png'

const img = new Image()
img.src = Icon
element.appendChild(img)
(img is an HTMLImageElement. Check the Image docs)
file-loader can handle other asset types as well, like fonts, CSV files, xml, and more.
Another nice tool to work with images is the url-loader loader.
This example loads any PNG file smaller than 8KB as a data URL.
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.png$/,
        use: [
          {
            loader: 'url-loader',
            options: {
              limit: 8192
            }
          }
        ]
      }
    ]
  }
  /*...*/
}
Process your SASS code and transform it to CSS
Using sass-loader, css-loader and style-loader:
module.exports = {
  /*...*/
  module: {
    rules: [
      {
        test: /\.scss$/,
        use: [
          'style-loader',
          'css-loader',
          'sass-loader'
        ]
      }
    ]
  }
  /*...*/
}
Generate Source Maps
Since webpack bundles the code, Source Maps are mandatory to get a reference to the original file that raised an error, for example.
You tell webpack to generate source maps using the devtool property of the configuration:
module.exports = {
  /*...*/
  devtool: 'inline-source-map',
  /*...*/
}
devtool has many possible values, the most used probably are:
•	none: adds no source maps
•	source-map: ideal for production, provides a separate source map that can be minimized, and adds a reference into the bundle, so development tools know that the source map is available. Of course you should configure the server to avoid shipping this, and just use it for debugging purposes
•	inline-source-map: ideal for development, inlines the source map as a Data URL
-
-
-
SonarQube (formerly Sonar)[2] is an open-source platform developed by SonarSource for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security vulnerabilities.[3][4]
SonarQube can record metrics history and provides evolution graphs. SonarQube provides fully automated analysis and integration with Maven, Ant, Gradle, MSBuild and continuous integration tools (Atlassian Bamboo, Jenkins, Hudson, etc.).
 
Bir önceki yazımda ( Yazılım Ekiplerinde Kod Kalitesi ve Kalitede Sürekliliğin Sağlanması ) Continuous Code Quality kavramından, DevOps kültüründeki, ve bir CI pipleline’ındaki yerinden bahsetmiştim. Özellikle büyük veya büyüme kapasitesi olan ekiplerde her commit’in ana branch’e merge edilmeden önce mutlaka code-review işlemine tabi tutulması gerektiğini ve bu işlemden önce bir statik kod analiz aracı kullanımının, kod kalitesi, hatasızlık ve zaman tasarrufu açısından faydalı olacağından bahsetmiştim. Bu yazımda SonarQube statik kod analiz aracından bahsedeceğim.
 
SonarQube (eski ismiyle Sonar), Sonarsource firmasının iki ana ürününden birisidir. Diğer ürün SonarLint’de yine bir code quality tool’udur, ancak SonarQube’den farklı olarak yazılımın geliştirme aşamasında, yani henüz kodumuzu commit’lemeden önce bizi uyarır. Bir çok popüler IDE için desteği mevcuttur. “Fix issues before they exist” diye de bir sloganları var. Bu yazıda SonarLint’den bahsetmeyeceğim. Merak edenleriniz buradan inceleyebilirler.
SonarQube’ü açık kaynak projeleriniz için cloud üzerinden ücretsiz olarak kullanabileceğiniz gibi, private projeler için on-premise kurulum yaparak da kullanabilirsiniz. Cloud üzerinden private projeleriniz için ücret ödemeniz gerekiyor.
Amacım SonarQube kullanımı hakkında bilgi vermek olduğu için, kurulum kısmını atlayıp, konuyu cloud üzerinden yani https://sonarcloud.io uygulaması üzerinden anlatacağım. SonarQube’ü aşağıda listelediğim 6 ana başlıkta inceleyeceğiz;
•	https://sonarcloud.io uygulamasına giriş ve boş bir proje oluşturma
•	Kişisel bilgisayarımıza SonarScanner kurulumu
•	SonarScanner ile örnek bir .Net projesinin analiz edilmesi ve raporun sonarcloud.io ya gönderilmesi
•	sonarcloud.io ya gönderilen raporun yorumlanması
•	Quality Gates, Quality Profiles ve Rules kavramları hakkında
•	SonarQube’ün bir CI pipeline’ına dahil edilmesi
sonarcloud.io SaaS Uygulamasına Giriş
SonarQube’ü açık kaynak projeler için cloud üzerinden ücretsiz olarak kullanabiliyoruz. Giriş ekranında sizi 3 OAuth seçeneği karşılıyor. Ben github hesabımla giriş yaptım.
 
Ekran 1: OAuth login seçenekleri
Sağ üst köşedeki menüden “Create new organization” diyerek bir organizasyon oluşturarak başlıyoruz. SonarQube de her proje bir organizasyona bağlıdır. Daha sonra yine sağ üst köşeden “Analyze new project” diyoruz ve aşağıdaki ekranla karşılaşıyoruz.
 
 
Ekran 2: Mevcut organizasyona proje ekleme
Önceki adımda oluşturduğumuz organizasyonumuz seçili geliyor. Devam diyoruz ve sonraki adımda bir token oluşturuyoruz. Oluşan token değeri lazım olacak, hemen bir yere kopyalayıp devam ediyoruz ve aşağıdaki ekrandan projemizin ana programlama dilini seçiyoruz ve projemiz için bir anahtar kelime belirliyoruz.( Projede birden fazla dil olabileceğinden ötürü (c#, html, js) ana dil ifadesi kullanılmış )
 
 
Ekran 3: Projenin ana programlama dili seçimi
Done butonuna tıkladıktan sonra aşağıdaki ekranla karşılaşacaksınız. Bu ekranda gördüğünüz üzere Scanner’ı çalıştırmak ve projenizi analiz edebilmek için komut satırından çalıştırmanız gereken 3 adet komut yer almakta. Bu 3 komutu bir yere kopyalayıp ekranı kapatabilirsiniz.
 
 
Ekran 4: Proje ekleme son adım
SonarScanner Kurulumu
Yukarıdaki ekran görüntüsünde dikkat ederseniz ”Download and unzip the Scanner for MSBuild” ifadesi yer alıyor. Peki neden MSBuild Scanner? Dil olarak C# seçtiğimiz için SonarQube ilgili Scanner hangisiyse onu indirmemiz gerektiğini söylüyor. Download butonu ile yönlendirileceğiniz ekrandaki talimatları yerine getirerek MSBuild Scanner’ını bilgisayarınıza kurmuş olacaksınız. (%PATH% güncellemesini unutmayınız) Bu arada hangi tür Scanner kurarsanız kurun sisteminizde mutlaka java’nın kurulu olması gerektiğini belirteyim. JRE hakkındaki detayları da yine download butonuyla yönlendirileceğiniz sayfadan görebilirsiniz.
MSBuild SonarScanner İle Projenin Kod Analizi
Önceki adımda kurduğumuz Scanner’ı kullanarak C# dili ile yazılmış olan projemiz için static kod analizini yapabiliriz artık. Bunun için projenin ana dizinindeyken sırasıyla aşağıdaki 3 komutu çalıştırmamız yeterli. Son komuttan sonra 5 nolu ekran görüntüsüyle karşılaşmanız raporun oluştuğu ve sonarcloud’a gönderildiği anlamına gelir. (rabbitmq sonarcloud da projemi oluştururken verdiğim key’in ismi)
 
Ekran 5: Kod analizinin başarıyla yapılması
SonarScanner.MSBuild.exe begin /k:"<proje unique key buraya>" /d:sonar.organization="<organizasyon_adi_buraya>" /d:sonar.host.url="https://sonarcloud.io" /d:sonar.login="<olusturulan_token_adi_buraya>"MsBuild.exe /t:RebuildSonarScanner.MSBuild.exe end /d:sonar.login="<olusturulan_token_degeri_buraya>"
Bu 3 komutta koyulaştırdığım kısımlar sizin proje ve organizasyon isminize göre değişecektir. 4 nolu ekran görüntüsünde yer alan bu 3 komutu bir yere kopyaladıysanız, hızlıca komut satırından çalıştırabilirsiniz. Komut satırında “Post-processing succeded” ifadesini gördükten sonra artık sonarcloud’a geçiş yaparak projenizin analiz edildiğini görebilmelisiniz.
 
 
Ekran 6: “Passed” :)
106 sayısı kod satır sayısını ifade etmekte. Hemen altında da C# ibaresi, programlama dilini işaret ediyor. Evet, çok küçük ve XS (X Small) kategorisinde bir proje olmasına rağmen yeşil renkli “Passed” ifadesini görmek güzel hissettiriyor :) Buradan inceleyebilirsiniz.
Oluşan Raporun Yorumlanması
Raporu yorumlamadan önce hemen belirteyim, örnek proje zamanında RabbitMQ yü kurcalamak için oluşturduğum bir Asp.Net MVC uygulaması.
Proje public bir proje olduğu için sonarcloud hesabınız olmasa bile, yani sonarcloud a giriş yapmadan da oluşan son raporu buradan görebilirsiniz.
Linke tıkladıysanız aşağıdaki ekranla karşılaşmış olmanız gerekiyor.
 
Ekran 7: rabbitmq key’i ile oluşturduğum projemin sonarcloud dashboard’u
Dikkat ederseniz rapor 5 ana başlıktan oluşuyor. Bunlar;
Bugs
Projenizde bu kategoriye alınan kodlarınız varsa bir şeylerin yanlış veya eksik olduğundan emin olabilirsiniz. Eğer düzeltilmezse ileride başınız ağrıyabilir, dolayısıyla bu kategoriyi önemseyin. Bizim 109 satırlık küçük uygulamamızda Bug olarak nitelendirilecek bir şey bulamamış sevgili SonarQube. A alarak geçmişiz bu dersten :) Eğer ne tür durumların bug olarak ele alındığını merak ediyorsanız buraya tıklayarak public projeleri keşfedebilir, raporları dilediğinizce inceleyerek bir çok şey öğrenebilirsiniz.
Vulnerabilities
Güvenlik zafiyetine sebebiyet verecek olan kod parçacıkları bu kategoride raporlanmaktadır. Yani bu kategoride en az Bug kategorisi kadar önemli. Göz ardı etmemekte fayda var, bir örnek olması açısından rastgele public bir projeden aldığım aşağıda ekran görüntüsünü paylaşmak istedim.
 
 
Ekran 8: SonarQube’ün zafiyet olarak gördüğü alert kullanımı
Burada bir js dosyası içerisinde kullanılan alert metodu zafiyet olarak görülmüş. Üç nokta (…) icon’una tıklayarak açılan aşağıdaki bilgilendirme ekranında SonarQube’ün gerekçesini de görebilmeniz mümkün. Özetle, debug modda alert’e tamam ama production da hassas veri gösterme riski taşıdığından burada bir zafiyet var arkadaş, diyor SonarQube.
 
 
Ekran 9: SonarQube: Production ortamında unutulan alert can yakabilir
Code Smells
İlk 2 maddemiz kadar risk teşkil etmeyen ancak, kod okunabilirliği ve bakım maliyetleri açısından negatif etki yapabilecek kod parçacıkları bu kategoride değerlendirilmekte. Raporumuzda 3 adet code smells olduğu görülüyor. Hemen yanında yazan 14 min ifadesi ise, bu 3 adet sorunun tahmini çözüm süresi :) Evet SonarQube sağ olsun onu da hesaplıyor ama tabi bu değerde bir hata payı olabileceğini söylememe gerek yok sanırım. Şimdi bu 3 rakamına tıklayarak aşağıdaki detay ekranına erişiyoruz.
 
 
Ekran 10: Code Smell liste ekranı
Listede koyu renkle yazılı olan özet bilgisi aslında gerekli detayı veriyor. Örneğin ilk maddede sadece constructor içerisinden değer atanan _hostName adındaki field’ın readonly olarak işaretlenmesi gerektiğinden bahsediyor. Eğer bu maddenin üzerine tıklarsanız aşağıdaki detay ekranında ilgili kod dosyasının ilgili satırını görebiliyorsunuz.
 
 
Ekran 11: Code Smell detayı
Burada yine Make _hostName readonly ifadesinin yanındaki üç nokta (…) ya tıklarsanız ekranın en altında o maddeyle alakalı bilgilendirici açıklamayı görebilirsiniz. SonarQube sadece açıklarınız bulmuyor aynı zamanda eğitiyor da :) Gördüğünüz gibi içerikte örnek kod bile var.

Coverage
Buradan yazılan testlerin projenizin ne kadarını cover ettiğiniz görebilirsiniz. Bu küçük projede test yazılmadığı için değerimiz %0 ve kırmızı renk bir uyarı manasında, yani unit test yazılmadığını ifade ediyor.
Duplications
Proje genelinde tekrarlı kod satırlarının toplam kod satırına oranını ifade eder. Örneğin 1000 satır kod içerisinde toplamda 100 satır eğer tekrarlı ise bu değer %10 olacaktır. Örnek projemiz az sayıda kod içerdiğinden ötürü hiç tekrarlı kod yoktur. Ben orta ve büyük ölçekli projelerde bu değerin %40- 50 'lerde olabildiğini görmüştüm ki çok ciddi bir oran. SonarQube detaylı olarak aşağıdaki ekran görüntüsünde gördüğünüz gibi dosya bazında tekrarlı kod oranlarını başarılı şekilde raporlayabiliyor.
 
 
Ekran 13: Kod dosyası bazında rapor detayları
Quality Gates, Quality Profiles ve Rules Kavramları Hakkında
Quality Gate’ler projemizin production ortamı için hazır olup olmadığına karar veren kural dizileridir. Bir başka değişle, kod analiz raporunun başarılı mı (Passed), başarısız mı (Failed) olacağının belirlenmesi için kullanılırlar. 6 nolu ekran görüntüsünde yer alan yeşil renkli “Passed” ifadesi, kod analiz işlemi esnasında mevcut Quality Gate’lerin hepsinden geçildiği ve rapor sonucunun başarılı olduğu anlamına geliyor.
SonarQube desteklediği her programlama dili için default (ön tanımlı) olarak, Quality Profile, Rules ve Quality Gate tanımlamalarına sahip. Örneğin C# dili için buraya tıklayarak ulaşabileceğiniz, benim oluşturduğum public organizasyonun aşağıdaki ekranında ön tanımlı olarak gelen Quality Gate’leri görebilirsiniz.
Örneğin; en altta belirtilen kuraldan, Security notunun A’dan daha kötü olması raporun başarısız sonuçlanacağını anlamalıyız. Ve en önemlisi, sol üst köşedeki Create butonuyla, kendi custom Quality Gate’inizi oluşturabilirsiniz. Örneğin siz projenizde Security notunun B den daha kötü olması durumunda raporun başarısız olmasını sağlayabilirsiniz.
 
Ekran 14: Ön tanımlı Quality Gate’ler
Peki Security notu nasıl belirleniyor? Burada Rules kavramına değinmemiz gerekiyor. Yukarıdaki ekran görüntüsünde Rules sekmesine tıkladığınızda aşağıdaki ekrana erişeceksiniz. Sol taraftaki filtreleme özelliğini kullanarak yalnızca C# için tanımlı olan kuralları listeledim. Kurallara tıklayarak örnek kod parçacıklarının da yer verildiği bilgilendirici detay sayfalarına göz atabilirsiniz. Dikkat ederseniz C# dili için kural havuzunda toplam 358 adet (21.08.2018 tarihli kural sayısı ) kural tanımlı. Bu kurallar C# ve diğer diller için sürekli olarak artmakta, eklenen yeni kurallarla SonarQube geliştirilmeye devam etmektedir.
 
 
Ekran 15: C# dili için tanılı kurallar listesi
Quality Profile içinse en basit haliyle, çeşitli kuralları içeren gruplardır diyebiliriz. Yani C# dili için kendinize özel oluşturacağınız bir Quality Profile, en az 1 en fazla 358 adet kuraldan oluşabilir.
Özetle SonarQube’ün ön tanımlı Quality Gate ve Quality Profile ını kullanmayıp, kendi tanımlamalarınızı yapabilirsiniz. Üstelik bunları proje bazlı da yapabilirsiniz. Aşağıdaki ekrandan rabbitmq projemiz için Administrator sekmesinde tıkladığımızda açılan menüden bu değişikliği yapabiliriz.
 
 
Ekran 16: Proje bazlı admin ekranları için menü
Bu menüden Quality Gate sekmesinde tıkladığımızda gelen ekrandan, projemiz için istediğimiz Quality Gate’i seçebiliriz. Buradaki sample gate isimli Quality Gate benim örnek olması için yaptığım tanımlama. Dikkat ederseniz Sonar way için, default ibaresi var yani tüm projeler ilk oluşturulduklarında ön tanımlı olarak Sonar way Quality Gate’ine tabi tutulmaktalar.
 
 
Ekran 17: Projenin Quality Gate’inin değiştirilmesi
Aynı şekilde Quality Profiles linkine tıklayarak SonarQube’ün projenize atadığı ön tanımlı profili kullanmayıp kendi tanımladığınız profilin kullanılmasını sağlayabilirsiniz. Ön tanımlı gelen Sonar way isimli Quality Profile tüm kuralları içerirken siz kendi profiliniz için kapsamı daraltmak isteyebilirsiniz mesela.
Örnek vermek gerekirse; kendi özel tanımlayacağınız Quality Profile’ınızdan projemizde 2 adet Code Smell olarak değerlendirilen, readonly field kuralını çıkarırsanız, Code Smell sayımız bire inecektir. Bu yolla siz, “readonly field kuralı benim için code smell değildir”, demiş olursunuz aslında.
SonarQube’ü CI Pipeline’ınıza Dahil Edin
Eğer hali hazırda bir CI yazılımı kullanmaktaysanız, SonarQube veya benzeri bir statik kod analiz aracını CI pipeline’ınıza dahil edebilirsiniz. Bu noktada kullandığınız CI yazılımına göre eforunuz değişecektir. Jenkins, TeamCtiy, Travis CI vs. gibi bazı CI araçları SonarQube için sahip oldukları plugin’ler sayesinde bu eforu minimuma indirmekteler. Diğer bir ifadeyle SonarQube için doğal desteğe sahipler.
Doğal entegrasyona sahip CI araçlarında SonarQube rapor sonucuna göre pipeline’ınızı kırıp, ilgili yerlere bildirim yapabilirsiniz. Örneğin build adımından sonra SonarQube’ü tetikler, raporu sonucunu bekletir ve rapor başarısız olursa bir sonraki adıma geçmeyebilirsiniz.
Plugin ile doğal entegrasyon imkanı olmayan CI araçlarında SonarQube’ü tetiklemek için ise, yml scriptinize MSBuild SonarScanner İle Projenin Kod Analizi bölümünde belirttiğimiz komutları ekleyerek ilerleyebilirsiniz. Bu noktada rapor sonucunu bekleyerek, gelen sonucuna göre bir aksiyon almak mümkün müdür açıkçası bilmiyorum. Yorumlarınızı alabilirim.

-
-
-
UNIX		(GITHUB)
Unix nedir
Unix (/ˈjuːnɪks/; trademarked as UNIX) is a family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix, development starting in the 1970s at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[3]
-Unix vs linux yazılar var: https://chandigarhinfo.in/unix-vs-linux-whats-the-difference/
-
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
Linux 		(GITHUB)
Linux nedir

The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Bilgisayar işletim sistemlerinin en temel parçası olan çekirdek yazılımlarından bir tanesidir.verilmiştir.[1]Günümüzde süper bilgisayarlarda, akıllı cihazların ve internet altyapısında kullanılan cihazların işletim sistemlerinde yaygın olarak kullanılmaktadır. Bunlardan en popüler olanı Google tarafından geliştirilen Android işletim sistemidir.
Android, Linux çekirdeği üzerine inşa edilmiş bir mobil işletim sistemidir.
Android işletim sistemi beş kısımdan oluşur.
1.	Çekirdek: Linux kernelidir. Güvenlik, hafıza yönetimi, süreç yönetimi, ağ yığınları ve sürücü modellerini içermektedir.
2.	Android Runtime: Sanal makinedir. Dalvik Sanal Makinesini de içermektedir. 5.0 ile Dalvik kaldırılmış ve ART'ye geçilmiştir.
3.	Kütüphaneler: Veritabanı kütüphaneleri, web tarayıcı kütüphaneleri, grafik ve arayüz kütüphanelerini içermektedir.
4.	Uygulama Çatısı: Uygulama geliştiricilere geniş bir platform sunan kısımdır.
5.	Uygulama Katmanı: Doğrudan Java (programlama dili) ile geliştirilmiş uygulamaları içermektedir.
-
-
-
-
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
UNIX		(bosch)	 (bosch)
unix nedir
UNIX türevi işletim sistemleri çok işlemcili çok pahalı makinalardan, tek işlemcili basit ve çok ucuz ev bilgisayarlarına kadar pek çok cihaz üzerinde çalışabilen esnek ve sağlamlığı çok değişik koşullarda test edilmiş sistemlerdir. Fakat özellikle kararlı yapısı ve çok kullanıcılı-çok görevli yapısıyla çok işlemcili sunucularda adeta standard haline gelmiştir ve özellikle akademik dünyada iş istasyonları üzerinde çok yaygın bir kullanım alanı bulmuştur. UNIX, Interdata 7/32, VAX, ve Motorola 68000 arasında hızla yayıldı.
Unix işletim sistemi 1969 yılında AT&T Bell Laboratuvarları'nda ABD de Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından tasarlanıp uygulamaya konmuştur.İlk olarak 1971'de yayınlandı ve başlangıçta tamamen bilgisayar programlarının yazılmasında kullanılan alt seviyeli bir çevirme dilinde yazılmıştı. Daha sonra 1973'te Dennis Ritche tarafından C programlama dili ile tekrar yazıldı. Üst düzey bir dilde yazılmış bir işletim sisteminin geçerliliği diğer farklı bilgisayar platformlarına kolayca taşınabilirlik için olanak sağlar. Lisans için AT&T'yi zorlayan yasal bir aksaklık nedeniyle, UNIX hızlıca büyüdü ve öğretim kurumları ve işletmeler tarafından kabul edilir oldu.
UNIX, 1969 yılında,Ken Thompson, Dennis Ritchie, Brian Kernighan, Douglas McIlroy, Michael Lesk ve Joe Ossanna tarafından Bell Laboratuvarları'nda geliştirilmiş, çok kullanıcılı (multiuser), çok görevli yapıyı destekleyen (multitasking) bir bilgisayar işletim sistemidir. Komut yorumlayıcı yazılımlar (shell) aracılığı ile kullanıcı ve bilgisayar sisteminin iletişimi sağlanır.
Linus Torvalds tarafından temelleri atılan Linux, UNIX olmayıp bir UNIX türevidir. UNIX'ten ilham alan, bir grup bağımsız yazılımcı tarafından geliştirilen bir işletim sistemi çekirdeğidir. 
-
-
-
-
-
UNIX vs. LINUX		(bosch)	 (bosch)
unix nedir, linux nedir, UNIX nedir
The UNIX is a multiuser computer operating systems was born in the late 1960s. AT & T Bell Labs released an OS called Unix written in C, which allows for faster modification, portability and acceptance. It began as a one-man project led by Ken Thompson of Bell Labs. It became the most widely used operating system. Unix is a proprietary operating system.
Unix OS works on CLI (Command Line Interface), but recently it has been developed for GUI on Unix systems. Unix is an operating system that is popular in companies, universities, large companies, etc
What is LINUX? 
Linux is an operating system built by Linus Torvalds at the University of Helsinki in 1991. The name “Linux” comes from the Linux kernel. It is the software on a computer that allows applications and users to access the devices on the computer to perform certain specific functions.
Linux OS forwards instructions from an application from the computer processor and sends the results back to the application via Linux OS. It can be installed on another type of computers, mobile phones, tablets, video game consoles, etc.
The development of Linux is one of the most prominent examples of collaboration with free and open source. Today, many companies and similar individuals have released their own version of the OS based on the Linux kernel.
 
 
 
Features of Unix OS
	Multitasking and Multi-user operating system
	It can be used as a master control program in servers and  workstations.
	Hundreds of commercial applications available
	In its day, UNIX was rapidly adopted and became the standard OS in universities.
if you are learn more about linux then Join CBitss Technologies. CBitss Provides Best Linux Training in Chandigarh Sector 34A. More details Call Now –  (+91) 9988741983
Features of Linux Operating System
	Support multitasking
	Programs consist of one or more processes, and each process has one or more threads
	It can easily co-exists along with other Operating systems.
	It can run multiple user programs
	Individual accounts are protected because of appropriate authorization
	Linux is a copy of UNIX but does not use its code
Difference between Unix and Linux
Cost 
	 Linux is freely distributed, downloaded via newspapers, books, websites, etc. There are also paid versions available for Linux.
	Unix – Different flavors of Unix have different prices depending on the supplier type
Development
	Linux is Open Source and thousands of programmers collaborate online and contribute to its development
	Unix systems have different versions. These versions are mainly developed by AT&T and other commercial suppliers
User
	Linux – All. From home users to both developers and computer enthusiasts.
	UNIX can be used on Internet servers, workstations, and computers
Text made interface
	BASH is the Linux shell. It offers support for several command interpreters
	Unix Originally build up to work in Bourne Shell. But it is now compatible with much other software.
GUI
	Linux has two GUIs, KDE and Gnome. Although there are many options like Mate, LXDE, Xfce, etc.
	Unix – Shared desktop environment and also has Gnome.
Viruses
	Linux has had about 60-100 viruses so far listed that are not currently spreading.
	There are between 80 and 120 viruses reported to date in Unix.
Threat detection
	Threat detection and resolution are very fast as Linux is mainly community-driven. So if any Linux user publishes some kind of threat, a team of qualified developers will start working to resolve this threat
	Unix users require longer waiting times to get the correct fix
Architectures
	Linux – Originally developed for Intel x86 hardware processors. It is available for over 20 different types of CPU, which also includes an ARM
	Unix – It is available on PA-RISC and Itanium machines
Usage
	Linux OS can be installed on different types of devices such as mobile, tablets
	The UNIX operating system is used for Internet servers, workstations, and computers
Best feature
	Linux – Kernel update without a reboot
	Unix – Feta ZFS – next-generation DTrace file system – dynamic kernel tracking
Versions
	Various versions of Linux are Redhat, Ubuntu, OpenSuse, Solaris etc
	Different versions of Unix are AIS, BSD, HP-UX, etc.
Supported file type
	File system supported by file type such as xfs, nfs, cramfsm ext 1 to 4, ufs, devpts, NTFS
	The file systems supported by file types are zfs, hfx, GPS, xfs, vxfs
Portability
	Linux is portable and is booted from a USB Stick
	Unix is not portable
Source Code
	Linux – The source is available to the general public
	Unix – The source code is not available to anyone
Limitation of Linux
	There’s no standard edition of Linux
	Linux has patchier support for drivers that can cause system-wide errors.
	Linux, at least for new users, is not as easy to use as Windows.
	Many of the programs we use for Windows will only run on Linux using a complicated emulator. For example. Microsoft Office
	Linux is best suitable for a corporate user. It’s much harder to introduce in a home setting.
Limitations of Unix
	The unfriendly, terse, inconsistent, and non-mnemonic user interface
	Unix Operating system is designed for a slow computer system, so you can’t expect fast performance.
	The Shell interface can be treacherous because typos can destroy files.
	Versions on different machines are slightly different, so it lacks consistency.
	Unix does not provide a secure response time for hardware failure, so it does not support real-time response systems.
-
-
-
-
Combining webpack with ASP.NET MVC 5
 

Jonathan Harrison


Sep 20, 2017·4 min read

I am currently working on an ASP.NET MVC 5 web application in Visual Studio 2017 with my colleague matthewygf; on other recent projects we have been using a typical JavaScript Single Page Application (SPA) stack e.g. npm, gulp, etc.
Even though ASP.NET MVC 5 and Visual Studio includes everything needed for development i.e. package management with NuGet, TypeScript and LESS compiling, bundling and minification etc; we wanted a development pipeline such that if we ever decided to move to a SPA, reuse it on other new projects or replace gulp on others we could.
This lead us to build a “hybrid” combining webpack with ASP.NET MVC 5; it suits our needs very well so we thought we would share our template.
The complete project is over on GitHub if you just want to download it and start using it, otherwise you can create it from scratch by following these steps.
Create an ASP.NET Web Application project
First of all create an ASP.NET MVC Web application project in Visual Studio. Specifically we used:
•	.NET Framework 4.6.1
•	MVC
•	Authentication: No Authentication
Uninstall JavaScript and CSS NuGet packages
By default the ASP.NET MVC template includes Bootstrap and jQuery downloaded from NuGet. We want to use npm instead, so uninstall the following NuGet packages:
•	bootstrap
•	jQuery
•	jQuery.Validation
•	Microsoft.jQuery.Unobstrusive.Validation
•	Modernizr
•	Respond
After uninstalling the packages, make sure the Fonts and Scripts folders are deleted and also,the Content folder only contains Site.css.
Create package.json and install dependencies
Now that we have a clean starting point, in the project folder open a terminal and run.
npm init
Just accept all the default values and then you will have a package.json file created. After that its time to use:
npm install --save
To install the following packages:
•	bootstrap
•	jquery
•	jquery-validation
•	jquery-validation-unobtrusive
With the main dependencies installed, we need to install the development dependencies which includes webpack and other to support TypeScript and LESS. Use:
npm install -D
To install the following packages:
•	@types/bootstrap
•	@types/jquery
•	@types/jquery-validation-unobtrusive
•	@types/jquery.validation
•	awesome-typescript-loader
•	clean-webpack-plugin
•	css-loader
•	extract-text-webpack-plugin
•	file-loader
•	html-loader
•	html-webpack-plugin
•	less
•	less-loader
•	source-map-loader
•	style-loader
•	typescript
•	webpack
•	webpack-merge
Create src folder
All the dependencies are installed so create a folder called src in the root of the ASP.NET MVC project; this folder will contain all the TypeScript and LESS source files.
After creating the folder, move the Site.css file from the Content folder to src and rename it index.less. Open the index.less file and at the top add this:
@import "../node_modules/bootstrap/less/bootstrap.less";
This will make the bootstrap classes and styles available across our application.
Now create a index.ts file and add the following to it:
import "./index.less";
import "bootstrap";
This imports the LESS file we just created and also bootstrap in order that bootstrap components such as the navigation bar work as expected.
Modify _Layout.cshtml
As per webpack’s best practices for production, our production webpack configuration will generate multiple bundles each having a unique name every time they build. webpack can inject tags for these generated files into a HTML file, we will use the MVC shared layout file for this.
First of all rename the _Layout.cshtml file in Views/Shared to _Layout_Template.cshtml; this is because webpack will generate _Layout.cshtml from _Layout_Template.cshtml with the script and link tags appended at the end of the body element.
Create tsconfig.json
To support TypeScript compilation, you will need to create a tsconfig.json file at the root of the ASP .NET project. This file specifies the root files and the compiler options required to compile the TypeScript files. Simply copy the file from over on the GitHub project: tsconfig.json.
Create webpack configuration files
Now its time to create the webpack configuration files, create these at the root of the ASP .NET project. Simply copy the files from over on the GitHub project: webpack.common.js, webpack.dev.js, webpack.prod.js
Running webpack
Now that everything is setup, we just need to run webpack. We can use npm scripts to do this, so in your package.json add the following:
"scripts": {
"build:dev": "webpack --config webpack.dev.js",
"build:prod": "webpack --config webpack.prod.js"
}
You will need to be run build:dev for example every time you build your ASP.NET project. This can be done by using the NPM Task Runner extension for Visual Studio, where you can setup a binding for BeforeBuild to run build:dev.
After you have configured the extension, every time you build it runs the selected script. Note — this will only work in Visual Studio not on a build server; for a build server you will need to run these npm scripts manually.
If you followed all the steps above or grabbed the code from GitHub, just run the project and you will see the same old ASP.NET bootstrap starter site.
-
-
-
A single-page application (SPA) is a web application or website that interacts with the user by dynamically rewriting the current web page with new data from the web server, instead of the default method of a web browser loading entire new pages. The goal is faster transitions that make the website feel more like a native app.
In a SPA, a page refresh never occurs; instead, all necessary HTML, JavaScript, and CSS code is either retrieved by the browser with a single page load,[1] or the appropriate resources are dynamically loaded and added to the page as necessary, usually in response to user actions. The page does not reload at any point in the process, nor does it transfer control to another page, although the location hash or the HTML5 History API can be used to provide the perception and navigability of separate logical pages in the application.[2]
History
The origins of the term single-page application are unclear, though the concept was discussed at least as early as 2003.[3] Stuart Morris, a programming student at Cardiff University, Wales, wrote the Self-Contained website at slashdotslash.com with the same goals and functions in April 2002,[4] and later the same year Lucas Birdeau, Kevin Hakman, Michael Peachey and Clifford Yeh described a single-page application implementation in US patent 8,136,109.[5]
JavaScript can be used in a web browser to display the user interface (UI), run application logic, and communicate with a web server. Mature open-source libraries are available that support the building of a SPA, reducing the amount of JavaScript code developers have to write.
Technical approaches
There are various techniques available that enable the browser to retain a single page even when the application requires server communication.
Document Hashes
HTML authors can leverage element IDs to show or hide different sections of the HTML document. Then, using CSS, authors can use the `#target` selector to only show the section of the page which the browser navigated to.
JavaScript frameworks
Web browser JavaScript frameworks and libraries, such as AngularJS, Ember.js, ExtJS, Knockout.js, Meteor.js, React, Vue.js, and Svelte have adopted SPA principles. Aside from ExtJS, all of these are open-source.
•	AngularJS is a fully client-side framework. AngularJS's templating is based on bidirectional UI data binding. Data-binding is an automatic way of updating the view whenever the model changes, as well as updating the model whenever the view changes. The HTML template is compiled in the browser. The compilation step creates pure HTML, which the browser re-renders into the live view. The step is repeated for subsequent page views. In traditional server-side HTML programming, concepts such as controller and model interact within a server process to produce new HTML views. In the AngularJS framework, the controller and model states are maintained within the client browser. Therefore, new pages are capable of being generated without any interaction with a server.
•	Ember.js is a client-side JavaScript web application framework based on the model–view–controller (MVC) software architectural pattern. It allows developers to create scalable single-page applications by incorporating common idioms and best practices into a framework that provides a rich object model, declarative two-way data binding, computed properties, automatically updating templates powered by Handlebars.js, and a router for managing application state.
•	ExtJS is also a client side framework that allows creating MVC applications. It has its own event system, window and layout management, state management (stores) and various UI components (grids, dialog windows, form elements etc.). It has its own class system with either dynamic or static loader. The application built with ExtJS can either exist on its own (with state in the browser) or with the server (e.g. with REST API that is used to fill its internal stores). ExtJS has only built in capabilities to use localStorage so larger applications need a server to store state.
•	Knockout.js is a client side framework which uses templates based on the Model-View-ViewModel pattern.
•	Meteor.js is a full-stack (client-server) JavaScript framework designed exclusively for SPAs. It features simpler data binding than Angular, Ember or ReactJS,[6] and uses the Distributed Data Protocol[7] and a publish–subscribe pattern to automatically propagate data changes to clients in real-time without requiring the developer to write any synchronization code. Full stack reactivity ensures that all layers, from the database to the templates, update themselves automatically when necessary. Ecosystem packages such as Server Side Rendering[8] address the problem of Search Engine Optimization.
•	React is a JavaScript library for building user interfaces. It is maintained by Facebook, Instagram and a community of individual developers and corporations. React uses a new language which is a mix of JS and HTML (a subset of HTML). Several companies use React with Redux (JavaScript library) which adds state management capabilities, which (with several other libraries) lets developers create complex applications.[9]
•	Vue.js is a JavaScript framework for building user interfaces. Vue developers also provide Vuex for state management.
•	Svelte is a framework for building user interfaces that compiles Svelte code to JavaScript DOM manipulations, avoiding the need to bundle a framework to the client, and allowing for simpler application development syntax.
Ajax
As of 2006, the most prominent technique used was Ajax.[1] Ajax involves using asynchronous requests to a server for XML or JSON data, such as with JavaScript's XMLHttpRequest or more modern fetch() (since 2017), or the deprecated ActiveX Object. In contrast to the declarative approach of most SPA frameworks, with Ajax the website directly uses JavaScript or a JavaScript library such as jQuery to manipulate the DOM and edit HTML elements. Ajax has further been popularized by libraries like jQuery, which provides a simpler syntax and normalizes Ajax behavior across different browsers which historically had varying behavior.
WebSockets
WebSockets are a bidirectional real-time client-server communication technology that are part of the HTML5 specification. For real-time communication, their use is superior to Ajax in terms of performance[10] and simplicity.
Server-sent events
Server-sent events (SSEs) is a technique whereby servers can initiate data transmission to browser clients. Once an initial connection has been established, an event stream remains open until closed by the client. SSEs are sent over traditional HTTP and have a variety of features that WebSockets lack by design such as automatic reconnection, event IDs, and the ability to send arbitrary events.[11]
Browser plugins
Although this method is outdated, asynchronous calls to the server may also be achieved using browser plug-in technologies such as Silverlight, Flash, or Java applets.
Data transport (XML, JSON and Ajax)
Requests to the server typically result in either raw data (e.g., XML or JSON), or new HTML being returned. In the case where HTML is returned by the server, JavaScript on the client updates a partial area of the DOM (Document Object Model).[12] When raw data is returned, often a client-side JavaScript XML / (XSL) process (and in the case of JSON a template) is used to translate the raw data into HTML, which is then used to update a partial area of the DOM.
Server architecture
Thin server architecture
A SPA moves logic from the server to the client, with the role of the web server evolving into a pure data API or web service. This architectural shift has, in some circles, been coined "Thin Server Architecture" to highlight that complexity has been moved from the server to the client, with the argument that this ultimately reduces overall complexity of the system.
Thick stateful server architecture
The server keeps the necessary state in memory of the client state of the page. In this way, when any request hits the server (usually user actions), the server sends the appropriate HTML and/or JavaScript with the concrete changes to bring the client to the new desired state (usually adding/deleting/updating a part of the client DOM). At the same time, the state in server is updated. Most of the logic is executed on the server, and HTML is usually also rendered on the server. In some ways, the server simulates a web browser, receiving events and performing delta changes in server state which are automatically propagated to client.
This approach needs more server memory and server processing, but the advantage is a simplified development model because a) the application is usually fully coded in the server, and b) data and UI state in the server are shared in the same memory space with no need for custom client/server communication bridges.
Thick stateless server architecture
This is a variant of the stateful server approach. The client page sends data representing its current state to the server, usually through Ajax requests. Using this data, the server is able to reconstruct the client state of the part of the page which needs to be modified and can generate the necessary data or code (for instance, as JSON or JavaScript), which is returned to the client to bring it to a new state, usually modifying the page DOM tree according to the client action that motivated the request.
This approach requires that more data be sent to the server and may require more computational resources per request to partially or fully reconstruct the client page state in the server. At the same time, this approach is more easily scalable because there is no per-client page data kept in the server and, therefore, Ajax requests can be dispatched to different server nodes with no need for session data sharing or server affinity.
Running locally
Some SPAs may be executed from a local file using the file URI scheme. This gives users the ability to download the SPA from a server and run the file from a local storage device, without depending on server connectivity. If such a SPA wants to store and update data, it must use browser-based Web Storage. These applications benefit from advances available with HTML5.[13]
Challenges with the SPA model
Because the SPA is an evolution away from the stateless page-redraw model that browsers were originally designed for, some new challenges have emerged. Possible solutions (of varying complexity, comprehensiveness, and author control) include:[14]
•	Client-side JavaScript libraries.
•	Server-side web frameworks that specialize in the SPA model.[15][16][17]
•	The evolution of browsers and the HTML5 specification,[18] designed for the SPA model.
Search-engine optimization
Because of the lack of JavaScript execution on crawlers of some popular Web search engines,[19] SEO (Search engine optimization) has historically presented a problem for public facing websites wishing to adopt the SPA model.[20]
Between 2009 and 2015, Google Webmaster Central proposed and then recommended an "AJAX crawling scheme"[21][22] using an initial exclamation mark in fragment identifiers for stateful AJAX pages (#!). Special behavior must be implemented by the SPA site to allow extraction of relevant metadata by the search engine's crawler. For search engines that do not support this URL hash scheme, the hashed URLs of the SPA remain invisible. These "hash-bang" URIs have been considered problematic by a number of writers including Jeni Tennison at the W3C because they make pages inaccessible to those who do not have JavaScript activated in their browser. They also break HTTP referer headers as browsers are not allowed to send the fragment identifier in the Referer header.[23] In 2015, Google deprecated their hash-bang AJAX crawling proposal.[24]
Alternatively, applications may render the first page load on the server and subsequent page updates on the client. This is traditionally difficult, because the rendering code might need to be written in a different language or framework on the server and in the client. Using logic-less templates, cross-compiling from one language to another, or using the same language on the server and the client may help to increase the amount of code that can be shared.
In 2018, Google introduced dynamic rendering as another option for sites wishing to offer crawlers a non-JavaScript heavy version of a page for indexing purposes.[25] Dynamic rendering switches between a version of a page that is rendered client-side and a pre-rendered version for specific user agents. This approach involves your web server detecting crawlers (via the user agent) and routing them to a renderer, from which they are then served a simpler version of HTML content.
Because SEO compatibility is not trivial in SPAs, it is worth noting that SPAs are commonly not used in a context where search engine indexing is either a requirement, or desirable. Use cases include applications that surface private data hidden behind an authentication system. In the cases where these applications are consumer products, often a classic "page redraw" model is used for the applications landing page and marketing site, which provides enough meta data for the application to appear as a hit in a search engine query. Blogs, support forums, and other traditional page redraw artifacts often sit around the SPA that can seed search engines with relevant terms.
Another approach used by server-centric web frameworks like the Java-based ItsNat is to render any hypertext on the server using the same language and templating technology. In this approach, the server knows with precision the DOM state on the client, any big or small page update required is generated in the server, and transported by Ajax, the exact JavaScript code to bring the client page to the new state executing DOM methods. Developers can decide which page states must be crawlable by web spiders for SEO and be able to generate the required state at load time generating plain HTML instead of JavaScript. In the case of the ItsNat framework, this is automatic because ItsNat keeps the client DOM tree in the server as a Java W3C DOM tree; rendering of this DOM tree in the server generates plain HTML at load time and JavaScript DOM actions for Ajax requests. This duality is very important for SEO because developers can build with the same Java code and pure HTML-based templating the desired DOM state in server; at page load time, conventional HTML is generated by ItsNat making this DOM state SEO-compatible.
As of version 1.3,[26] ItsNat provides a new stateless mode, and the client DOM is not kept on the server because, with the stateless mode client, DOM state is partially or fully reconstructed on the server when processing any Ajax request based on required data sent by the client informing the server of the current DOM state; the stateless mode may be also SEO-compatible because SEO compatibility happens at load time of the initial page unaffected by stateful or stateless modes. Another possible choice is frameworks like PreRender, Puppeteer, Rendertron which can be easily integrated into any website as a middleware with web server configuration enabling bot requests (google bot and others) to be served by the middleware while non-bot requests are served as usual. These frameworks cache the relevant website pages periodically to allow latest versions be available to search engines. These frameworks have been officially approved by google.[27]
There are a couple of workarounds to make it look as though the web site is crawlable. Both involve creating separate HTML pages that mirror the content of the SPA. The server could create an HTML-based version of the site and deliver that to crawlers, or it's possible to use a headless browser such as PhantomJS to run the JavaScript application and output the resulting HTML.
Both of these do require quite a bit of effort, and can end up giving a maintenance headache for the large complex sites. There are also potential SEO pitfalls. If server-generated HTML is deemed to be too different from the SPA content, then the site will be penalized. Running PhantomJS to output the HTML can slow down the response speed of the pages, which is something for which search engines – Google in particular – downgrade the rankings.[28]
Client/server code partitioning
One way to increase the amount of code that can be shared between servers and clients is to use a logic-less template language like Mustache or Handlebars. Such templates can be rendered from different host languages, such as Ruby on the server and JavaScript in the client. However, merely sharing templates typically requires duplication of business logic used to choose the correct templates and populate them with data. Rendering from templates may have negative performance effects when only updating a small portion of the page—such as the value of a text input within a large template. Replacing an entire template might also disturb a user's selection or cursor position, where updating only the changed value might not. To avoid these problems, applications can use UI data bindings or granular DOM manipulation to only update the appropriate parts of the page instead of re-rendering entire templates.
Browser history
With a SPA being, by definition, "a single page", the model breaks the browser's design for page history navigation using the "forward" or "back" buttons. This presents a usability impediment when a user presses the back button, expecting the previous screen state within the SPA, but instead, the application's single page unloads and the previous page in the browser's history is presented.
The traditional solution for SPAs has been to change the browser URL's hash fragment identifier in accord with the current screen state. This can be achieved with JavaScript, and causes URL history events to be built up within the browser. As long as the SPA is capable of resurrecting the same screen state from information contained within the URL hash, the expected back-button behavior is retained.
To further address this issue, the HTML5 specification has introduced pushState and replaceState providing programmatic access to the actual URL and browser history.
Analytics
Analytics tools such as Google Analytics rely heavily upon entire new pages loading in the browser, initiated by a new page load. SPAs do not work this way.
After the first page load, all subsequent page and content changes are handled internally by the application, which should simply call a function to update the analytics package. Failing to call said function, the browser never triggers a new page load, nothing gets added to the browser history, and the analytics package has no idea who is doing what on the site.
Security Scanning
Similarly to the problems encountered with search engine crawlers, DAST tools may struggle with these JavaScript-rich applications. Problems can include the lack of hypertext links, memory usage and resources loaded by the SPA typically being made available by an Application Programming Interface or API. Single Page Applications are still subject to the same security risks as traditional web pages such as Cross-Site Scripting (XSS), but also a host of other unique vulnerabilities such as Data Exposure via API and Client Side Logic & Client-Side Enforcement of Server-Side Security.[29] In order to effectively scan a Single Page Application, a DAST scanner must be able to navigate the client-side application in a reliable and repeatable manner to allow discovery of all areas of the application and interception of all requests that the application sends to remote servers (e.g. API requests). There are few commercial tools capable of such actions but such tools definitely exist.
Adding page loads to a SPA
It is possible to add page load events to a SPA using the HTML5 history API; this will help integrate analytics. The difficulty comes in managing this and ensuring that everything is being tracked accurately – this involves checking for missing reports and double entries. Some frameworks provide open source analytics integrations addressing most of the major analytics providers. Developers can integrate them into the application and make sure that everything is working correctly, but there is no need to do everything from scratch.[28]
Speeding up the page load
There are some ways of speeding up the initial load of a SPA, such as a heavy approach to caching and lazy-loading modules when needed. But it's not possible to get away from the fact that it needs to download the framework, at least some of the application code, and will most likely hit an API for data before displaying something in the browser.[28] This is a "pay me now, or pay me later" trade-off scenario. The question of performance and wait-times remains a decision that the developer must make.
Page lifecycle
This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Single-page application" – news · newspapers · books · scholar · JSTOR (October 2020) (Learn how and when to remove this template message)
A SPA is fully loaded in the initial page load and then page regions are replaced or updated with new page fragments loaded from the server on demand. To avoid excessive downloading of unused features, a SPA will often progressively download more features as they become required, either small fragments of the page, or complete screen modules.
In this way an analogy exists between "states" in a SPA and "pages" in a traditional website. Because "state navigation" in the same page is analogous to page navigation, in theory, any page-based web site could be converted to single-page replacing in the same page only the changed parts.
The SPA approach on the web is similar to the single-document interface (SDI) presentation technique popular in native desktop applications.
-
-
-
IoT / Internet Of Things / 
Nesnelerin İnterneti     	(GITHUB) (test)
internet of things nedir, Internet of things nedir, nesnelerin interneti nedir, iot nedir, IoT nedir, iot nedir
Birbiriyle konuşan makinelerin sistemi diyebiliriz… İnsansız…
Amazon videosu, güzel açıklıyor: https://aws.amazon.com/tr/iot/?nc2=h_ql_prod_it
The Internet of things (IoT) is a system of interrelated computing devices, mechanical and digital machines provided with unique identifiers (UIDs) and the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction. 
The definition of the Internet of things has evolved due to the convergence of multiple technologies, real-time analytics, machine learning, commodity sensors, and embedded systems.[1] Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), and others all contribute to enabling the Internet of things. In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the "smart home", including devices and appliances (such as lighting fixtures, thermostats, home security systems and cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers.
There are a number of serious concerns about dangers in the growth of IoT, especially in the areas of privacy and security, and consequently industry and governmental moves to address these concerns have begun.
-
-
videoyu izle: https://www.youtube.com/watch?v=rrT6v5sOwJg

Material Design   (GITHUB)  (test) 
Material Design nedir
Material Design (codenamed Quantum Paper)[1] is a design language that Google developed in 2014. Expanding on the "card" motifs that debuted in Google Now, Material Design uses more grid-based layouts, responsive animations and transitions, padding, and depth effects such as lighting and shadows.
Google announced Material Design on June 25, 2014, at the 2014 Google I/O conference.
Overview
Designer Matías Duarte explained that, "unlike real paper, our digital material can expand and reform intelligently. Material has physical surfaces and edges. Seams and shadows provide meaning about what you can touch." Google states that their new design language is based on paper and ink but implementation takes place in an advanced manner.
Material Design will gradually be extended throughout Google's array of web and mobile products, providing a consistent experience across all platforms and applications. Google has also released application programming interfaces (APIs) for third-party developers to incorporate the design language into their applications.[5][6][7] The main purpose of material design is creation of new visual language that combines principles of good design with technical and scientific innovation.
In 2018, Google detailed a revamp of the language, with a focus on providing more flexibility for designers to create custom "themes" with varying geometry, colors, and typography. Google released Material Theme Editor exclusively for the macOS design application Sketch.[8][9]
Implementation
As of 2020, most of Google's mobile applications for Android as well as its web app counterparts had applied the new design language, including Gmail, YouTube, Google Drive, Google Docs, Google Sheets, Google Slides, Google Photos, Google Maps, Google Classroom, Google Translate, Google Chrome, Google Keep, Google Play, and most other Google products. It is also the primary design language of Android and Chrome OS.
In 2018, with the introduction of the ability to create custom themes, Google also began redesigning most of their apps into a customized and adapted version of Material Design called the Google Material Theme[10], also dubbed "Material Design 2"[11], which heavily emphasized white space, rounded corners, colorful icons, bottom navigation bars, and utilized a special size-condensed version of Google's proprietary Product Sans font called Google Sans.[12] As of 2020, most Google applications have also applied the new Google Material Theme design, with the notable exception of YouTube.
The canonical implementation of Material Design for web application user interfaces is called Polymer.[13] It consists of the Polymer library, a shim that provides a Web Components API for browsers that do not implement the standard natively, and an elements catalog, including the "paper elements collection" that features visual elements of the Material Design.[14]
-
-
-

- 
Cross-platform software  / Multi-platform software  (GITHUB)  (test)
Cross-platform nedir, cross platform nedir, multi platform nedir, multi-platform nedir
For example, a cross-platform application may run on Microsoft Windows, Linux, and macOS. Cross-platform programs may run on as many as all existing platforms, or on as few as two platforms. Cross-platform frameworks (such as Qt, Flutter, NativeScript, Xamarin, Phonegap, Ionic, and React Native) exist to aid cross-platform development.
Platform can refer to the type of processor (CPU) or other hardware on which a given operating system or application runs, the type of operating system on a computer or the combination of the type of hardware and the type of operating system running on it.[4] An example of a common platform is Microsoft Windows running on the x86 architecture. Other well-known desktop computer platforms include Linux/Unix and macOS - both of which are themselves cross-platform.
-
-
- 
-

Multiversion concurrency control (MCC or MVCC),	(empty)  (empty) 
[Multiversion concurrency control (MCC or MVCC), mcc nedir, mvcc nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
Multiversion concurrency control (MCC or MVCC), is a concurrency control method commonly used by database management systems to provide concurrent access to the database and in programming languages to implement transactional memory.[1]
-
-
Multiversion concurrency control (MCC or MVCC), is a concurrency control method commonly used by database management systems to provide concurrent access to the database and in programming languages to implement transactional memory.[1]
Description
Without concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data. For instance, when making a wire transfer between two bank accounts if a reader reads the balance at the bank when the money has been withdrawn from the original account and before it was deposited in the destination account, it would seem that money has disappeared from the bank. Isolation is the property that provides guarantees in the concurrent accesses to data. Isolation is implemented by means of a concurrency control protocol. The simplest way is to make all readers wait until the writer is done, which is known as a read-write lock. Locks are known to create contention especially between long read transactions and update transactions. MVCC aims at solving the problem by keeping multiple copies of each data item. In this way, each user connected to the database sees a snapshot of the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed (or, in database terms: until the transaction has been committed.)
When an MVCC database needs to update a piece of data, it will not overwrite the original data item with new data, but instead creates a newer version of the data item. Thus there are multiple versions stored. The version that each transaction sees depends on the isolation level implemented. The most common isolation level implemented with MVCC is snapshot isolation. With snapshot isolation, a transaction observes a state of the data as when the transaction started.
MVCC provides point-in-time consistent views. Read transactions under MVCC typically use a timestamp or transaction ID to determine what state of the DB to read, and read these versions of the data. Read and write transactions are thus isolated from each other without any need for locking. However, despite locks being unnecessary, they are used by some MVCC databases such as Oracle. Writes create a newer version, while concurrent reads access an older version.
MVCC introduces the challenge of how to remove versions that become obsolete and will never be read. In some cases, a process to periodically sweep through and delete the obsolete versions is implemented. This is often a stop-the-world process that traverses a whole table and rewrites it with the last version of each data item. PostgreSQL can use this approach with its VACUUM FREEZE process. Other databases split the storage blocks into two parts: the data part and an undo log. The data part always keeps the last committed version. The undo log enables the recreation of older versions of data. The main inherent limitation of this latter approach is that when there are update-intensive workloads, the undo log part runs out of space and then transactions are aborted as they cannot be given their snapshot. For a document-oriented database it also allows the system to optimize documents by writing entire documents onto contiguous sections of disk—when updated, the entire document can be re-written rather than bits and pieces cut out or maintained in a linked, non-contiguous database structure.
Implementation
MVCC uses timestamps (TS), and incrementing transaction IDs, to achieve transactional consistency. MVCC ensures a transaction (T) never has to wait to Read a database object (P) by maintaining several versions of the object. Each version of object P has both a Read Timestamp (RTS) and a Write Timestamp (WTS) which lets a particular transaction Ti read the most recent version of the object which precedes the transaction's Read Timestamp RTS(Ti).
If transaction Ti wants to Write to object P, and there is also another transaction Tk happening to the same object, the Read Timestamp RTS(Ti) must precede the Read Timestamp RTS(Tk), i.e., RTS(Ti) < RTS(Tk)[clarification needed], for the object Write Operation (WTS) to succeed. A Write cannot complete if there are other outstanding transactions with an earlier Read Timestamp (RTS) to the same object. Like standing in line at the store, you cannot complete your checkout transaction until those in front of you have completed theirs.
To restate; every object (P) has a Timestamp (TS), however if transaction Ti wants to Write to an object, and the transaction has a Timestamp (TS) that is earlier than the object's current Read Timestamp, TS(Ti) < RTS(P), then the transaction is aborted and restarted. (This is because a later transaction already depends on the old value.) Otherwise, Ti creates a new version of object P and sets the read/write timestamp TS of the new version to the timestamp of the transaction TS ← TS(Ti).[2]
The drawback to this system is the cost of storing multiple versions of objects in the database. On the other hand, reads are never blocked, which can be important for workloads mostly involving reading values from the database. MVCC is particularly adept at implementing true snapshot isolation, something which other methods of concurrency control frequently do either incompletely or with high performance costs.
Examples
Concurrent read–write
At Time = 1, the state of a database could be:
Time	Object 1	Object 2
0	"Foo" by T0	"Bar" by T0
1	"Hello" by T1	
T0 wrote Object 1="Foo" and Object 2="Bar". After that T1 wrote Object 1="Hello" leaving Object 2 at its original value. The new value of Object 1 will supersede the value at 0 for all transactions that start after T1 commits at which point version 0 of Object 1 can be garbage collected.
If a long running transaction T2 starts a read operation of Object 2 and Object 1 after T1 committed and there is a concurrent update transaction T3 which deletes Object 2 and adds Object 3="Foo-Bar", the database state will look like this at time 2:
Time	Object 1	Object 2	Object 3
0	"Foo" by T0	"Bar" by T0	
1	"Hello" by T1		
2		(deleted) by T3	"Foo-Bar" by T3
There is a new version as of time 2 of Object 2 which is marked as deleted and a new Object 3. Since T2 and T3 run concurrently T2 sees the version of the database before 2 i.e. before T3 committed writes, as such T2 reads Object 2="Bar" and Object 1="Hello". This is how multiversion concurrency control allows snapshot isolation reads without any locks.
History
Multiversion concurrency control is described in some detail in the 1981 paper "Concurrency Control in Distributed Database Systems"[3] by Phil Bernstein and Nathan Goodman, then employed by the Computer Corporation of America. Bernstein and Goodman's paper cites a 1978 dissertation[4] by David P. Reed which quite clearly describes MVCC and claims it as an original work.
The first shipping, commercial database software product featuring MVCC was VAX Rdb/ELN, created at Digital Equipment Corporation by Jim Starkey. Starkey went on to create the second commercially successful MVCC database - InterBase.[5]
-
-
-
ilk değeri sıfır olan listeler şunlar bunlar vb.

örneğe bkz:

This will assign the value 23 to the first element of the array.
Arrays in C# are zero-indexed meaning the first member has index 0, the second has index 1, and so on.
-
-
Zero-based numbering  
Connected to:
 NumberingDerivativeComputer science
From Wikipedia, the free encyclopedia	
Zero-based numbering is a way of numbering in which the initial element of a sequence is assigned the index 0, rather than the index 1 as is typical in everyday non-mathematical or non-programming circumstances. 

Under zero-based numbering, the initial element is sometimes termed the zeroth element,[1] rather than the first element; zeroth is a coined ordinal number corresponding to the number zero. In some cases, an object or value that does not (originally) belong to a given sequence, but which could be naturally placed before its initial element, may be termed the zeroth element. There is not wide agreement regarding the correctness of using zero as an ordinal (nor regarding the use of the term zeroth) as it creates ambiguity for all subsequent elements of the sequence when lacking context.
Numbering sequences starting at 0 is quite common in mathematics notation, in particular in combinatorics, though programming languages for mathematics usually index from 1.[2][3][4] In computer science, array indices usually start at 0 in modern programming languages, so computer programmers might use zeroth in situations where others might use first, and so forth. In some mathematical contexts, zero-based numbering can be used without confusion, when ordinal forms have well established meaning with an obvious candidate to come before first; for instance a zeroth derivative of a function is the function itself, obtained by differentiating zero times. Such usage corresponds to naming an element not properly belonging to the sequence but preceding it: the zeroth derivative is not really a derivative at all. However, just as the first derivative precedes the second derivative, so also does the zeroth derivative (or the original function itself) precede the first derivative.
Computer programming
Origin
Martin Richards, creator of the BCPL language (a precursor of C), designed arrays initiating at 0 as the natural position to start accessing the array contents in the language, since the value of a pointer p used as an address accesses the position p + 0 in memory.[5][6] Canadian systems analyst Mike Hoye asked Richards the reasons for choosing that convention. BCPL was first compiled for the IBM 7094; the language introduced no run time indirection lookups, so the indirection optimization provided by these arrays was done at compile time.[6] The optimization was nevertheless important.[6][7]
Edsger W. Dijkstra later wrote a pertinent note Why numbering should start at zero[8] in 1982, analyzing the possible designs of array indices by enclosing them in a chained inequality, combining sharp and standard inequalities to four possibilities, demonstrating that to his conviction zero-based arrays are best represented by non-overlapping index ranges, which start at zero, alluding to open, half-open and closed intervals as with the real numbers. Dijkstra's criteria for preferring this convention are in detail that it represents empty sequences in a more natural way (a ≤ i < a ?) than closed "intervals" (a ≤ i ≤ (a−1) ?), and that with half-open "intervals" of naturals, the length of a sub-sequence equals the upper minus the lower bound (a ≤ i < b gives (b−a) possible values for i, with a, b, i all integers).
-
-
-
Microsoft Azure	(GITHUB)  (test)
Azure nedir
Microsoft Azure, commonly referred to as Azure (/ˈæʒər/), is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through Microsoft-managed data centers. It provides software as a service (SaaS), platform as a service (PaaS) and infrastructure as a service (IaaS) and supports many different programming languages, tools, and frameworks, including both Microsoft-specific and third-party software and systems.
Azure was announced in October 2008, started with codename "Project Red Dog",[1] and released on February 1, 2010, as Windows Azure before being renamed to Microsoft Azure on March 25, 2014.
-
-
Eager Loading: Eager Loading helps you to load all your needed entities at once. i.e. related objects (child objects) are loaded automatically with its parent object.
When to use:
1.	Use Eager Loading when the relations are not too much. Thus, Eager Loading is a good practice to reduce further queries on the Server.
2.	Use Eager Loading when you are sure that you will be using related entities with the main entity everywhere.
Lazy Loading: In case of lazy loading, related objects (child objects) are not loaded automatically with its parent object until they are requested. By default LINQ supports lazy loading.
When to use:
1.	Use Lazy Loading when you are using one-to-many collections.
2.	Use Lazy Loading when you are sure that you are not using related entities instantly.
NOTE: Entity Framework supports three ways to load related data - eager loading, lazy loading and explicit loading.
-
-
class Personel
{
    public int Id { get; set; }
    public string Adi { get; set; }
    public string SoyAdi { get; set; }
}
 
class Personeller
{
    List<Personel> PersonelListesi = new List<Personel>();
    public void Add(Personel p)
    {
        PersonelListesi.Add(p);
    }
    public IEnumerator GetEnumerator()
    {
        return PersonelListesi.GetEnumerator();
    }
}
-
-
-
-
-
-
C#’ta IEnumerable ve IEnumerator Interfaceleri Nedir? ve Nasıl Kullanılır?
YAZAR: GENÇAY · 24 MART 2017
Merhaba,
C#’ta koleksiyon yahut array yapıları üzerinde periyodik bir düzende dönmemizi ve verileri bu şekilde tek tek elde etmemizi sağlayan foreach döngüsünün temel çalışma prensibi olan iterasyon mantığının kendi sınıflarımız üzerinde nasıl uygulanacağını inceleyeceğiz. Biliyorsunuz ki C#’ta iterasyon dendiğinde akla ilk olarak foreach döngüsü gelmektedir. Yani, bir sınıfa iterasyon özelliği kazandırabilirsek foreach döngüsü bu sınıfla etkileşime girebilecek iterasyonun periyoduna göre ilgili sınıf içerisinde belirlenen işlemleri gerçekleştirecek. Bir sınıfa iterasyon özelliklerini kazandırmak için gereken tüm özellikler IEnumerator interface’i aracılığıyla elde edilebilmektedir. IEnumerable interface’i ise bir sınıfa foreach mekanizması tarafından tanınması için gerekli yetenekleri/nitelikleri kazandırır. Yani enumerator yapısını… Şimdi gelin bu iki interface yapısını detaylıca irdeleyerek, nasıl kullanıldıklarına değinelim.
Şimdi herşeyi en temelden ele alarak anlatmaya başlayalım.
Hani koleksiyonlarda kullandığımız “List” gibi, “Dictionary” gibi referanslardan üretilen nesneler direkt olarak foreach döngüsüne verilebilmektedir. Öyne değil mi?
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15	    static void Main(string[] args)
    {
        List<string> Isimler = new List<string>();
        Isimler.Add("Gençay");
        Isimler.Add("Nurgül");
        Isimler.Add("Ayşe");
        Isimler.Add("Fatih");
        Isimler.Add("Ilgaz");
 
        foreach (var isim in Isimler)
            Console.WriteLine(isim);
 
        Console.Read();
    }
}
Bakın Isimler referansında bir List nesnesi oluşturdum ve bu nesneyi işaret eden referansı foreach döngüsünde verdiğim vakit sıkıntısız çalışmaktadır.
Eee… Peki örneğin “Personeller” isminde bir sınıf oluştursak. Bunu direkt olarak foreach döngüsüne verebiliyor muyuz?
 
Hayır… Neden?
Çünkü, yazmış olduğumuz sınıfımızı foreach döngüsünün istediği şartlarda yapılandırmadığımız için. foreach döngüsü bir sınıf üzerinde çalışacaksa o sınıfın kesinlikle ve kesinlikle içerisinde geriye IEnumerator döndüren GetEnumerator metodunun bulunmasını ister. Tek şartı budur.
Videodan da gördüğünüz gibi GetEnumerator metodu sınıf içerisine eklendiğinde foreach döngüsü ilgili sınıfın nesnesini kabul etmektedir.
Peki nedir bu GetEnumerator metodu?
GetEnumerator metodu, bir sınıfa iterasyon yapılarını kazandıracak özellikleri barındıran IEnumerator nesnesi dönen bir metotdur.
İşte o yüzden foreach bu metodu gördüğü anda ilgili sınıfın bir iterasyon mantığıyla çalıştığını düşünmekte ve kabul etmektedir. Velhasıl birazdan bu metodun detaylarına geleceğiz.
Şimdi kaldığımız yerden devam edelim.
Metodumuzun içerisine GetEnumerator metodunu ekledikten sonra direkt olarak foreach döngüsünde ilgili sınıfımızı çalıştıramayacağız. Bunun sebebi, sınıfımız içerisinde hangi veri topluluğu üzerinde işlem yapılacağının belirlenmiş olmamasıdır. Ayriyetten GetEnumerator metodu içerisinde de bir enumerator nesnesi dönmemiz gerekecektir.
O halde hemen eksikliklerimizi tamamlayalım…
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19	class Personel
{
    public int Id { get; set; }
    public string Adi { get; set; }
    public string SoyAdi { get; set; }
}
 
class Personeller
{
    List<Personel> PersonelListesi = new List<Personel>();
    public void Add(Personel p)
    {
        PersonelListesi.Add(p);
    }
    public IEnumerator GetEnumerator()
    {
        return PersonelListesi.GetEnumerator();
    }
}
Örneğimizde bir “Personel” sınıfı oluşturup, “Personeller” sınıfı içerisinde ilgili sınıfın koleksiyon yapısını ve bu koleksiyona veri ekleme işlevini yapan Add metodunu tanımladım. GetEnumerator metodu içerisinde ise koleksiyonumuzun GetEnumerator() metodu sayesinde bir enumerator elde edip return ettim.
Diziler ve koleksiyonlar(collections) itere(itersayon) edilebilir yapılar oldukları için içlerinde GetEnumerator metodu bulunmaktadır.
Bu yazımızın ileri satırlarında IEnumerator interface’ini detaylandırırken, kendi enumeratorümüzü oluşturmayıda konuşacağız. Ama şimdilik bu örneğimizde koleksiyon yahut dizi yapılarının GetEnumerator metodunu kullanmamız işimizi yeterince görmektedir.
Velhasıl… Yaptığımız bu işlemler neticesinde “Personeller” sınıfımız, içerisinde bir “Personel” veri kümesi barındıran ve bu veri kümesi üzerinde itere edilebilir bir nitelik arz eden bir sınıf mahiyetindedir.
Şuana kadar yaptığımız çalışmayı derleyip foreach döngüsünde sınıfımızı test ettiğimiz vakit çalıştığını göreceğiz.
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15	class Program
{
    static void Main(string[] args)
    {
        Personeller personeller = new Personeller();
        personeller.Add(new Personel { Id = 1, Adi = "Gençay", SoyAdi = "Yıldız" });
        personeller.Add(new Personel { Id = 2, Adi = "Aslı", SoyAdi = "Cambaz" });
        personeller.Add(new Personel { Id = 3, Adi = "Elif", SoyAdi = "Gök" });
        personeller.Add(new Personel { Id = 4, Adi = "Aykız", SoyAdi = "Yıldız" });
        personeller.Add(new Personel { Id = 5, Adi = "Erol", SoyAdi = "Burçak" });
        foreach (Personel personel in personeller)
            Console.WriteLine($"ID : {personel.Id}\nAdı : {personel.Adi}\nSoyadı : {personel.SoyAdi}\n*****");
        Console.Read();
    }
}
 
Burada dikkatinizi çekmek istediğim bir husus var. “Personeller” sınıfını foreach döngüsünde kullanırken döngü değişkeninin(personel) tipini “var” olarak belirleyip ilgili nesnenin tipini belirlemeyi compilera bırakabilirdik. Lakin şuana kadar yapmış olduğumuz tüm işlemler döngü değişkeninin object olarak gelmesini sağlamaktadır. O yüzden direkt olarak cast işlemi uygulatıyor, “var” yerine “Personel” tipini kullanıyorum.
 
Eğer ki siz “var” kullanmak istiyorsanız GetEnumerator metodunun geri dönüş tipini aşağıdaki gibi generic IEnumerator olarak tanımlamanız gerekmektedir.
1
2
3
4
5
6	...
        public IEnumerator<Personel> GetEnumerator()
        {
            return PersonelListesi.GetEnumerator();
        }
...
 
Bu ek bilgiden sonra artık konumuzun ikinci ana unsuruna gelebiliriz.
Evet sevgili okurlarım… Gördüğünüz gibi, oluşturmuş olduğumuz herhangi bir sınıfı itere edilebilir hale getirmeyi ve foreach döngüsü ile bu sınıf üzerinde dönmeyi görmüş olduk.
Şimdi makalemizin bu noktasına gelen okuyucularımın kafalarında muhakemesini yaptıkları konuşmalar sanırım üç aşağı beş yukarı aşağıda tahmin ettiğime benzer niteliktedir.
Ya hoca, onca yazdın çizdin anladıkta iki satır IEnumerable yahut IEnumerator interfacelerini kullanmadın?
Evet. Şu ana kadar hiç IEnumerable ve IEnumerator interfacelerini kullanmadım diyebiliriz. Haydi gelin şimdi bu interfaceleri tek tek ele alalım ve bu sırada yukarıdaki satırlarda bahsettiğimiz GetEnumerator metodunuda tam teferruatlı masaya yatıralım.
IEnumerable Interface’i
IEnumerable interface’i sayesinde bir sınıf itere edilebilir özellik kazanmaktadır. Peki bir sınıfın itere edilebilirlik özellik kazanması neydi? diye sorarsak eğer üst satırlarda bahsettiğimiz gibi o sınıfın geriye IEnumerator nesnesi dönen GetEnumerator isimli metodu barındırıyor olması demekti. Ee haliyle IEnumerable interface’i ilgili sınıfa uygulandığında GetEnumerator metodunu implement edecektir.
Yani uzun lafın kısası IEnumerable interface’in implement edildiği bir class üzerine GetEnumerator metodu uygulattırılır. Haliyle yukarıda yaptığımız gibi ilgili metodu manuel olarak yazmaktan ve olası imla hatalarından bizleri kurtarmaktadır.
01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16
17
18
19
20
21
22
23	class Personel
{
    public int Id { get; set; }
    public string Adi { get; set; }
    public string SoyAdi { get; set; }
}
 
class Personeller : IEnumerable<Personel>
{
    List<Personel> PersonelListesi = new List<Personel>();
    public void Add(Personel p)
    {
        PersonelListesi.Add(p);
    }
    public IEnumerator<Personel> GetEnumerator()
    {
        return PersonelListesi.GetEnumerator();
    }
    IEnumerator IEnumerable.GetEnumerator()
    {
        return PersonelListesi.GetEnumerator();
    }
}
Görüldüğü üzere çok rahat bir şekilde ilgili sınıfımız itere edilebilir hale getirilmektedir.
IEnumerator Interface’i
IEnumerable interface’i ile bir sınıf itere edilebilir hale getiriliyor, bu işlem içinde GetEnumerator metodu sınıfa implement ediliyordu. IEnumerator interface’i ise iterasyon özelliği kazandıracak ve iterasyon işleminde kullanılacak elemanları ve özellikleri barındırmaktadır.
Bu elemanlar/özellikler;
	Current
İterasyon’da kalınan yeri temsil eder.
	MoveNext
İterasyon’da bir sonraki adım var mı?/yok mu? kontrolünü sağlar.
	Reset
İterasyon’u başa alır.
	Dispose
İterasyon’un bittiğini temsil eder.
 
Bu görsel http://www.yazilimgunlugu.com/ienumerable-ve-ienumerator-ara-yuzleri-ve-kullanimi-makalesi/738.aspx adresinden alınmıştır.
Biz şuana kadar ilgili sınıfımız içerisinde kullandığımız veri kümesinin GetEnumerator metodu aracılığıyla enumerator’umüzü elde ettik. Haydi gelin şimdi de kendi enumerator’umüzü yazalım.
01
02
03
04
05
06
07
08
09
10
11	class PersonelEnumerator : IEnumerator<Personel>
{
    List<Personel> Kaynak;
    int currentIndex = -1;
    public PersonelEnumerator(List<Personel> Kaynak) => this.Kaynak = Kaynak;
    public Personel Current => Kaynak[currentIndex];
    object IEnumerator.Current => Kaynak[currentIndex];
    public void Dispose() => Console.WriteLine("İterasyon bittiii...");
    public bool MoveNext() => ++currentIndex < Kaynak.Count;
    public void Reset() => currentIndex = 0;
}
Ve “Personeller” sınıfındaki GetEnumerator metodundan kendi yazdığımız enumerator nesnesini dönelim.
14	class Personel
{
    public int Id { get; set; }
    public string Adi { get; set; }
    public string SoyAdi { get; set; }
}
 
class Personeller : IEnumerable<Personel>
{
    List<Personel> PersonelListesi = new List<Personel>();
    public void Add(Personel p) => PersonelListesi.Add(p);
    public IEnumerator<Personel> GetEnumerator() => new PersonelEnumerator(PersonelListesi);
    IEnumerator IEnumerable.GetEnumerator() => new PersonelEnumerator(PersonelListesi);
}
Sonuç;
 
IEnumerable ve IEnumerator interfaceler’i ile sizlerde oluşturduğunuz sınıflara itere özellikleri kazandırabilir, hatta IEnumerator interface’i ile oluşturduğunuz enumerator’de isteğinize göre iterasyonun periyodunu ayarlayabilir ve foreach döngüsünde kullanabilirsiniz.
Sonraki yazılarımda görüşmek üzere…
İyi çalışmalar…
-
-
-
css tarafında değişkenler oluşturup işi kolaylaştıran teknoloji.

LESS isn’t the only CSS preprocessor out there. Syntactically Awesome Stylesheets (Sass) is another popular one (sass-lang.com), for just one example. 
-
-
-
In this article, I’ll discuss Web development using the LESS Framework for dynamic generation of CSS content.
No doubt that CSS represented a big leap forward with its promise—largely fulfilled—of completely separating content from presentation of Web pages. Even though CSS is (or should be?) in the realm of designers, it heralds the principle of separation of concerns, to which nearly every developer is sensitive. So use of CSS picked up quickly and has become deeply ingrained into Web development to the point that it sometimes struggles to keep up with modern Web sites.
The point isn’t that CSS is insufficient to style modern, graphically rich and appealing Web sites, but rather a purely declarative language isn’t always appropriate to express complex and interconnected declarations of styles. Thankfully, browsers can still make sense of any CSS as long as it’s correctly written—but can we say the same for humans?
A relatively new direction in Web development aims to build an infrastructure around CSS so developers and designers can produce the same CSS in a more sustainable way. The final stylesheet for the browser doesn’t change, but the manner in which it’s produced should be different, easier to read and more manageable.
This field of Web development started a few years ago and is now reaching maturity, as several available frameworks can help you out with dynamic CSS content generation. I’ll provide an executive summary of one of these frameworks—the LESS Framework—and show how it can be integrated with ASP.NET MVC solutions.
Why LESS?
One of the biggest issues developers address with LESS is repetition of information. As a software developer, you probably know the “don’t repeat yourself” (DRY) principle and apply it every day. The major benefit of DRY is that it reduces the places where the same information is stored, and thus the number of places where it should be updated.
In plain CSS, you simply have no DRY issues. For example, in some other scenarios, if some color is used in multiple classes and you have to change it, you likely have no better way than updating it in every single occurrence. CSS classes let you define the appear¬ance of certain elements and reuse them across pages to style related elements in the same way. While CSS classes certainly reduce repetition, they’re sometimes insufficient in other aspects.
One problem with CSS classes is they operate at the level of the semantic HTML element. In building various CSS classes, you often face repetition of small pieces of information such as colors or widths. You can’t easily have a class for each of these repeatable small pieces. Even if you manage to have a CSS class for nearly any repeatable style, such as colors and widths, then when it comes to style the semantic element—say, a container—you should concatenate multiple CSS classes together to achieve the desired effect.
If you’ve ever used a framework such as Bootstrap for designing your Web page, you know what I mean. Here’s an example:
XMLCopy
<a class="btn btn-primary" ... />
The anchor is first set to be a button (class btn) and then a particular flavor of button (class btn-primary). This approach works, but it might require some significant work to plan ahead for the classes you need. It results in overhead in Web projects that often are on the brink of deadlines.
A dynamic stylesheet language such as LESS represents a sort of lateral thinking. You don’t spend any time trying to make your plain CSS smarter; you simply use different tools—mostly languages—to generate it. LESS, therefore, is a framework that adds programmer-friendly concepts to CSS coding, such as variables, blocks and functions.
Strictly related to dynamic CSS generation is the problem of processing it to plain CSS for the browser to consume. The client can process the LESS code through ad hoc JavaScript code or preprocess it on the server so the browser just receives final CSS.
Setting up LESS in ASP.NET MVC
I’ll demonstrate what it takes to use LESS from within an ASP.NET MVC application. To start out, I’ll focus on client-side processing of the LESS code. In the HEAD section of the layout file, add the following:
XMLCopy
<link rel="stylesheet/less"
  type="text/css"
 href="@Url.Content("~/content/less/mysite.less")" />
<script type="text/javascript"
 src="@Url.Content("~/content/scripts/less-1.3.3.min.js")"></script>
This assumes you’ve created a Content/Less folder in the project to contain all your LESS files. You’ll need a JavaScript file to do the actual LESS-to-CSS processing within the browser. You can get the script file from lesscss.org. I’ll review a few scenarios where LESS proves useful.
LESS in Action: Variables
A good way to understand the role of LESS variables is by looking into CSS gradients. For years, designers used small GIF files to paint the background of HTML containers with gradients. More recently, browsers added CSS support for gradients. These are also part of the official CSS3 standard through the linear-gradient syntax and its variations. Unfortunately, if you want to make sure the gradient is picked up by the widest possible range of browsers, you have to resort to something like the code in Figure 1.
The code in Figure 1 is nearly unreadable. Worse yet, it must be repeated anywhere you want that gradient. Also, if you want to change the gradient color slightly (or simply the saturation or fading), the only option is editing all occurrences manually. Without beating around the bush, this can be extremely tough. However, it’s the only way it can work in plain CSS.
Figure 1 Comprehensive Code for Displaying Gradients on a Wide Range of Browsers
XMLCopy
/* Old browsers fallback */
background-color: #ff0000;
background: url(images/red_gradient.png);
background-repeat: repeat-x;
/* Browser specific syntax */
background: -moz-linear-gradient(  left,  #fceabb 0%, 
  #fccd4d 50%, #f8b500 51%, #fbdf93 100%);
background: -Webkit-linear-gradient(  left, #fceabb 0%,
  #fccd4d 50%,#f8b500 51%,#fbdf93 100%);
background: -o-linear-gradient(  left, #fceabb 0%,
  #fccd4d 50%,#f8b500 51%,#fbdf93 100%);
background: -ms-linear-gradient(  left, #fceabb 0%,
  #fccd4d 50%,#f8b500 51%,#fbdf93 100%);
/* Standard syntax */
background: linear-gradient(  to right, #fceabb 0%,
  #fccd4d 50%,#f8b500 51%,#fbdf93 100%);
To find a better solution, you need to look outside CSS and enter the territory of LESS. In LESS, you define the CSS for the gradient once and refer to it by name wherever appropriate. Here’s an example:
XMLCopy
.background-gradient-orange { background: #fceabb; ... }
.container { .background-gradient-orange; }
The class named background-gradient-orange is embedded by name in the class container and any other class where appropriate. The definition of the gradient, though, is kept in one place.
There’s nothing revolutionary in this if you look at it from a developer’s perspective. However, it uses a feature—variables—that just doesn’t exist in CSS. The preceding syntax, in fact, won’t work if you save and reference the file as a plain stylesheet. Some code is required to turn the extended syntax into plain CSS. The LESS JavaScript parser does just this and expands variables to their actual CSS content.
Variables apply also to scalar values such as colors or sizes. Consider the following LESS code:
XMLCopy
@black: #111;
#main {  color: @black; }
.header { background-color: @black; }
The parser expands the @black variable to the assigned value and replaces it all over the file. The net effect is that you change the actual color in one place and changes ripple through the file automatically.
LESS in Action: Imports
You can split your LESS code across multiple files, reference files and contained classes where necessary. Suppose, for example, you create a gradients.less file with the following content:
XMLCopy
.background-gradient-orange { background: #fceabb; ... }
In another LESS file, say main.less, you can reference any gradients by importing the file:
XMLCopy
@import "gradients";
.container { .background-gradient-orange; }
If gradients.less (the extension isn’t strictly required) lives in a different folder, you should indicate path information in the call to import.
LESS Mixins
I called the LESS artifact for gradients a variable. To be picky, that’s not entirely correct. In LESS, a variable encompasses a single value. A container for a CSS class is known as a mixin. This is similar to a function but doesn’t contain any custom logic. Just like a function, a LESS mixin can take and process parameters. Consider the code in Figure 2 that demonstrates a mixin.
In Figure 2, a mixin named shadow defines the styles for a box shadow and exposes the color as an external parameter. Similarly, the text-box mixin defines the basic appearance of an input field. It imports the shadow definition and keeps the width parametric. In this way, defining three classes for input fields of different sizes (mini, normal and large) is a breeze. More important, it requires a fraction of the editing and takes only minimal effort to update (see Figure 3).
Figure 2 Mixins in the LESS Framework
XMLCopy
/*  Mixins  */
.shadow(@color) {
  box-shadow: 3px 3px 2px @color;
}
.text-box(@width) {
  .shadow(#555);
  border: solid 1px #000;
  background-color: #dddd00;
  padding: 2px;
  width: @width;
}
/*  CSS classes  */
.text-box-mini {
  .text-box(50px);
}
.text-box-normal {
  .text-box(100px);
}
.text-box-large {
  .text-box(200px);
}
 
Figure 3 LESS Mixins in Action
Mixins can accept multiple parameters and also a variable number of parameters. In addition, individual parameters support default values:
XMLCopy
.mixin(@color: #ff0) { ... }
LESS isn’t an expression of a rich programming language, and by design it lacks commands to indicate conditions or loops. However, the behavior of a mixin can still be different depending on the passed value. Suppose you want to give a larger button a thicker border and font. You define a parametric mixin named button and use the keyword “when” to bind settings to a condition. The condition must be based on a single parameter:
XMLCopy
.button (@size) when (@size < 100px) {
 padding: 3px;
 font-size: 0.7em;
 width: @size *2;
}
.button (@size) when (@size >= 100px) {
  padding: 10px;
  font-size: 1.0em;
  font-weight: 700;
  background-color: red;
  width: @size *3;
}
You apply different settings, but you also can use basic operations to multiply the size by a factor. Next, use mixins in actual CSS classes:
XMLCopy
.push-button-large {
  .button(150px);
}
.push-button-small {
  .button(80px);
}
The results of running this code are shown in Figure 4.
 
Figure 4 Effects of Using LESS Mixins in CSS Classes
LESS comes with a long list of predefined functions for manipulating colors. You have functions to darken, lighten and saturate colors by a percentage and fade colors in and out by a percentage, as shown here:
XMLCopy
.push-button {
  background-color: fade(red, 30%);
}
For full documentation about the functions supported by LESS, check out lesscss.org.
Nesting Classes
Personally, I find rather annoying the need to repeat CSS blocks to indicate sibling styles. A typical example is:
XMLCopy
#container h1 { ... }
#container p { ... }
#container p a { ... }
#container img { ... }
In well-written plain CSS, you can actually avoid much of the repetition, but the manner in which styles are laid out—using a flat listing—isn’t optimal. In this case, a bit of a hierarchy is preferable. In LESS, you can nest style rules like this:
XMLCopy
.container {
  h1 {
    font-size: 0.8em;
   color: fade(#333, 30%);
   a {
     color: #345;
     &:hover {color: red;}
    }
  }
}
Once processed, the preceding LESS code produces the following styles:
XMLCopy
.container h1
.container h1 a
.container h1a:hover
Server-Side Processing
You can download the LESS code as is and process it on the client through JavaScript code. You can also preprocess on the server and download it to the client as plain CSS. In the former case, everything works as if you were using plain CSS files: Server-side changes are applied to the client with the next page refresh.
Server-side preprocessing might be a better option if you have performance concerns and are dealing with large, complex CSS files. Server-side preprocessing takes place every time you modify the CSS on the server. You can manually take care of the extra step at the end of the build process. You preprocess LESS code to CSS using the LESS compiler from the command line. The compiler is part of the dotless NuGet package you install for server-side work.
In ASP.NET MVC 4, however, you can integrate the LESS Framework with the bundle mechanism covered in my October 2013 column, “Programming CSS: Bundling and Minification” (msdn.microsoft.com/magazine/dn451436). This ensures the LESS-to-CSS transformation is performed whenever you make a request for a LESS file. It also ensures caching is managed properly via the If-Modified-Since header. Finally, you can mix together parsing and minifying. To integrate LESS in ASP.NET MVC, first download and install the dotless NuGet package. Second, add the following code to the BundleConfig class:
XMLCopy
var lessBundle =
  new Bundle("~/myless").IncludeDirectory("~/content/less", "*.less");
lessBundle.Transforms.Add(new LessTransform());
lessBundle.Transforms.Add(new CssMinify());
bundles.Add(lessBundle);
The bundle will package all .less files found in the specified folder. The LessTransform class is responsible for the LESS-to-CSS transformation. The class uses the dotless API to parse LESS scripts. The code for LessTransform is fairly simple:
XMLCopy
public class LessTransform : IBundleTransform
{
  public void Process(BundleContext context, BundleResponse response)
  {
    response.Content = dotless.Core.Less.Parse(response.Content);
    response.ContentType = "text/css";
  }
}
More Intelligent Tools
LESS isn’t the only CSS preprocessor out there. Syntactically Awesome Stylesheets (Sass) is another popular one (sass-lang.com), for just one example. The bottom line is that regardless of the tool, a CSS preprocessor is definitely something you want to consider for extensive Web programming. Whether you’re a graphic designer or a developer, more intelligent tools to manage and organize CSS code are almost a necessity. They’re even better when they’re also integrated into the Web platform. Finally, note that Visual Studio 2012 and Visual Studio 2013 offer excellent support for LESS (and related technologies) through the Web Essentials extension, which you can download at vswebessentials.com. Also, the LESS editor is available in Visual Studio 2012 Update 2 and in the upcoming Visual Studio 2013.
________________________________________
Dino Esposito is the author of “Architecting Mobile Solutions for the Enterprise” (Microsoft Press, 2012) and the upcoming “Programming ASP.NET MVC 5” (Microsoft Press). A technical evangelist for the .NET and Android platforms at JetBrains and frequent speaker at industry events worldwide, Esposito shares his vision of software at software2cents.wordpress.com and on Twitter at twitter.com/despos.
Thanks to the following technical expert for reviewing this article: Mads Kristensen (Microsoft)
Mads Kristensen is a Program Manager on the Web Platforms & Tools team at Microsoft, working on the Web developer experiences of Visual Studio. He has more than a decade of experience in developing Web applications on the Microsoft platform. He founded the BlogEngine.NET open source project, which became the most popular blog application on the ASP.NET platform, used by 800,000 people worldwide. Kristnsen is also the creator of some popular Visual Studio extension including Web Essentials, Image Optimizer and CssCop.
-
-
LINQ  (GITHUB)
Linq nedir
Language-Integrated Query (LINQ) is the name for a set of technologies based on the integration of query capabilities directly into the C# language. Traditionally, queries against data are expressed as simple strings without type checking at compile time or IntelliSense support. Furthermore, you have to learn a different query language for each type of data source: SQL databases, XML documents, various Web services, and so on. With LINQ, a query is a first-class language construct, just like classes, methods, events. You write queries against strongly typed collections of objects by using language keywords and familiar operators. The LINQ family of technologies provides a consistent query experience for objects (LINQ to Objects), relational databases (LINQ to SQL), and XML (LINQ to XML).
For a developer who writes queries, the most visible "language-integrated" part of LINQ is the query expression. Query expressions are written in a declarative query syntax. By using query syntax, you can perform filtering, ordering, and grouping operations on data sources with a minimum of code. You use the same basic query expression patterns to query and transform data in SQL databases, ADO .NET Datasets, XML documents and streams, and .NET collections.
You can write LINQ queries in C# for SQL Server databases, XML documents, ADO.NET Datasets, and any collection of objects that supports IEnumerable or the generic IEnumerable<T> interface. LINQ support is also provided by third parties for many Web services and other database implementations.
The following example shows the complete query operation. The complete operation includes creating a data source, defining the query expression, and executing the query in a foreach statement.
C#Copy
class LINQQueryExpressions
{
    static void Main()
    {
        
        // Specify the data source.
        int[] scores = new int[] { 97, 92, 81, 60 };

        // Define the query expression.
        IEnumerable<int> scoreQuery =
            from score in scores
            where score > 80
            select score;

        // Execute the query.
        foreach (int i in scoreQuery)
        {
            Console.Write(i + " ");
        }            
    }
}
// Output: 97 92 81
The following illustration from Visual Studio shows a partially-completed LINQ query against a SQL Server database in both C# and Visual Basic with full type checking and IntelliSense support:
 
Query expression overview
•	Query expressions can be used to query and to transform data from any LINQ-enabled data source. For example, a single query can retrieve data from a SQL database, and produce an XML stream as output.
•	Query expressions are easy to master because they use many familiar C# language constructs.
•	The variables in a query expression are all strongly typed, although in many cases you do not have to provide the type explicitly because the compiler can infer it. For more information, see Type relationships in LINQ query operations.
•	A query is not executed until you iterate over the query variable, for example, in a foreach statement. For more information, see Introduction to LINQ queries.
•	At compile time, query expressions are converted to Standard Query Operator method calls according to the rules set forth in the C# specification. Any query that can be expressed by using query syntax can also be expressed by using method syntax. However, in most cases query syntax is more readable and concise. For more information, see C# language specification and Standard query operators overview.
•	As a rule when you write LINQ queries, we recommend that you use query syntax whenever possible and method syntax whenever necessary. There is no semantic or performance difference between the two different forms. Query expressions are often more readable than equivalent expressions written in method syntax.
•	Some query operations, such as Count or Max, have no equivalent query expression clause and must therefore be expressed as a method call. Method syntax can be combined with query syntax in various ways. For more information, see Query syntax and method syntax in LINQ.
•	Query expressions can be compiled to expression trees or to delegates, depending on the type that the query is applied to. IEnumerable<T> queries are compiled to delegates. IQueryable and IQueryable<T> queries are compiled to expression trees. For more information, see Expression trees.


using System;
using System.Collections;
using System.Collections.Generic;
using System.Linq;
 
class Program
{
    static void Main(string[] args)
    {
        Console.WriteLine("Hello World!");
 
        // Specify the data source.
 
        int[] scores = new int[] { 97, 92, 81, 60 };
 
        // Define the query expression.
        // Burada List<int> yazamayız. listemiz IEnumerable olmalıdır:
 
        IEnumerable<int> scoreQuery =
            from score in scores
            where score > 80
            select score;
 
        // Execute the query:
 
        foreach (int i in scoreQuery)
        {
            Console.WriteLine(i + "(scoreQuery");
        }
 
        // Yukarıda Ienumerable bir değişkenin içinde döndüğümüze dikkat edelim. Yani toList işlemi yapmadan içinde dönmüş olduk.
 
        // Yukarıdaki IEnumerable olan scoreQuery üzerinde ToList işlemi yapabiliriz ve oluşacak yeni değeri bir değişkene atayabiliriz
        // Aşağıda bunu yapıyoruz:
 
        List<int> toListtenGelenList = scoreQuery.ToList();
 
        // ToList metodunun ne yaptığını okuyalım şunu yapıyor, çok önemli:
 
        // Ienumerable<out T> tipinden List<T> tipine dönüşüm sağlıyor. --> english -->
        // Creates a List<T> from an IEnumerable<T>.
 
        // şimdi tolist ile listeye dönüştürdüğümüz değişkenin içinde dönelim ve değerleri ekrana yazdıralım:
 
        foreach (var item in toListtenGelenList)
        {
            Console.WriteLine(item + "(toListtenGelenList)");
        }
 
        Console.ReadKey();
    }
}

LINQ neden var? (GITHUB)
A query is an expression that retrieves data from a data source. Queries are usually expressed in a specialized query language. Different languages have been developed over time for the various types of data sources, for example SQL for relational databases and XQuery for XML. Therefore, developers have had to learn a new query language for each type of data source or data format that they must support. LINQ simplifies this situation by offering a consistent model for working with data across various kinds of data sources and formats. In a LINQ query, you are always working with objects. You use the same basic coding patterns to query and transform data in XML documents, SQL databases, ADO.NET Datasets, .NET collections, and any other format for which a LINQ provider is available.
Örnek  (GITHUB)


using System;
using System.Collections;
using System.Collections.Generic;
using System.Linq;

class Program
{
    static void Main()
    {
        // The Three Parts of a LINQ Query:
        // 1. Data source.
        int[] numbers = new int[] { 0, 1, 2, 3, 4, 5, 6 };
 
        // 2. Query creation.
        // numQuery is an IEnumerable<int>
        var numQuery =
            from num in numbers
            where (num % 2) == 0    // burada parantezler kullanılmasa da olur, redundant.
            select num;             // where şartına uyanları "select" ile seçtik. Select'in ne yaptığı kafa karıştırıyorsa şöyle düşünülebilir:
 
        // zaten select'in öncesinde where clause var. Yani bir şarta uyan şeyler dönecek yukarıda. E bu durumda select ne yapabilir? Tabi ki bu datayı "seçer".
        // her zaman where olacak diye bir kural yok. Fikirsel açıdan yazıldı sadece bu öneri.
 
        // DİKKAT. Yukarıda numbers değişkeni int array'ı tipindeydi. Yani IEnumerable tipinde değildi. Peki nasıl IEnumerable olmayan bir değişkeni sorgu yazarken kullanabildik? Cevap:
 
        // In the previous example, because the data source is an array, it implicitly supports the generic IEnumerable<T> interface. This fact means it can be queried with LINQ. A query is executed in a foreach statement, and foreach requires IEnumerable or IEnumerable<T>. Types that support IEnumerable<T> or a derived interface such as the generic IQueryable<T> are called queryable types.
 
        // 3. Query execution.
        foreach (int num in numQuery)
        {
            Console.Write("{0} ", num);
        }
    }
}

-
-
Veri yapısı, bilgisayar ortamında verilerin etkin olarak saklanması ve işlenmesi için kullanılan yapı.
Veri yapıları, verilerin düzenlenme biçimini belirleyen yapıtaşlarıdır. Bir yazılım değişkeni bile basit bir veri yapısı olarak kabul edilebilir. Değişik algoritmalarda verilerin diziler, listeler, yığıtlar, kuyruklar, ağaçlar ve çizgeler gibi veri modellerine uydurularak düzenlenmesi gerekebilir. Veri, yapı ve algoritma bir yazılımın birbirinden ayrılmaz bileşenleridir. Algoritması hazırlanmış her yapı için verilerin düzenli bir şekilde kullanımı önemlidir. Çünkü yapı iyi kurulduğunda, etkin, doğru, anlaşılır ve hızlı çalışıp az kaynak kullanan algoritma geliştirmek kolaylaşır.
8 Common Data Structures every Programmer must know
A quick introduction to 8 commonly used data structures

 


Vijini Mallawaarachchi


Feb 28, 2020·11 min read

Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. Data structures have a wide and diverse scope of usage across the fields of Computer Science and Software Engineering.
 
 
Image by author
Data structures are being used in almost every program or software system that has been developed. Moreover, data structures come under the fundamentals of Computer Science and Software Engineering. It is a key topic when it comes to Software Engineering interview questions. Hence as developers, we must have good knowledge about data structures.
In this article, I will be briefly explaining 8 commonly used data structures every programmer must know.
1. Arrays
An array is a structure of fixed-size, which can hold items of the same data type. It can be an array of integers, an array of floating-point numbers, an array of strings or even an array of arrays (such as 2-dimensional arrays). Arrays are indexed, meaning that random access is possible.

 
Fig 1. Visualization of basic Terminology of Arrays (Image by author)
Array operations
•	Traverse: Go through the elements and print them.
•	Search: Search for an element in the array. You can search the element by its value or its index
•	Update: Update the value of an existing element at a given index
Inserting elements to an array and deleting elements from an array cannot be done straight away as arrays are fixed in size. If you want to insert an element to an array, first you will have to create a new array with increased size (current size + 1), copy the existing elements and add the new element. The same goes for the deletion with a new array of reduced size.
Applications of arrays
•	Used as the building blocks to build other data structures such as array lists, heaps, hash tables, vectors and matrices.
•	Used for different sorting algorithms such as insertion sort, quick sort, bubble sort and merge sort.
2. Linked Lists
A linked list is a sequential structure that consists of a sequence of items in linear order which are linked to each other. Hence, you have to access data sequentially and random access is not possible. Linked lists provide a simple and flexible representation of dynamic sets.
Let’s consider the following terms regarding linked lists. You can get a clear idea by referring to Figure 2.
•	Elements in a linked list are known as nodes.
•	Each node contains a key and a pointer to its successor node, known as next.
•	The attribute named head points to the first element of the linked list.
•	The last element of the linked list is known as the tail.
 
 
Fig 2. Visualization of basic Terminology of Linked Lists (Image by author)
Following are the various types of linked lists available.
•	Singly linked list — Traversal of items can be done in the forward direction only.
•	Doubly linked list — Traversal of items can be done in both forward and backward directions. Nodes consist of an additional pointer known as prev, pointing to the previous node.
•	Circular linked lists — Linked lists where the prev pointer of the head points to the tail and the next pointer of the tail points to the head.
Linked list operations
•	Search: Find the first element with the key k in the given linked list by a simple linear search and returns a pointer to this element
•	Insert: Insert a key to the linked list. An insertion can be done in 3 different ways; insert at the beginning of the list, insert at the end of the list and insert in the middle of the list.
•	Delete: Removes an element x from a given linked list. You cannot delete a node by a single step. A deletion can be done in 3 different ways; delete from the beginning of the list, delete from the end of the list and delete from the middle of the list.
Applications of linked lists
•	Used for symbol table management in compiler design.
•	Used in switching between programs using Alt + Tab (implemented using Circular Linked List).
3. Stacks
A stack is a LIFO (Last In First Out — the element placed at last can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as “stack” because it resembles a real-world stack — a stack of plates.

 
Image by congerdesign from Pixabay
Stack operations
Given below are the 2 basic operations that can be performed on a stack. Please refer to Figure 3 to get a better understanding of the stack operations.
•	Push: Insert an element on to the top of the stack.
•	Pop: Delete the topmost element and return it.

 
Fig 3. Visualization of basic Operations of Stacks (Image by author)
Furthermore, the following additional functions are provided for a stack in order to check its status.
•	Peek: Return the top element of the stack without deleting it.
•	isEmpty: Check if the stack is empty.
•	isFull: Check if the stack is full.
Applications of stacks
•	Used for expression evaluation (e.g.: shunting-yard algorithm for parsing and evaluating mathematical expressions).
•	Used to implement function calls in recursion programming.
4. Queues
A queue is a FIFO (First In First Out — the element placed at first can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as “queue” because it resembles a real-world queue — people waiting in a queue.

 
Image by Sabine Felidae from Pixabay
Queue operations
Given below are the 2 basic operations that can be performed on a queue. Please refer to Figure 4 to get a better understanding of the queue operations.
•	Enqueue: Insert an element to the end of the queue.
•	Dequeue: Delete the element from the beginning of the queue.

 
Fig 4. Visualization of Basic Operations of Queues (Image by author)
Applications of queues
•	Used to manage threads in multithreading.
•	Used to implement queuing systems (e.g.: priority queues).
5. Hash Tables
A Hash Table is a data structure that stores values which have keys associated with each of them. Furthermore, it supports lookup efficiently if we know the key associated with the value. Hence it is very efficient in inserting and searching, irrespective of the size of the data.
Direct Addressing uses the one-to-one mapping between the values and keys when storing in a table. However, there is a problem with this approach when there is a large number of key-value pairs. The table will be huge with so many records and may be impractical or even impossible to be stored, given the memory available on a typical computer. To avoid this issue we use hash tables.
Hash Function
A special function named as the hash function (h) is used to overcome the aforementioned problem in direct addressing.
In direct accessing, a value with key k is stored in the slot k. Using the hash function, we calculate the index of the table (slot) to which each value goes. The value calculated using the hash function for a given key is called the hash value which indicates the index of the table to which the value is mapped.
h(k) = k % m
•	h: Hash function
•	k: Key of which the hash value should be determined
•	m: Size of the hash table (number of slots available). A prime value that is not close to an exact power of 2 is a good choice for m.

 
Fig 5. Representation of a Hash Function (Image by author)
Consider the hash function h(k) = k % 20, where the size of the hash table is 20. Given a set of keys, we want to calculate the hash value of each to determine the index where it should go in the hash table. Consider we have the following keys, the hash and the hash table index.
•	1 → 1%20 → 1
•	5 → 5%20 → 5
•	23 → 23%20 → 3
•	63 → 63%20 → 3
From the last two examples given above, we can see that collision can arise when the hash function generates the same index for more than one key. We can resolve collisions by selecting a suitable hash function h and use techniques such as chaining and open addressing.
Applications of hash tables
•	Used to implement database indexes.
•	Used to implement associative arrays.
•	Used to implement the “set” data structure.
6. Trees
A tree is a hierarchical structure where data is organized hierarchically and are linked together. This structure is different from a linked list whereas, in a linked list, items are linked in a linear order.
Various types of trees have been developed throughout the past decades, in order to suit certain applications and meet certain constraints. Some examples are binary search tree, B tree, treap, red-black tree, splay tree, AVL tree and n-ary tree.
Binary Search Trees
A binary search tree (BST), as the name suggests, is a binary tree where data is organized in a hierarchical structure. This data structure stores values in sorted order.
Every node in a binary search tree comprises the following attributes.
1.	key: The value stored in the node.
2.	left: The pointer to the left child.
3.	right: The pointer to the right child.
4.	p: The pointer to the parent node.
A binary search tree exhibits a unique property that distinguishes it from other trees. This property is known as the binary-search-tree property.
Let x be a node in a binary search tree.
•	If y is a node in the left subtree of x, then y.key ≤ x.key
•	If y is a node in the right subtree of x, then y.key ≥ x.key

 
Fig 6. Visualization of Basic Terminology of Trees (Image by author)
Applications of trees
•	Binary Trees: Used to implement expression parsers and expression solvers.
•	Binary Search Tree: used in many search applications where data are constantly entering and leaving.
•	Heaps: used by JVM (Java Virtual Machine) to store Java objects.
•	Treaps: used in wireless networking.
Check my articles below on 8 useful tree data structures and self-balancing binary search trees.

8 Useful Tree Data Structures Worth Knowing
An overview of 8 different tree data structures
towardsdatascience.com


Self-Balancing Binary Search Trees 101
Introduction to Self-Balancing Binary Search Trees
towardsdatascience.com

7. Heaps
A Heap is a special case of a binary tree where the parent nodes are compared to their children with their values and are arranged accordingly.
Let us see how we can represent heaps. Heaps can be represented using trees as well as arrays. Figures 7 and 8 show how we can represent a binary heap using a binary tree and an array.

 
Fig 7. Binary Tree Representation of a Heap (Image by author)

 
Fig 8. Array Representation of a Heap (Image by author)
Heaps can be of 2 types.
1.	Min Heap — the key of the parent is less than or equal to those of its children. This is called the min-heap property. The root will contain the minimum value of the heap.
2.	Max Heap — the key of the parent is greater than or equal to those of its children. This is called the max-heap property. The root will contain the maximum value of the heap.
Applications of heaps
•	Used in heapsort algorithm.
•	Used to implement priority queues as the priority values can be ordered according to the heap property where the heap can be implemented using an array.
•	Queue functions can be implemented using heaps within O(log n) time.
•	Used to find the kᵗʰ smallest (or largest) value in a given array.
Check my article below on implementing a heap using the python heapq module.


8. Graphs
A graph consists of a finite set of vertices or nodes and a set of edges connecting these vertices.
The order of a graph is the number of vertices in the graph. The size of a graph is the number of edges in the graph.
Two nodes are said to be adjacent if they are connected to each other by the same edge.
Directed Graphs
A graph G is said to be a directed graph if all its edges have a direction indicating what is the start vertex and what is the end vertex.
We say that (u, v) is incident from or leaves vertex u and is incident to or enters vertex v.
Self-loops: Edges from a vertex to itself.
Undirected Graphs
A graph G is said to be an undirected graph if all its edges have no direction. It can go in both ways between the two vertices.
If a vertex is not connected to any other node in the graph, it is said to be isolated.

 
Fig 9. Visualization of Terminology of Graphs (Image by author)
You can read more about graph algorithms from my article 10 Graph Algorithms Visually Explained.

10 Graph Algorithms Visually Explained
A quick introduction to 10 basic graph algorithms with examples and visualisations
medium.com

Applications of graphs
•	Used to represent social media networks. Each user is a vertex, and when users connect they create an edge.
•	Used to represent web pages and links by search engines. Web pages on the internet are linked to each other by hyperlinks. Each page is a vertex and the hyperlink between two pages is an edge. Used for Page Ranking in Google.
•	Used to represent locations and routes in GPS. Locations are vertices and the routes connecting locations are edges. Used to calculate the shortest route between two locations.
Final Thoughts
A cheat sheet for the time complexities of the data structure operations can be found in this link. Moreover, check out my article below where I have implemented a few common data structures from scratch using C++.

Data Types: Structured vs. Unstructured Data
In computer science, a data structure is a particular way of organising and storing data in a computer such that it can be accessed and modified efficiently. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.
Three different data structures
For the analysis of data, it is important to understand that there are three common types of data structures:
 
Structured Data
Structured data is data that adheres to a pre-defined data model and is therefore straightforward to analyse. Structured data conforms to a tabular format with relationship between the different rows and columns. Common examples of structured data are Excel files or SQL databases. Each of these have structured rows and columns that can be sorted.
Structured data depends on the existence of a data model – a model of how data can be stored, processed and accessed. Because of a data model, each field is discrete and can be accesses separately or jointly along with data from other fields. This makes structured data extremely powerful: it is possible to quickly aggregate data from various locations in the database.
Structured data is is considered the most ‘traditional’ form of data storage, since the earliest versions of database management systems (DBMS) were able to store, process and access structured data.
Unstructured Data
Unstructured data is information that either does not have a predefined data model or is not organised in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in structured databases. Common examples of unstructured data include audio, video files or No-SQL databases.
The ability to store and process unstructured data has greatly grown in recent years, with many new technologies and tools coming to the market that are able to store specialised types of unstructured data. MongoDB, for example, is optimised to store documents. Apache Giraph, as an opposite example, is optimised for storing relationships between nodes.
The ability to analyse unstructured data is especially relevant in the context of Big Data, since a large part of data in organisations is unstructured. Think about pictures, videos or PDF documents. The ability to extract value from unstructured data is one of main drivers behind the quick growth of Big Data.
Semi-structured Data
Semi-structured data is a form of structured data that does not conform with the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contain tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure. Examples of semi-structured data include JSON and XML are forms of semi-structured data.
The reason that this third category exists (between structured and unstructured data) is because semi-structured data is considerably easier to analyse than unstructured data. Many Big Data solutions and tools have the ability to ‘read’ and process either JSON or XML. This reduces the complexity to analyse structured data, compared to unstructured data.
Metadata – Data about Data
A last category of data type is metadata. From a technical point of view, this is not a separate data structure, but it is one of the most important elements for Big Data analysis and big data solutions. Metadata is data about data. It provides additional information about a specific set of data.
In a set of photographs, for example, metadata could describe when and where the photos were taken. The metadata then provides fields for dates and locations which, by themselves, can be considered structured data. Because of this reason, metadata is frequently used by Big Data solutions for initial analysis.
What is Structured Data?
Structured data is the data which conforms to a data model, has a well define structure, follows a consistent order and can be easily accessed and used by a person or a computer program.
Structured data is usually stored in well-defined schemas such as Databases. It is 	
SQL (Structured Query language) is often used to manage structured data stored in databases.
Characteristics of Structured Data:
•	Data conforms to a data model and has easily identifiable structure
•	Data is stored in the form of rows and columns
Example : Database
•	Data is well organised so, Definition, Format and Meaning of data is explicitly known
•	Data resides in fixed fields within a record or file
•	Similar entities are grouped together to form relations or classes
•	Entities in the same group have same attributes
•	Easy to access and query, So data can be easily used by other programs
•	Data elements are addressable, so efficient to analyse and process
Sources of Structured Data:
•	SQL Databases
•	Spreadsheets such as Excel
•	OLTP Systems
•	Online forms
•	Sensors such as GPS or RFID tags
•	Network and Web server logs
•	Medical devices
Advantages of Structured Data:
•	Structured data have a well defined structure that helps in easy storage and access of data
•	Data can be indexed based on text string as well as attributes. This makes search operation hassle-free
•	Data mining is easy i.e knowledge can be easily extracted from data
•	Operations such as Updating and deleting is easy due to well structured form of data
•	Business Intelligence operations such as Data warehousing can be easily undertaken
•	Easily scalable in case there is an increment of data
•	Ensuring security to data is easy
Note: Structured data accounts for only about 20% of data but because of its high degree of organisation and performance make it foundation of Big data
To read Differences between Structured, Semi-structured and Unstructured data refer the following article –
•	Difference between Structured, Semi-structured and Unstructured data
Attention reader! Don’t stop learning now. Get hold of all the important CS Theory concepts for SDE interviews with the CS Theory Course at a student-friendly price and become industry ready.
-
Difference between Structured, Semi-structured and Unstructured data
Last Updated: 18-08-2020
Big Data includes huge volume, high velocity, and extensible variety of data. These are 3 types: Structured data, Semi-structured data, and Unstructured data. 
 
1.	Structured data – 
Structured data is data whose elements are addressable for effective analysis. It has been organized into a formatted repository that is typically a database. It concerns all data which can be stored in database SQL in a table with rows and columns. They have relational keys and can easily be mapped into pre-designed fields. Today, those data are most processed in the development and simplest way to manage information. Example: Relational data. 
 
2.	Semi-Structured data – 
Semi-structured data is information that does not reside in a relational database but that have some organizational properties that make it easier to analyze. With some process, you can store them in the relation database (it could be very hard for some kind of semi-structured data), but Semi-structured exist to ease space. Example: XML data. 
 
3.	Unstructured data – 
Unstructured data is a data which is not organized in a predefined manner or does not have a predefined data model, thus it is not a good fit for a mainstream relational database. So for Unstructured data, there are alternative platforms for storing and managing, it is increasingly prevalent in IT systems and is used by organizations in a variety of business intelligence and analytics applications. Example: Word, PDF, Text, Media logs. 
 
Differences between Structured, Semi-structured and Unstructured data: 
 
PROPERTIES	STRUCTURED DATA	SEMI-STRUCTURED DATA	UNSTRUCTURED DATA
Technology	It is based on Relational database table	It is based on XML/RDF(Resource Description Framework).	It is based on character and binary data
Transaction management	Matured transaction and various concurrency techniques	Transaction is adapted from DBMS not matured	No transaction management and no concurrency
Version management	Versioning over tuples,row,tables	Versioning over tuples or graph is possible	Versioned as a whole
Flexibility	It is schema dependent and less flexible	It is more flexible than structured data but less flexible than unstructured data	It is more flexible and there is absence of schema
Scalability	It is very difficult to scale DB schema	It’s scaling is simpler than structured data	It is more scalable.
Robustness	Very robust	New technology, not very spread	—
Query performance	Structured query allow complex joining 	Queries over anonymous nodes are possible	Only textual queries are possible
 
Attention reader! Don’t stop learning now. Get hold of all the important CS Theory concepts for SDE interviews with the CS Theory Course at a student-friendly price and become industry ready.
-
-
Data Structure
Data Structure nedir
In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.
Usage
Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.
Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,[6] while compiler implementations usually use hash tables to look up identifiers.
Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.
Implementation
Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself.
The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).
Examples
There are numerous types of data structures, generally built upon simpler primitive data types:[10]
An array is a number of elements in a specific order, typically all of the same type (depending on the language, individual elements may either all be forced to be the same type, or may be of almost any type). Elements are accessed using an integer index to specify which element is required. Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable.
A linked list (also just called list) is a linear collection of data elements of any type, called nodes, where each node has itself a value, and points to the next node in the linked list. The principal advantage of a linked list over an array is that values can always be efficiently inserted and removed without relocating the rest of the list. Certain other operations, such as random access to a certain element, are however slower on lists than on arrays.
A record (also called tuple or struct) is an aggregate data structure. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.
A union is a data structure that specifies which of a number of permitted primitive types may be stored in its instances, e.g. float or long integer. Contrast with a record, which could be defined to contain a float and an integer; whereas in a union, there is only one value at a time. Enough space is allocated to contain the widest member datatype.
A tagged union (also called variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type, for enhanced type safety.
An object is a data structure that contains data fields, like a record does, as well as various methods which operate on the data contents. An object is an in-memory instance of a class from a taxonomy. In the context of object-oriented programming, records are known as plain old data structures to distinguish them from objects.[11]
In addition, graphs and binary trees are other commonly used data structures.
-
RFC (GITHUB)
Rfc nedir 
Yorumlar İçin Talep (Orijinal adı: Request For Comments, RFC), TCP/IP nin tanımlanmasında kullanılan standart numaralara sahip dokümanlardır.

A Request for Comments (RFC) is a publication from the Internet Society (ISOC) and its associated bodies, most prominently the Internet Engineering Task Force (IETF), the principal technical development and standards-setting bodies for the Internet.
An RFC is authored by individuals or groups of engineers and computer scientists in the form of a memorandum describing methods, behaviors, research, or innovations applicable to the working of the Internet and Internet-connected systems. It is submitted either for peer review or to convey new concepts, information, or occasional engineering humor.[1] The IETF adopts some of the proposals published as RFCs as Internet Standards. However, many RFCs are informational or experimental in nature and are not standards.[2] The RFC system was invented by Steve Crocker in 1969 to help record unofficial notes on the development of ARPANET. RFCs have since become official documents of Internet specifications, communications protocols, procedures, and events.[3] According to Crocker, the documents "shape the Internet's inner workings and have played a significant role in its success", but are not well known outside the community.[4]
Requests for Comments are produced in non-reflowable text format, but work has begun to change the format so that documents can be viewed optimally in devices with varying display sizes.[5]
Outside of the Internet community, other documents also called requests for comments have been published in U.S. Federal government work, such as the National Highway Traffic Safety Administration.[6]
-
Xamarin (GITHUB)
Xamarin nedir
Xamarin is an open source app platform from Microsoft for building modern & performant iOS and Android apps with C# and .NET.
Xamarin is a Microsoft-owned San Francisco-based software company founded in May 2011[2] by the engineers that created Mono,[3] Xamarin.Android (formerly Mono for Android) and Xamarin.iOS (formerly MonoTouch), which are cross-platform implementations of the Common Language Infrastructure (CLI) and Common Language Specifications (often called Microsoft .NET).
With a C#-shared codebase, developers can use Xamarin tools to write native Android, iOS, and Windows apps with native user interfaces and share code across multiple platforms, including Windows, macOS, and Linux.[4] According to Xamarin, over 1.4 million developers were using Xamarin's products in 120 countries around the world as of April 2017.[5]
On February 24, 2016, Micros oft announced it had signed a definitive agreement to acquire Xamarin.[6]
-
-
-
Terminal 	(GITHUB)
Terminal nedir

Terminal, Apple bilgisayarları ve Linux işletim sistemi kullanılan bilgisayarların içerisinde yer alan, kullanıcıların bir klavye ve ekran yardımıyla işletim sistemi yada yazılımları kontrol etmesine yardımcı olan komut ekranıdır. 
A terminal is simply a text-based interface to the computer. In a terminal, you can type commands, manipulate files, execute programs, and open documents. When working in a terminal, the current directory is called your working directory. A terminal will usually start in the top-level directory of your account.
Directories are separated by a backslash /. The topmost directory is indicated by a single backslash. The total directory tree is all relative to the top level directory.
On MacOS, there is a Volumes directory at the top that lists all mounted drives (networked or local). So if you have mounted your network directory, the path to it will look like the following if you substitute your username for mine.
/Volumes/Personal/bmaxwell
In a terminal, you generally type a command and any modifiers or arguments the command requires, then hit return and the command is executed. For example, to set your current (working) directory to your home directory, type cd and the prompt and hit return. Below is an example of a terminal and some commands.
-
-
-
Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.

github ve açık kaynak yazılım ile yazılım dünyasına katılmıştır. bir projeye destek verilmek isteniyorsa, ilgili proje çatallanır* daha sonra ilgili değişiklikler yapılır. yalnız bu yapılan değişiklikler hala sizin taraftadır. bunların asıl projeye aktarılması için pull request atılır. proje sahibi yapılan değişiklikleri gözden geçirir ve uygunsa değişiklikleri uygular. *
ilk duyduğumda "ismi çok saçma push request olması gerekmiyor mu sonuçta karşı tarafin reposuna push yapmak istiyoruz" diye düşünürdüm ama sonra öğrendim ki "kardeş bak bu benim kodum sana zahmet repona pull'ar misin" şeklinde ortaya çıkmış.

gitlab gibi bazı mecralar bu karışıklığı engellemek için merge request demeyi tercih etmiş
-
-
-





Microsoft Exchange Server  (GITHUB) (örnek)
Microsoft Exchange Server nedir
Microsoft Exchange Server, Microsoft tarafından üretilen bir haberleşme yazılımıdır. Sunucu ürünlerinden Microsoft Servers'ın bir parçası olup Microsoft altyapısına sahip sistemlerde sıkça kullanılmaktadır. Exchange'in öne çıkan özellikleri e-posta, takvim, kişiler ve işler, bilgiye web tabanlı erişim olanağı ve veri depolama desteğidir.
Microsoft Exchange Server is a mail server and calendaring server developed by Microsoft. It runs exclusively on Windows Server operating systems.
The first version was called Exchange Server 4.0, to position it as the successor to the related Microsoft Mail 3.5. Exchange initially used the X.400 directory service but switched to Active Directory later. Until version 5.0 it came bundled with an email client called Microsoft Exchange Client. This was discontinued in favor of Microsoft Outlook.
Exchange Server primarily uses a proprietary protocol called MAPI to talk to email clients, but subsequently added support for POP3, IMAP, and EAS. The standard SMTP protocol is used to communicate to other Internet mail servers.
Exchange Server is licensed both as on-premises software and software as a service (SaaS). In the on-premises form, customers purchase client access licenses (CALs); as SaaS, Microsoft charges a monthly service fee instead.
-
Microsoft Exchange Server™, şirketlerin iletişim trafiğini merkezi olarak yönetmesi için Microsoft tarafından geliştirilen ölçeklenebilir bir iletişim ve iş birliği platformudur. E-posta ve belge paylaşımı, takvim, rehber ve veri depolama gibi birçok özelliği destekleyen Microsoft Exchange Server, sahip olduğu üstün güvenlik ve veri depolama özellikleri sayesinde şirketlerin iletişim trafiğini mevzuatlara uygun ve güvenli bir şekilde yönetmelerine olanak tanır.
Microsoft Outlook™ ve benzeri e-posta istemcileri ve akıllı cihazlar ile etkileşimli bir şekilde çalışan Microsoft Exchange Server, mobil ve bulut tabanlı iş birliği ve iletişim özellikleri ile kullanıcıların birçok farklı platform üzerinden iletişim kurmasına ve verileri tek bir noktada depolamasına olanak verir.
Microsoft Exchange Server, tüm bu özelliklere ek olarak sunduğu veri yedekleme ve olağanüstü durum kurtarma çözümleri ile işletmelerin sunucu, veri tabanı ve ağ bağlantısı arızalarının yol açabileceği negatif etkilerden korunmasına yardımcı olur.
Microsoft Exchange Server ile Neler Yapabilirsiniz?
•	E-posta ve doküman paylaşma
•	Merkezi iletişim trafiği yönetimi
•	Güvenli veri depolama ve veri yedekleme
•	Takvim ve rehber oluşturma ve yönetme
•	Bulut ve mobil tabanlı işbirliği
•	Kurallar aracılığıyla işlem otomatikleştirme
-
-
-
-
düz javascript'e denir… frameworksüz.

Using "VanillaJS" means using plain JavaScript without any additional libraries like jQuery.
People use it as a joke to remind other developers that many things can be done nowadays without the need for additional JavaScript libraries.
Here's a funny site that jokingly talks about this: http://vanilla-js.com/

-


Vanilla JS 
Vanilla JS is a fast, lightweight, cross-platform framework
for building incredible, powerful JavaScript applications.
Introduction
The Vanilla JS team maintains every byte of code in the framework and works hard each day to make sure it is small and intuitive. Who's using Vanilla JS? Glad you asked! Here are a few:
•	Facebook
•	Google
•	YouTube
•	Yahoo
•	Wikipedia
•	Windows Live
•	Twitter
•	Amazon
•	LinkedIn
•	MSN
•	eBay
•	Microsoft
•	Tumblr
•	Apple
•	Pinterest
•	PayPal
•	Reddit
•	Netflix
•	Stack Overflow
In fact, Vanilla JS is already used on more websites than jQuery, Prototype JS, MooTools, YUI, and Google Web Toolkit - combined.
Download
Ready to try Vanilla JS? Choose exactly what you need!
  Core Functionality DOM (Traversal / Selectors) Prototype-based Object System AJAX Animations Event System Regular Expressions Functions as first-class objects Closures Math Library Array Library String Library
Options
 Minify Source Code Produce UTF8 Output Use "CRLF" line breaks (Windows)
Final size: 0.00 KBytes uncompressed, 0.02 KBytes gzipped.   Show human-readable sizes
 
Testimonials
Vanilla JS is the lowest-overhead, most comprehensive framework I've ever used.
Getting Started
The Vanilla JS team takes pride in the fact that it is the most lightweight framework available anywhere; using our production-quality deployment strategy, your users' browsers will have Vanilla JS loaded into memory before it even requests your site.
To use Vanilla JS, just put the following code anywhere in your application's HTML:
1.	<script src="path/to/vanilla.js"></script>
When you're ready to move your application to a production deployment, switch to the much faster method:
1.	 
That's right - no code at all. Vanilla JS is so popular that browsers have been automatically loading it for over a decade.
Speed Comparison
Here are a few examples of just how fast Vanilla JS really is:
Retrieve DOM element by ID
	Code	ops / sec
Vanilla JS	document.getElementById('test-table');	12,137,211
 
Dojo	dojo.byId('test-table');	5,443,343
 
Prototype JS	$('test-table')	2,940,734
 
Ext JS	delete Ext.elCache['test-table']; Ext.get('test-table');	997,562
 
jQuery	$jq('#test-table');	350,557
 
YUI	YAHOO.util.Dom.get('test-table');	326,534
 
MooTools	document.id('test-table');	78,802
 
Retrieve DOM elements by tag name
	Code	ops / sec
Vanilla JS	document.getElementsByTagName("span");	8,280,893
 
Prototype JS	Prototype.Selector.select('span', document);	62,872
 
YUI	YAHOO.util.Dom.getElementsBy(function(){return true;},'span');	48,545
 
Ext JS	Ext.query('span');	46,915
 
jQuery	$jq('span');	19,449
 
Dojo	dojo.query('span');	10,335
 
MooTools	Slick.search(document, 'span', new Elements);	5,457
 
Code Examples
Here are some examples of common tasks in Vanilla JS and other frameworks:
Fade an element out and then remove it
Vanilla JS	var s = document.getElementById('thing').style;
s.opacity = 1;
(function fade(){(s.opacity-=.1)<0?s.display="none":setTimeout(fade,40)})();

jQuery	<script src="//ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
<script>
$('#thing').fadeOut();
</script>

Make an AJAX call
Vanilla JS	var r = new XMLHttpRequest();
r.open("POST", "path/to/api", true);
r.onreadystatechange = function () {
  if (r.readyState != 4 || r.status != 200) return;
  alert("Success: " + r.responseText);
};
r.send("banana=yellow");

jQuery	<script src="//ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
<script>
$.ajax({
  type: 'POST',
  url: "path/to/api",
  data: "banana=yellow",
  success: function (data) {
    alert("Success: " + data);
  },
});
</script>

Further Reading
For more information about Vanilla JS:
•	check out the Vanilla JS documentation
•	read some books on Vanilla JS
•	or try out one of the many Vanilla JS plugins.
-
-
-
Node.js       	(GITHUB)  (test)
Nodejs nedir node.js nedir, node js nedir
Node.js is an open-source, cross-platform, back-end, JavaScript runtime environment that executes JavaScript code outside a web browser. Node.js lets developers use JavaScript to write command line tools and for server-side scripting—running scripts server-side to produce dynamic web page content before the page is sent to the user's web browser. Consequently, Node.js represents a "JavaScript everywhere" paradigm,[6] unifying web-application development around a single programming language, rather than different languages for server- and client-side scripts.
Node.js
In contrast, Node may be easy to learn, but it takes more effort to implement web applications with it. What makes it difficult is the asynchronous programming that executes the non-blocking code. However, this does not prevent the execution of a piece of code. That’s why, sometimes, it is difficult to program.
Though .js is the standard filename extension for JavaScript code, the name "Node.js" doesn't refer to a particular file in this context and is merely the name of the product. Node.js has an event-driven architecture capable of asynchronous I/O. These design choices aim to optimize throughput and scalability in web applications with many input/output operations, as well as for real-time Web applications (e.g., real-time communication programs and browser games).[7]
The Node.js distributed development project was previously governed by the Node.js Foundation,[8] and has now merged with the JS Foundation to form the OpenJS Foundation, which is facilitated by the Linux Foundation's Collaborative Projects program.
Corporate users of Node.js software include GoDaddy,[10] Groupon,[11] IBM,[12] LinkedIn,[13][14] Microsoft,[15][16] Netflix,[17] PayPal,[18][19] Rakuten, SAP,[20] Voxer,[21] Walmart,[22] Yahoo!,[23] and Amazon Web Services.
-
Node.js, açık kaynaklı, sunucu tarafında çalışan ve ağ bağlantılı uygulamalar için geliştirilmiş bir çalıştırma ortamıdır (İng. runtime environment). Node.js uygulamaları genelde istemci tarafı betik dili olan JavaScript kullanılarak geliştirilir.
En önemli avantajı JavaScript'in sağladığı bloklamayan G/Ç (İng. non-blocking I/O) imkânıyla yüksek ölçeklenebilirliği (İng. scalability) ve yüksek veri aktarabilmesidir. Bu teknolojiler sık sık gerçek zamanlı Web uygulamalarında tercih edilmekle beraber kullanım alanı popülaritesiyle orantılı olarak genişlemiştir.
Node.js, Google V8 JavaScript motorunu kullanarak betik dilini yorumlar ve içerisinde standart olarak dağıtılan kütüphaneler sayesinde ek bir sunucu yazılımına (Apache HTTP Sunucusu, Nginx, IIS v.s.) gerek kalmadan uygulamanın Web sunucusu görevini görür.
-
-





BAŞLIK     	(empty)  (empty) 
[başlık nedir]
Renkler: yazı örnek yazı yazı örnek yazı yazı örnek yazı 
-
-
ECMAScript is the language, whereas JavaScript, JScript, and even ActionScript 3 are called "dialects". Wikipedia sheds some light on this.

ECMAScript veya ES, Ecma International tarafından ECMA-262 ve ISO/IEC 16262 standartlarıyla standartlaştırılmış, markalaşmış bir betik dili spesifikasyonudur. Şu anda kendisini izleyen Javascript tabanlı olarak geliştirilmiştir. Yaygın olarak Dünya Çapında Ağ (www) için istemci taraflı betik dili olarak kullanılır. ECMAScript'in diğer uygulamaları JSCript ve ActionScript'tir.

ECMAScript betik dili, Netscape'ten Brendan Eich tarafından geliştirilmiş olan bir programlama dilinin -başlangıçta Mocha, daha sonra LiveScript olarak adlandırılan bu dil son olarak Javascript adını almıştır- standart spesifikasyonudur. Aralık 1995'te, Sun Microsystem ve Netspace Javascript'i basın bülteniyle duyurmuşlardır. Mart 1996'da ise, Javascript dilini destekleyen Netscape Navigator 2.0 piyasaya sürüldü.
-
-
JavaScript
JavaScript was first known as LiveScript, but Netscape changed its name to JavaScript, possibly because of the excitement being generated by Java. (Jscript: JScript is Microsoft's dialect of the ECMAScript standard[2] that is used in Microsoft's Internet Explorer.) 

JavaScript made its first appearance in Netscape 2.0 in 1995 with the name LiveScript. The general-purpose core of the language has been embedded in Netscape, Internet Explorer, and other web browsers.
The standard of scripting languages like JavaScript is ECMAScript.
ECMAScript
The full form of ECMA is European Computer Manufacturer's Association. ECMAScript is a Standard for scripting languages such as JavaScript, JScript, etc. It is a trademark scripting language specification. JavaScript is a language based on ECMAScript. A standard for scripting languages like JavaScript, JScript is ECMAScript. JavaScript is considered as one of the most popular implementations of ECMAScript.
-
-
-




JavaScript
JavaScript was first known as LiveScript, but Netscape changed its name to JavaScript, possibly because of the excitement being generated by Java. JavaScript made its first appearance in Netscape 2.0 in 1995 with the name LiveScript. The general-purpose core of the language has been embedded in Netscape, Internet Explorer, and other web browsers.
The standard of scripting languages like JavaScript is ECMAScript.
ECMAScript
The full form of ECMA is European Computer Manufacturer's Association. ECMAScript is a Standard for scripting languages such as JavaScript, JScript, etc. It is a trademark scripting language specification. JavaScript is a language based on ECMAScript. A standard for scripting languages like JavaScript, JScript is ECMAScript. JavaScript is considered as one of the most popular implementations of ECMAScript.

ECMAScript = ES:
•	ECMAScript is a Standard for scripting languages.
•	Languages like Javascript are based on the ECMAScript standard.
•	ECMA Standard is based on several originating technologies, the most well known being JavaScript (Netscape) and JScript (Microsoft).
•	ECMA means European Computer Manufacturer’s Association
JavaScript = JS:
•	JavaScript is the most popular implementation of the ECMAScript Standard.
•	The core features of Javascript are based on the ECMAScript standard,  but Javascript also has other additional features that are not in the ECMA specifications/standard.
•	ActionScript and JScript are other languages that implement the ECMAScript.
•	JavaScript was submitted to ECMA for standardization but due to trademark issues with the name Javascript the standard became called ECMAScript.
•	Every browser has a JavaScript interpreter.
ES5 = ECMAScript 5:
•	ES5 is a version of the ECMAScript (old/current one).
•	ES5 is the JavaScript you know and use in the browser today.
•	ES5 does not require a build step (transpilers) to transform it into something that will run in today's browsers.
•	ECMAScript version 5 was finished in December 2009,  the latest versions of all major browsers (Chrome, Safari, Firefox, and IE)  have implemented version 5.
•	Version 5.1 was finished in June, 2011.
ES6 = ECMAScript 6 = ES2015 = ECMAScript 2015:
•	ES2015 is a version of the ECMAScript (new/future one).
•	Officially the name ES2015 should be used instead of ES6.
•	ES6 will tackle many of the core language shortcomings addressed in  TypeScript and CoffeeScript.
•	ES6 is the next iteration of JavaScript, but it does not run in today's browsers.
•	There are quite a few transpilers that will export ES5 for running in browsers.
BabelJS:
•	BabelJS is the most popular transpiler that transforms new JavaScript ES6 to Old JavaScript ES5.
•	BabelJS makes it possible for writing the next generation of JavaScript today (means ES2015).
•	BabelJS simply takes ES2015 file and transform it into ES5 file.
•	Current browsers versions can now understand the new JavaScript code (ES2015), even if they don't yet support it.
TypeScript and CoffeeScript:
•	Both provides syntactic sugar on top of ES5  and then are transcompiled into ES5 compliant JavaScript. 
•	You write TypeScript or CoffeeScript then the transpiler transforms it into ES5 JavaScript.
-
-
-
JScript is Microsoft's dialect of the ECMAScript standard[2] that is used in Microsoft's Internet Explorer.
JScript is implemented as an Active Scripting engine. This means that it can be "plugged in" to OLE Automation applications that support Active Scripting, such as Internet Explorer, Active Server Pages, and Windows Script Host.[3] It also means such applications can use multiple Active Scripting languages, e.g., JScript, VBScript or PerlScript.
JScript was first supported in the Internet Explorer 3.0 browser released in August 1996. Its most recent version is JScript 9.0, included in Internet Explorer 9.
JScript 10.0[4] is a separate dialect, also known as JScript .NET, which adds several new features from the abandoned fourth edition of the ECMAScript standard. It must be compiled for .NET Framework version 2 or version 4, but static type annotations are optional.
Comparison to JavaScript
As explained by Douglas Crockford in his talk titled The JavaScript Programming Language on YUI Theater,
[Microsoft] did not want to deal with Sun Microsystems about the trademark issue, and so they called their implementation JScript. A lot of people think that JScript and JavaScript are different but similar languages. That's not the case. They are just different names for the same language, and the reason the names are different was to get around trademark issues.[5]
However, JScript supports conditional compilation, which allows a programmer to selectively execute code within block comments. This is an extension to the ECMAScript standard that is not supported in other JavaScript implementations, thus making the above statement not completely true, although conditional compilation is no longer supported in Internet Explorer 11 Standards mode.
Other internal implementation differences between JavaScript and JScript, at some point in time, are noted on the Microsoft Developer Network (MSDN).[6] Although, the default type value for the script element in Internet Explorer is JavaScript, while JScript was its alias.[7] In an apparent transition from JScript to JavaScript, online, the Microsoft Edge Developer Guide refers to the Mozilla MDN web reference library as its definitive documentation.[8] As of October 2017, Microsoft MSDN pages for scripting in Internet Explorer are being redirected there as well.[9] This information may not include JScript specific objects, such as Enumerator, which are listed in the JavaScript language reference on Microsoft Docs.[10] Those provide additional features that are not included in the ECMA Standards, whether they are supported in the Edge browser or its predecessor.[11]
-
-
-
ASP.NET MVC Framework
 
ASP.NET MVC Framework "Community Technology Preview", 10 Aralık 2007 tarihinde, Microsoft'un ASP.NET için geliştirdiği Model-view-controller iskeletidir. Bu iskelet ile ASP.NET uygulamaları, Model, View ve Controller rollerine bölünerek geliştiriliyor.Model (Tanım) uygulamanın üzerinde duracağı meseleyi temsil ediyor. Genel olarak bu bir veritabanıdır. Controller, Model üzerindeki operasyonları tarif eden roldür. View ise, gerekli bilginin görünümünden sorumludur. ASP.NET MVC Framework, "interface"ler aracılığıyla bu üç rolü de destekliyor. Resmi olarak geliştirilen View kısmı sadece Web Formlarını destekliyor. Ama bağımsız olarak başka görünüm motorları uyarlanabilir. Sayfalar etkileşimde PostBack değil, URL Routing mekanizmasını kullanıyor. Böylelikle fiziksel dosya hiyerarşisiyle ilişkisiz, REST uyumlu URL'ler tanımlanabiliyor. ASP.NET MVC Framework bu URL'lerin ilgili Controller metotlarına eşlenmesi ve bu URL'lerin üretimi için hazır bir altyapı sunuyor.
Asp.net MVC Model-View-Controller Yapısı
Model
Veritabanına erişim, sınıflar(class),ilişkiler(kategori-ürün ilişkisi) gibi yapıların bulunduğu çeşitli frameworklerinde: Entity Framework,Linq to Sql,Nhibernate,Ado.Net gibi frameworkleri içerisinde barındırır.
View
Kullanıcının gördüğü ve arayüze ait Html,Css ve Javascript gibi teknolojilerin bulunduğu ve yazıldığı kısımdır(javascript kodları buraya yazılabilir veya dışarıdan çağrılabilir)
Controller
İstemci tarafından isteklerin yakalandığı ve işleme tabi tutulduğu,model ile view arasında köprü görevi gören değişkenlerin tanımlandığı,method,fonksiyonların çağrıldığı bir yapıdır
ASP.Net Mvc Nasıl Çalışır ?
Bir istek(request) oluyor controller kısmında gerekli method,fonksiyon çalışıyor model kısmına gidiyor model yeniden controller kısmına giderek işlemler gerçekleştiriyor yaptığı işlemleri view kısmında bir arayüze dönüştürüp çıktı olarak view de ekrana basıyor
-
-
-


























